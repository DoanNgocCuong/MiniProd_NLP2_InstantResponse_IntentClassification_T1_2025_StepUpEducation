{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLWoGrdRXTSg"
      },
      "source": [
        "## 1. Tải Dữ Liệu từ CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbMKcafkKCPk",
        "outputId": "1b443434-4fe4-4d23-c623-0390b5d85848"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.11)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.27.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "zIVYywvyiq2L"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, TrainingArguments, Trainer, AutoModel\n",
        "import numpy as np\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "import torch.nn as nn\n",
        "import os\n",
        "from typing import List\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\" ## Setup CUDA GPU 1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Kiểm tra GPU khả dụng\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"Number of GPUs:\", torch.cuda.device_count())\n",
        "if torch.cuda.is_available():\n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
        "else:\n",
        "    print(\"No GPU found.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SHnftkneP6B",
        "outputId": "4a5b17be-36dc-48f2-b84b-984756cd6006"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: True\n",
            "Number of GPUs: 1\n",
            "GPU 0: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "def select_gpu():\n",
        "    \"\"\"\n",
        "    Kiểm tra GPU khả dụng và tự động chọn GPU phù hợp.\n",
        "    \"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        num_gpus = torch.cuda.device_count()\n",
        "        print(f\"Number of GPUs available: {num_gpus}\")\n",
        "\n",
        "        # Duyệt qua các GPU khả dụng để tìm GPU ít sử dụng nhất\n",
        "        available_gpus = [torch.cuda.get_device_name(i) for i in range(num_gpus)]\n",
        "        print(\"Available GPUs:\", available_gpus)\n",
        "\n",
        "        for i in range(num_gpus):\n",
        "            try:\n",
        "                # Đặt GPU\n",
        "                os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(i)\n",
        "                device = torch.device(f\"cuda:{i}\")\n",
        "                torch.cuda.set_device(device)\n",
        "                print(f\"Using GPU: {torch.cuda.get_device_name(device.index)}\")\n",
        "                return device\n",
        "            except Exception as e:\n",
        "                print(f\"GPU {i} is not suitable: {e}\")\n",
        "\n",
        "        print(\"No suitable GPU found. Falling back to CPU.\")\n",
        "        return torch.device(\"cpu\")\n",
        "    else:\n",
        "        print(\"No GPUs available. Using CPU.\")\n",
        "        return torch.device(\"cpu\")\n",
        "\n",
        "# Tự động chọn GPU hoặc CPU\n",
        "device = select_gpu()\n",
        "\n",
        "# Kiểm tra lại thiết bị đang sử dụng\n",
        "print(f\"Final selected device: {device}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ge41QgJadXLv",
        "outputId": "7f02fbd5-7f12-42d0-9711-d1e50ae8f1ea"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of GPUs available: 1\n",
            "Available GPUs: ['Tesla T4']\n",
            "Using GPU: Tesla T4\n",
            "Final selected device: cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "IkfPfDY3iskg"
      },
      "outputs": [],
      "source": [
        "\n",
        "class BERTIntentClassification(nn.Module):\n",
        "\n",
        "\n",
        "    def __init__(self, model_name=\"bert-base-uncased\", num_classes=10, dropout_rate=0.1, cache_dir = \"huggingface\"):\n",
        "        super(BERTIntentClassification, self).__init__()\n",
        "        self.bert = AutoModel.from_pretrained(model_name, cache_dir = cache_dir)\n",
        "        # Get BERT hidden size\n",
        "        hidden_size = self.bert.config.hidden_size\n",
        "        self.ffnn = nn.Sequential(\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.LayerNorm(hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(hidden_size, num_classes)\n",
        "        )\n",
        "\n",
        "\n",
        "    def freeze_bert(self):\n",
        "        for param in self.bert.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "\n",
        "    def get_pooling(self, hidden_state, attention_mask):\n",
        "        \"\"\"\n",
        "        Get mean pooled representation from BERT hidden states\n",
        "        Args:\n",
        "            hidden_state: BERT output containing hidden states\n",
        "        Returns:\n",
        "            pooled_output: Mean pooled representation of the sequence\n",
        "        \"\"\"\n",
        "        # Get last hidden state\n",
        "        last_hidden_state = hidden_state.last_hidden_state  # Shape: [batch_size, seq_len, hidden_size]\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            # Expand attention mask to match hidden state dimensions\n",
        "            attention_mask = attention_mask.unsqueeze(-1)  # [batch_size, seq_len, 1]\n",
        "\n",
        "            # Mask out padding tokens\n",
        "            masked_hidden = last_hidden_state * attention_mask\n",
        "\n",
        "            # Calculate mean (sum / number of actual tokens)\n",
        "            sum_hidden = torch.sum(masked_hidden, dim=1)  # [batch_size, hidden_size]\n",
        "            count_tokens = torch.sum(attention_mask, dim=1)  # [batch_size, 1]\n",
        "            pooled_output = sum_hidden / count_tokens\n",
        "        else:\n",
        "            # If no attention mask, simply take mean of all tokens\n",
        "            pooled_output = torch.mean(last_hidden_state, dim=1)\n",
        "\n",
        "        return pooled_output\n",
        "\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, **kwargs):\n",
        "        \"\"\"\n",
        "        Forward pass of the model\n",
        "        Args:\n",
        "            input_ids: Input token IDs\n",
        "            attention_mask: Attention mask for padding\n",
        "        Returns:\n",
        "            logits: Raw logits for each class\n",
        "        \"\"\"\n",
        "        # Get BERT hidden states\n",
        "        hidden_state = self.bert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "        )\n",
        "\n",
        "        # Get pooled representation\n",
        "        hidden_state_pooling = self.get_pooling(hidden_state=hidden_state, attention_mask=attention_mask)\n",
        "\n",
        "        # Pass through FFNN classifier\n",
        "        logits = self.ffnn(hidden_state_pooling)\n",
        "\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "zypDXvoaivAb"
      },
      "outputs": [],
      "source": [
        "class TrainerCustom(Trainer):\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        \"\"\"\n",
        "        How the loss is computed by Trainer. By default, all models return the loss in the first element.\n",
        "\n",
        "        Subclass and override for custom behavior.\n",
        "        \"\"\"\n",
        "        if \"labels\" in inputs:\n",
        "            labels = inputs.pop(\"labels\")\n",
        "        else:\n",
        "            labels = None\n",
        "\n",
        "        # Sử dụng nn.CrossEntropyLoss() thay vì nn.CrossEntropy\n",
        "        cross_entropy_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "        # Chạy mô hình và nhận đầu ra (logits)\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "        # Đảm bảo lấy logits từ outputs (mô hình trả về tuple, lấy phần tử đầu tiên là logits)\n",
        "        logits = outputs\n",
        "\n",
        "        # Tính toán loss\n",
        "        loss = cross_entropy_loss(logits, labels)\n",
        "\n",
        "        # Trả về loss và outputs nếu cần\n",
        "        return (loss, outputs) if return_outputs else loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zIJIYGcppMk"
      },
      "source": [
        "# 1. Load Dataset and with Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "DQBY8_d5rwlj"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # Bước 1: Tải dữ liệu\n",
        "# # Sử dụng dataset sẵn có từ Hugging Face hoặc tải từ file cục bộ\n",
        "# dataset = load_dataset(\"imdb\", cache_dir = \"huggingface\")  # Ví dụ: Dữ liệu IMDB để phân loại sentiment\n",
        "# # Thay thế trường 'text' thành 'input_ids' trong train_dataset và test_dataset\n",
        "# def preprocess_dataset(dataset):\n",
        "#     return dataset.map(lambda example: {\n",
        "#             \"input_ids\": example['text'],\n",
        "#             \"label\": example['label']\n",
        "#         },\n",
        "#         remove_columns=[\"text\"],\n",
        "#         num_proc=4  # Sử dụng 4 tiến trình song song để xử lý nhanh hơn\n",
        "#     )\n",
        "\n",
        "# train_dataset = preprocess_dataset(dataset[\"train\"])\n",
        "# test_dataset = preprocess_dataset(dataset[\"test\"])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "RBMrLoabi9DJ"
      },
      "outputs": [],
      "source": [
        "# print(train_dataset)\n",
        "# # Truy cập mẫu cụ thể\n",
        "# train_sample = train_dataset[:10]\n",
        "# test_sample = test_dataset[:2]\n",
        "# print(train_sample)\n",
        "\n",
        "\n",
        "# from datasets import Dataset\n",
        "\n",
        "# train_sample = train_dataset[:10]\n",
        "\n",
        "# # Chuyển từ dict về Dataset\n",
        "# train_sample_dataset = Dataset.from_dict(train_sample)\n",
        "# test_sample_dataset = Dataset.from_dict(test_sample)\n",
        "# print(train_sample_dataset)\n",
        "# print(type(train_sample_dataset))\n",
        "# # Output: <class 'datasets.arrow_dataset.Dataset'>\n",
        "\n",
        "\n",
        "# # In thử 1 hàng trong test_sample_dataset\n",
        "# print(\"First row in test_sample_dataset:\")\n",
        "# print(test_sample_dataset[0])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "slaqKCOhjh9U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "547ee797-0453-4732-972d-6d9973de399c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['label', 'input_ids'],\n",
            "    num_rows: 27\n",
            "})\n",
            "Dataset({\n",
            "    features: ['label', 'input_ids'],\n",
            "    num_rows: 10\n",
            "})\n",
            "First row in test_sample_dataset:\n",
            "{'label': 'Agree', 'input_ids': 'Yes, I want to show you the picture.'}\n"
          ]
        }
      ],
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "def load_csv_dataset(csv_path, text_column, label_column):\n",
        "    \"\"\"\n",
        "    Tải dataset từ file CSV và đổi tên cột.\n",
        "\n",
        "    Args:\n",
        "        csv_path (str): Đường dẫn đến file .csv.\n",
        "        text_column (str): Tên cột chứa văn bản.\n",
        "        label_column (str): Tên cột chứa nhãn.\n",
        "\n",
        "    Returns:\n",
        "        Dataset: Tập dữ liệu đã tải từ file .csv.\n",
        "    \"\"\"\n",
        "    # Tải dữ liệu từ file .csv\n",
        "    dataset = Dataset.from_csv(csv_path)\n",
        "    # Đổi tên cột\n",
        "    dataset = dataset.rename_columns({text_column: \"input_ids\", label_column: \"label\"})\n",
        "    return dataset\n",
        "\n",
        "# Sử dụng hàm\n",
        "csv_path = \"/content/chatbot_intent_data_v1_En.csv\"             # Đường dẫn file CSV\n",
        "text_column = \"input_ids\"       # Cột chứa văn bản\n",
        "label_column = \"label\"        # Cột chứa nhãn\n",
        "\n",
        "# Tải dataset\n",
        "dataset = load_csv_dataset(csv_path, text_column, label_column)\n",
        "\n",
        "# Kiểm tra dữ liệu\n",
        "print(dataset)\n",
        "\n",
        "# Truy cập mẫu cụ thể\n",
        "sample_dataset = dataset.select(range(10))  # Lấy 10 mẫu đầu tiên\n",
        "print(sample_dataset)\n",
        "\n",
        "\n",
        "# In thử 1 hàng trong test_sample_dataset\n",
        "print(\"First row in test_sample_dataset:\")\n",
        "print(sample_dataset[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Ví dụ minh họa\n",
        "Giả sử dữ liệu gốc:\n",
        "\n",
        "| question                       | answer                              | intent           |\n",
        "|--------------------------------|-------------------------------------|------------------|\n",
        "| Cậu có thể kể tên một số hành động không? | Tớ biết 'play football'.          | intent_positive  |\n",
        "| Cậu có biết thêm từ nào không? | Tớ không chắc lắm.                  | intent_neutral   |\n",
        "| Cậu có muốn thử lại không?     | NaN                                 | intent_neutral   |\n",
        "\n",
        "Sau khi ghép:\n",
        "\n",
        "| input_ids                                                 | label           |\n",
        "|-----------------------------------------------------------|-----------------|\n",
        "| Cậu có thể kể tên một số hành động không? Tớ biết 'play football'. | intent_positive |\n",
        "| Cậu có biết thêm từ nào không? Tớ không chắc lắm.          | intent_neutral  |\n",
        "| Cậu có muốn thử lại không?                                | intent_neutral  |\n"
      ],
      "metadata": {
        "id": "4Vpl4kj_auSC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Cách kết hợp question và answer:**\n",
        "\n",
        "1. **Ghép nối trực tiếp:** Bạn có thể kết hợp câu hỏi và câu trả lời thành một chuỗi duy nhất, sử dụng một ký tự đặc biệt hoặc dấu phân cách để tách biệt chúng. Ví dụ:\n",
        "\n",
        "   ```python\n",
        "   combined_text = question + \" [SEP] \" + answer\n",
        "   ```\n",
        "\n",
        "   Trong đó, `[SEP]` là một token đặc biệt thường được sử dụng trong các mô hình như BERT để phân tách các đoạn văn bản khác nhau.\n",
        "\n",
        "2. **Sử dụng token đặc biệt:** Một số mô hình hỗ trợ các token đặc biệt để đánh dấu bắt đầu và kết thúc của câu hỏi và câu trả lời. Ví dụ:\n",
        "\n",
        "   ```python\n",
        "   combined_text = \"[CLS] \" + question + \" [SEP] \" + answer + \" [SEP]\"\n",
        "   ```\n",
        "\n",
        "   - `[CLS]`: Token đánh dấu bắt đầu chuỗi (thường dùng trong BERT).\n",
        "   - `[SEP]`: Token phân tách giữa các phần của chuỗi.\n",
        "\n"
      ],
      "metadata": {
        "id": "CjW1FC-ibueZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Việc chọn cách tốt nhất giữa hai cách trên phụ thuộc vào yêu cầu của bài toán và loại mô hình bạn đang sử dụng. Dưới đây là phân tích để bạn chọn cách phù hợp nhất:\n",
        "\n",
        "---\n",
        "\n",
        "### 1. **Ghép nối trực tiếp (`question + \" [SEP] \" + answer`)**\n",
        "   - **Ưu điểm**:\n",
        "     - Dễ dàng thực hiện, không phụ thuộc vào kiến trúc mô hình cụ thể.\n",
        "     - Giữ được ngữ cảnh rõ ràng bằng cách sử dụng một ký tự phân cách như `[SEP]`.\n",
        "     - Phù hợp với hầu hết các mô hình xử lý ngôn ngữ hiện đại như BERT hoặc RoBERTa.\n",
        "   - **Nhược điểm**:\n",
        "     - Không sử dụng các token đặc biệt như `[CLS]`, có thể giảm khả năng mô hình hiểu cấu trúc câu.\n",
        "\n",
        "   - **Khi nào dùng**:\n",
        "     - Khi bạn cần một cách tiếp cận nhanh và không muốn tùy chỉnh thêm.\n",
        "     - Khi sử dụng mô hình đơn giản hoặc không có yêu cầu đặc biệt về định dạng dữ liệu.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Sử dụng token đặc biệt (`\"[CLS] \" + question + \" [SEP] \" + answer + \" [SEP]\"`)**\n",
        "   - **Ưu điểm**:\n",
        "     - Phù hợp với kiến trúc của các mô hình như BERT, nơi `[CLS]` được sử dụng để tạo embedding đại diện cho toàn bộ câu.\n",
        "     - `[SEP]` giúp mô hình phân biệt rõ ràng giữa câu hỏi và câu trả lời.\n",
        "     - Có thể cải thiện hiệu quả mô hình khi cần hiểu rõ ngữ cảnh giữa hai phần.\n",
        "   - **Nhược điểm**:\n",
        "     - Yêu cầu tokenizer của mô hình phải hỗ trợ các token đặc biệt này.\n",
        "     - Có thể phức tạp hơn một chút trong khâu xử lý dữ liệu ban đầu.\n",
        "\n",
        "   - **Khi nào dùng**:\n",
        "     - Khi sử dụng mô hình hỗ trợ các token đặc biệt như `[CLS]` và `[SEP]`.\n",
        "     - Khi bài toán yêu cầu độ chính xác cao và có đủ tài nguyên để thực hiện.\n",
        "\n",
        "---\n",
        "\n",
        "### **Cách nào tốt nhất?**\n",
        "- **Nếu sử dụng các mô hình như BERT hoặc các biến thể của nó** (mô hình dựa trên kiến trúc transformer), **cách thứ 2** thường là lựa chọn tốt nhất vì nó tận dụng được các token đặc biệt `[CLS]` và `[SEP]` để cải thiện hiệu suất mô hình.\n",
        "  \n",
        "- **Nếu sử dụng các mô hình đơn giản hơn hoặc không yêu cầu đặc biệt về token hóa**, **cách thứ 1** là đủ tốt và đơn giản.\n",
        "\n",
        "---\n",
        "\n",
        "**Tóm lại:**\n",
        "- **Chọn cách 1**: Nếu bạn cần sự đơn giản và nhanh chóng.\n",
        "- **Chọn cách 2**: Nếu bạn sử dụng mô hình transformer (BERT) và muốn tối ưu hiệu suất."
      ],
      "metadata": {
        "id": "_QbiJ5wNcCY7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ví dụ thực tế về chuỗi chuẩn:\n",
        "\n",
        "Một câu/đoạn duy nhất:\n",
        "```\n",
        "[CLS] This is the first sentence. [SEP]\n",
        "```\n",
        "Hai câu/đoạn (ví dụ: câu hỏi và trả lời):\n",
        "```\n",
        "[CLS] What is your name? [SEP] My name is John. [SEP]\n",
        "```\n",
        "Nhiều câu/đoạn (3 đoạn):\n",
        "```\n",
        "[CLS] Question 1 [SEP] Answer 1 [SEP] Extra information [SEP]\n",
        "```"
      ],
      "metadata": {
        "id": "hyd8l8VPdgpg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "                          input_ids  intent\n",
        "0  [CLS] Cậu có muốn tiếp tục không? [SEP]  silence\n",
        "1                          [CLS] [SEP]  silence\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "mSOI6zlGfQV_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "\n",
        "def combine_with_special_tokens(row, text_columns, cls_token=\"[CLS]\", sep_token=\"[SEP]\"):\n",
        "    \"\"\"\n",
        "    Thêm các token đặc biệt vào chuỗi kết hợp từ các cột văn bản.\n",
        "\n",
        "    Args:\n",
        "        row (pd.Series): Dòng dữ liệu từ DataFrame.\n",
        "        text_columns (list): Danh sách các cột văn bản cần kết hợp.\n",
        "        cls_token (str): Token bắt đầu câu.\n",
        "        sep_token (str): Token phân cách.\n",
        "\n",
        "    Returns:\n",
        "        str: Chuỗi văn bản đã thêm token đặc biệt.\n",
        "    \"\"\"\n",
        "    tokens = [cls_token]  # Thêm [CLS] đầu tiên\n",
        "\n",
        "    # Thêm nội dung từ các cột văn bản\n",
        "    for col in text_columns:\n",
        "        if pd.notna(row[col]) and row[col].strip():  # Kiểm tra không rỗng\n",
        "            tokens.append(row[col].strip())\n",
        "            tokens.append(sep_token)  # Thêm [SEP] sau mỗi đoạn\n",
        "\n",
        "    # Nếu không có nội dung nào được thêm, chỉ giữ lại [CLS] và [SEP]\n",
        "    if len(tokens) == 1:\n",
        "        tokens.append(sep_token)\n",
        "\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "def load_xlsx_dataset(xlsx_path, text_columns, label_column, cls_token=\"[CLS]\", sep_token=\"[SEP]\"):\n",
        "    \"\"\"\n",
        "    Tải dataset từ file Excel (.xlsx) và xử lý dữ liệu.\n",
        "\n",
        "    Args:\n",
        "        xlsx_path (str): Đường dẫn đến file .xlsx.\n",
        "        text_columns (list): Danh sách các cột cần ghép để tạo văn bản đầu vào.\n",
        "        label_column (str): Tên cột chứa nhãn.\n",
        "        cls_token (str): Token bắt đầu câu.\n",
        "        sep_token (str): Token phân cách.\n",
        "\n",
        "    Returns:\n",
        "        Dataset: Tập dữ liệu đã xử lý.\n",
        "    \"\"\"\n",
        "    # Đọc file Excel bằng pandas\n",
        "    df = pd.read_excel(xlsx_path)\n",
        "\n",
        "    # Kiểm tra các cột cần thiết\n",
        "    for col in text_columns + [label_column]:\n",
        "        if col not in df.columns:\n",
        "            raise ValueError(f\"Missing required column: {col}\")\n",
        "\n",
        "    # Ghép các cột text lại thành một chuỗi duy nhất với token đặc biệt\n",
        "    df[\"input_ids\"] = df.apply(lambda row: combine_with_special_tokens(row, text_columns, cls_token, sep_token), axis=1)\n",
        "\n",
        "    # Đổi tên cột nhãn\n",
        "    df = df.rename(columns={label_column: \"label\"})\n",
        "\n",
        "    # Chuyển đổi DataFrame thành Dataset\n",
        "    dataset = Dataset.from_pandas(df[[\"input_ids\", \"label\"]])\n",
        "\n",
        "    return dataset\n",
        "\n",
        "# Sử dụng hàm\n",
        "xlsx_path = \"/content/processed_data_example_v2.xlsx\"  # Đường dẫn file Excel\n",
        "text_columns = [\"question\", \"answer\"]  # Các cột cần ghép\n",
        "label_column = \"intent\"  # Cột chứa nhãn\n",
        "\n",
        "# Tải dataset từ Excel\n",
        "dataset = load_xlsx_dataset(xlsx_path, text_columns, label_column)\n",
        "\n",
        "# Kiểm tra dữ liệu\n",
        "print(dataset)\n",
        "\n",
        "# Lấy 10 mẫu đầu tiên\n",
        "sample_dataset = dataset.select(range(20))\n",
        "print(sample_dataset)\n",
        "\n",
        "# In thử một hàng\n",
        "print(\"First row in dataset:\")\n",
        "print(sample_dataset[0])\n",
        "print(sample_dataset[11])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHXznY2KbtzB",
        "outputId": "80dcf07b-c4d2-4f5a-b341-4c52b0da1672"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['input_ids', 'label'],\n",
            "    num_rows: 120\n",
            "})\n",
            "Dataset({\n",
            "    features: ['input_ids', 'label'],\n",
            "    num_rows: 20\n",
            "})\n",
            "First row in dataset:\n",
            "{'input_ids': \"[CLS] Cậu có thể kể tên một số hành động bắt đầu bằng từ 'play' không? [SEP] Tớ có thể nói 'play football' và 'play basketball'. [SEP]\", 'label': 'intent_positive'}\n",
            "{'input_ids': '[CLS] Cậu có muốn tiếp tục không? [SEP]', 'label': 'silence'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "1oFci1j5k1UY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12652bee-c30e-423d-b727-90138d0a3a6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Invalid Samples =====\n",
            "[]\n"
          ]
        }
      ],
      "source": [
        "def check_invalid_samples(dataset):\n",
        "    invalid_samples = []\n",
        "    for idx, sample in enumerate(dataset):\n",
        "        if not isinstance(sample[\"input_ids\"], str) or sample[\"input_ids\"].strip() == \"\":\n",
        "            invalid_samples.append((idx, sample))\n",
        "    return invalid_samples\n",
        "\n",
        "# Kiểm tra dữ liệu không hợp lệ\n",
        "invalid_samples = check_invalid_samples(dataset)\n",
        "print(\"\\n===== Invalid Samples =====\")\n",
        "print(invalid_samples)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "x0-3V7dLlCZp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240,
          "referenced_widgets": [
            "cdfcfcb4a9f349609fc9f52ec0c872c5",
            "eab882737bb74a35ad810005945a8aa9",
            "6acc8d453c9f48549501307caeb745ab",
            "86c4ea6bc9a346d29ff39eea3dd62ca7",
            "eded7dc6494e48d2831ff22fafdb505d",
            "7b11260de16e4993b64673de5ae464f4",
            "3779f947e6f643e78b7f8fca00cadb9b",
            "bd67655dc69a4b05920866cd9058cc84",
            "e1a93c86301f4b1ea17a4f58671c904d",
            "aeca87675938457caec953fdac04bf77",
            "8f28603486904ae8a7f8cb4db0651d6d"
          ]
        },
        "outputId": "85aaf682-ef50-43c6-e73f-bb1f138b4caa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ánh xạ nhãn: {'intent_fallback': 0, 'intent_learn_more': 1, 'intent_negative': 2, 'intent_neutral': 3, 'intent_positive': 4, 'silence': 5}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/120 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cdfcfcb4a9f349609fc9f52ec0c872c5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['input_ids', 'label'],\n",
            "    num_rows: 120\n",
            "})\n",
            "Dataset({\n",
            "    features: ['input_ids', 'label'],\n",
            "    num_rows: 10\n",
            "})\n",
            "First row in sample_dataset:\n",
            "{'input_ids': \"[CLS] Cậu có biết thêm từ nào khác không? [SEP] Tớ biết 'play games' nữa. [SEP]\", 'label': 4}\n"
          ]
        }
      ],
      "source": [
        "# Tự động phát hiện nhãn và tạo ánh xạ nhãn\n",
        "def create_label_mapping(dataset_list):\n",
        "    \"\"\"\n",
        "    Tự động phát hiện tất cả các nhãn từ danh sách dataset và ánh xạ chúng thành số nguyên.\n",
        "    \"\"\"\n",
        "    all_labels = set()\n",
        "    for dataset in dataset_list:\n",
        "        all_labels.update(dataset[\"label\"])  # Tập hợp tất cả các nhãn từ dataset\n",
        "\n",
        "    label_to_int = {label: idx for idx, label in enumerate(sorted(all_labels))}\n",
        "    print(f\"Ánh xạ nhãn: {label_to_int}\")\n",
        "    return label_to_int\n",
        "\n",
        "# Hàm chuyển đổi nhãn\n",
        "def preprocess_labels(example, label_to_int):\n",
        "    example[\"label\"] = label_to_int.get(example[\"label\"], -1)  # Gán -1 cho nhãn không hợp lệ\n",
        "    return example\n",
        "\n",
        "# Tạo ánh xạ nhãn\n",
        "label_mapping = create_label_mapping([dataset])\n",
        "\n",
        "# Áp dụng chuyển đổi nhãn\n",
        "dataset = dataset.map(lambda example: preprocess_labels(example, label_mapping))\n",
        "\n",
        "# Kiểm tra kết quả\n",
        "print(dataset)\n",
        "\n",
        "# Truy cập mẫu cụ thể\n",
        "sample_dataset = dataset.select(range(10))  # Lấy 10 mẫu đầu tiên\n",
        "print(sample_dataset)\n",
        "\n",
        "# In thử 1 hàng trong sample_dataset\n",
        "print(\"First row in sample_dataset:\")\n",
        "print(sample_dataset[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "hy_jM2gOk99M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd52a839-85e6-46f6-fa90-1a93e0678c22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chia dataset: 84 mẫu train, 36 mẫu test\n",
            "Train dataset: Dataset({\n",
            "    features: ['input_ids', 'label'],\n",
            "    num_rows: 84\n",
            "})\n",
            "Test dataset: Dataset({\n",
            "    features: ['input_ids', 'label'],\n",
            "    num_rows: 36\n",
            "})\n",
            "Sample train dataset: Dataset({\n",
            "    features: ['input_ids', 'label'],\n",
            "    num_rows: 8\n",
            "})\n",
            "Sample test dataset: Dataset({\n",
            "    features: ['input_ids', 'label'],\n",
            "    num_rows: 5\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "def split_dataset(dataset, test_size=0.2, seed=42):\n",
        "    \"\"\"\n",
        "    Chia dataset thành tập train và test.\n",
        "\n",
        "    Args:\n",
        "        dataset (Dataset): Tập dữ liệu đầy đủ.\n",
        "        test_size (float): Tỷ lệ dữ liệu test (0.0 - 1.0).\n",
        "        seed (int): Seed để chia dữ liệu ngẫu nhiên.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (train_dataset, test_dataset) - Tập train và test.\n",
        "    \"\"\"\n",
        "    if not (0.0 < test_size < 1.0):\n",
        "        raise ValueError(\"test_size phải nằm trong khoảng (0.0, 1.0)\")\n",
        "    if len(dataset) < 2:\n",
        "        raise ValueError(\"Dataset phải có ít nhất 2 mẫu để chia.\")\n",
        "\n",
        "    train_test_split = dataset.train_test_split(test_size=test_size, seed=seed)\n",
        "    print(f\"Chia dataset: {len(train_test_split['train'])} mẫu train, {len(train_test_split['test'])} mẫu test\")\n",
        "    return train_test_split[\"train\"], train_test_split[\"test\"]\n",
        "\n",
        "# Chia dataset\n",
        "train_dataset, test_dataset = split_dataset(dataset, test_size=0.3)\n",
        "\n",
        "# Kiểm tra dữ liệu\n",
        "print(\"Train dataset:\", train_dataset)\n",
        "print(\"Test dataset:\", test_dataset)\n",
        "\n",
        "# Truy cập mẫu cụ thể\n",
        "sample_train_dataset = train_dataset.select(range(8))  # Lấy 10 mẫu đầu tiên từ train\n",
        "sample_test_dataset = test_dataset.select(range(5))    # Lấy 10 mẫu đầu tiên từ test\n",
        "\n",
        "print(\"Sample train dataset:\", sample_train_dataset)\n",
        "print(\"Sample test dataset:\", sample_test_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCzQIjl_ptbw"
      },
      "source": [
        "# 2. Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the number of unique labels\n",
        "print(label_mapping)\n",
        "number_label = len(label_mapping)\n",
        "print(\"Number of unique labels:\", number_label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24GkcP7XhXn3",
        "outputId": "f00b5b57-970b-4d1c-87ab-3c344d385244"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'intent_fallback': 0, 'intent_learn_more': 1, 'intent_negative': 2, 'intent_neutral': 3, 'intent_positive': 4, 'silence': 5}\n",
            "Number of unique labels: 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "bknqLH2piJMv"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Bước 2: Chuẩn bị tokenizer và token hóa dữ liệu\n",
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir = \"huggingface\")\n",
        "model = BERTIntentClassification(\n",
        "    model_name=model_name,\n",
        "    num_classes=6\n",
        ")\n",
        "model.freeze_bert() # Froze Layer BERT\n",
        "max_seq_length = 512\n",
        "\n",
        "\n",
        "def collate_fn(features):\n",
        "    inputs = []\n",
        "    labels = []\n",
        "    for element in features:\n",
        "        inputs.append(element.get(\"input_ids\"))\n",
        "        labels.append(element.get(\"label\"))\n",
        "\n",
        "    labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    token_inputs = tokenizer(\n",
        "        inputs,\n",
        "        add_special_tokens=True,\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=max_seq_length,\n",
        "        return_overflowing_tokens=False,\n",
        "        return_length=False,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "    token_inputs.update({\n",
        "        \"labels\": labels,\n",
        "    })\n",
        "    return token_inputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZldUk54pj1N"
      },
      "source": [
        "# 3. Train Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptK7Cy22p2GK"
      },
      "source": [
        "## 3.1 Log Wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "xZkR3vuFp5uN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45226005-b7c0-4709-cae9-1e2267acc719"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.19.2)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.25.5)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.10.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.19.2)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.12.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.12.14)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "qux2ABzMp7Ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "563a0769-2ed7-4054-9e78-1de06e07d40e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (1.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "ZZRspV7jp8OT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d20454b2-ccb6-4e1f-9f90-45196ec3351b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "c8767\n"
          ]
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "# Load biến môi trường từ file .env\n",
        "load_dotenv()\n",
        "\n",
        "# Lấy key từ biến môi trường\n",
        "wandb_api_key = os.getenv(\"WANDB_API_KEY\")\n",
        "print(wandb_api_key[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "215vQ7cOp9oo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15cb1fcf-018d-4756-e9c7-5b9f01517d73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "import wandb\n",
        "import os\n",
        "\n",
        "# Lấy API key từ biến môi trường và đăng nhập\n",
        "wandb.login(key=os.getenv(\"WANDB_API_KEY\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJG_rUYTqwLJ"
      },
      "source": [
        "Cách thiết lập thông qua TrainingArguments\n",
        "Khi sử dụng Trainer, bạn có thể đặt tên dự án trực tiếp trong TrainingArguments bằng cách sử dụng tham số report_to và run_name. Tuy nhiên, để đặt project, bạn cần khởi tạo một phiên wandb trước hoặc truyền cấu hình này thông qua wandb.init().\n",
        "\n",
        "Điều chỉnh TrainingArguments:\n",
        "```python\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results_\",          # Thư mục lưu kết quả\n",
        "    eval_strategy=\"epoch\",           # Đánh giá sau mỗi epoch\n",
        "    learning_rate=2e-4,\n",
        "    per_device_train_batch_size=128,\n",
        "    per_device_eval_batch_size=128,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",            # Thư mục lưu log\n",
        "    logging_strategy=\"steps\",        # Log theo steps\n",
        "    logging_steps=10,                # Log sau mỗi 10 bước\n",
        "    save_strategy=\"epoch\",           # Lưu checkpoint sau mỗi epoch\n",
        "    save_total_limit=3,              # Lưu tối đa 3 checkpoint\n",
        "    report_to=\"wandb\",               # Báo cáo log tới wandb\n",
        "    run_name=\"bert_run_1\"            # Tên phiên chạy trên wandb\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAMgPe3NqAhz"
      },
      "source": [
        "## 3.2 Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3hWcz751mEx"
      },
      "source": [
        "### Ver 1.2.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gs-Z20WNRnE"
      },
      "source": [
        "Dưới đây là bảng tóm tắt chi tiết cách lưu mô hình dựa trên chiến lược được đề xuất:\n",
        "\n",
        "| **Loại Model**    | **Điều Kiện Lưu**                                                                 | **Thư Mục Lưu Trên Local**       | **Số Lượng Lưu Trên Local**        | **Thông Tin Thêm**                              | **Đồng Bộ Lên WandB**                  |\n",
        "|--------------------|-----------------------------------------------------------------------------------|-----------------------------------|------------------------------------|-----------------------------------------------|-----------------------------------------|\n",
        "| **Best Model**     | Khi `eval_loss` giảm                                                             | `output_dir/best_model`           | Chỉ lưu một bản duy nhất           | Lưu thông tin `epoch` và `eval_loss`.          | Có: Artifact `best_model`. Thêm `epoch` và `loss` vào `metadata`. |\n",
        "| **Final Checkpoint** | Sau mỗi epoch (checkpoint cuối của epoch)                                        | `output_dir/checkpoint-epoch-<n>` | Tối đa 3 checkpoint gần nhất       | Không có thông tin đặc biệt.                   | Không đồng bộ (tránh trùng lặp dữ liệu lớn). |\n",
        "| **Custom Checkpoint** (tùy chọn) | Sau một số bước cố định hoặc mốc quan trọng (nếu cần thiết, ví dụ: mỗi 5 epoch) | Tùy chỉnh, ví dụ: `output_dir/checkpoint-step-<n>` | Theo ý muốn, hoặc không giới hạn | Thêm các mốc quan trọng để phân tích sau này. | Tùy chọn (không bắt buộc).              |\n",
        "\n",
        "---\n",
        "\n",
        "### **Chi tiết về bảng**\n",
        "1. **Best Model**:\n",
        "   - Điều kiện: `eval_loss` giảm.\n",
        "   - Chỉ lưu một phiên bản tốt nhất.\n",
        "   - Lưu thông tin epoch và loss để dễ dàng tham khảo hoặc tải xuống sau này.\n",
        "\n",
        "2. **Final Checkpoint**:\n",
        "   - Được lưu sau mỗi epoch.\n",
        "   - Giới hạn số lượng checkpoint lưu trên local để tiết kiệm bộ nhớ (ví dụ: tối đa 3 checkpoint).\n",
        "   - Không lưu thông tin thêm vào checkpoint.\n",
        "\n",
        "3. **Custom Checkpoint** (tùy chọn):\n",
        "   - Có thể sử dụng nếu bạn muốn lưu checkpoint tại các mốc thời gian cụ thể, chẳng hạn như mỗi 5 epoch hoặc sau một số bước huấn luyện (steps).\n",
        "   - Thích hợp khi bạn cần kiểm tra tiến độ huấn luyện chi tiết hơn hoặc muốn lưu backup.\n",
        "\n",
        "---\n",
        "\n",
        "### **Tóm tắt logic**\n",
        "- **Best Model**:\n",
        "  - Lưu vào thư mục cố định (`best_model`).\n",
        "  - Ghi đè khi có `eval_loss` mới tốt hơn.\n",
        "  - Đồng bộ lên WandB.\n",
        "\n",
        "- **Final Checkpoint**:\n",
        "  - Lưu sau mỗi epoch.\n",
        "  - Xóa checkpoint cũ nhất nếu vượt giới hạn `save_total_limit`.\n",
        "  - Không đồng bộ lên WandB (tránh lãng phí không gian lưu trữ).\n",
        "\n",
        "- **Custom Checkpoint**:\n",
        "  - Tùy chọn nếu bạn cần lưu thêm để phục vụ các mục đích cụ thể.\n",
        "\n",
        "Nếu bạn cần thêm bất kỳ chi tiết nào khác, hãy cho mình biết nhé! 😊"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCbkX1KHeT0j"
      },
      "source": [
        "### **Bảng Tóm Tắt: Lưu Best Model và Last Model**\n",
        "\n",
        "| **Loại Model**    | **Khi Nào Cần Lưu**                                                                                         | **Ưu Điểm**                                                                                       | **Hạn Chế**                                                                                      |\n",
        "|--------------------|------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------|\n",
        "| **Best Model**     | - Khi muốn triển khai mô hình tốt nhất với `eval_loss` thấp nhất hoặc `accuracy` cao nhất.                   | - Đảm bảo lưu lại mô hình có hiệu suất tốt nhất trên tập validation.<br>- Phù hợp để triển khai.   | - Không lưu trạng thái đầy đủ (optimizer, scheduler).<br>- Không tiếp tục huấn luyện từ trạng thái này. |\n",
        "| **Last Model**     | - Khi cần tiếp tục huấn luyện (fine-tuning) hoặc khôi phục trạng thái sau khi huấn luyện kết thúc.         | - Lưu đầy đủ trạng thái (weights, optimizer, scheduler).<br>- Phù hợp để tiếp tục huấn luyện.    | - Có thể không phải là mô hình tốt nhất (do overfitting hoặc underfitting).                     |\n",
        "| **Chỉ Lưu Best**   | - Khi chỉ quan tâm đến triển khai mô hình tốt nhất, không cần tiếp tục huấn luyện sau này.                  | - Tiết kiệm tài nguyên lưu trữ.<br>- Tập trung vào mô hình tối ưu cho triển khai.                | - Không thể tiếp tục huấn luyện nếu cần.                                                         |\n",
        "| **Chỉ Lưu Last**   | - Khi muốn đảm bảo khả năng khôi phục trạng thái để tiếp tục huấn luyện.                                    | - Khôi phục hoàn toàn quá trình huấn luyện.<br>- Phù hợp cho fine-tuning hoặc thử nghiệm sau này. | - Không đảm bảo đây là mô hình tốt nhất để triển khai.                                           |\n",
        "| **Lưu Cả Hai**     | - Khi cần cả triển khai mô hình tốt nhất và tiếp tục huấn luyện sau này.                                    | - Kết hợp ưu điểm của cả Best Model và Last Model.<br>- Linh hoạt trong sử dụng.                 | - Tốn thêm tài nguyên lưu trữ và thời gian.                                                     |\n",
        "\n",
        "---\n",
        "\n",
        "### **Chiến Lược Tối Ưu**\n",
        "| **Loại Lưu** | **Tần Suất**                          | **Chiến Lược**                                                                                             |\n",
        "|--------------|---------------------------------------|-----------------------------------------------------------------------------------------------------------|\n",
        "| **Best Model** | Khi `eval_loss` giảm                 | Lưu mỗi lần `eval_loss` giảm để đảm bảo mô hình tốt nhất luôn được lưu.                                    |\n",
        "| **Last Model** | Sau khi huấn luyện kết thúc          | Lưu trạng thái cuối cùng của quá trình huấn luyện (weights + optimizer + scheduler).                      |\n",
        "| **Kết hợp**   | Best Model: Mỗi khi `eval_loss` giảm<br>Last Model: Sau khi kết thúc | Lưu cả Best Model để triển khai và Last Model để tiếp tục huấn luyện khi cần thiết.                      |\n",
        "\n",
        "---\n",
        "\n",
        "### **Lựa Chọn Phù Hợp**\n",
        "- **Dự án triển khai mô hình nhanh**: Lưu **Best Model**.\n",
        "- **Dự án nghiên cứu hoặc fine-tuning tiếp**: Lưu **Last Model**.\n",
        "- **Dự án quy mô lớn, cần cả triển khai và mở rộng**: Lưu **cả hai**.\n",
        "\n",
        "Hãy chọn chiến lược lưu phù hợp với mục tiêu dự án của bạn! 🚀"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLIKjrdaesyG"
      },
      "source": [
        "Thui, ko lưu local nữa, lưu tất trên wandb đi.\n",
        "- Với best model: lưu lên wandb khi loss giảm và đã sau 10 epochs  \n",
        "(Lưu Best Model ngay khi eval_loss giảm ở local, sau 10 epochs thì đồng bộ cái best lên wandb, sau đó xoá các file best ở local).\n",
        "Chỉ đồng bộ lên WandB mỗi 10 epochs.)\n",
        "- Với last model: lưu lên wandb sau mỗi 10 epochs. (lưu local trước -> đồng bộ lên wandb sẽ xoá file local)\n",
        "+, Trong quá trình lưu thì việc training vẫn diễn ra Parallel\n",
        "\n",
        "đều lưu đầy đủ toàn bộ tham số để có thể train thêm từ cả ở best model và last model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "bHYLAOxP3Lea"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "I5n7vK-a9_gM"
      },
      "outputs": [],
      "source": [
        "# class TrainerCustom(Trainer):\n",
        "\n",
        "#     def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "#         \"\"\"\n",
        "#         How the loss is computed by Trainer. By default, all models return the loss in the first element.\n",
        "\n",
        "#         Subclass and override for custom behavior.\n",
        "#         \"\"\"\n",
        "#         if \"labels\" in inputs:\n",
        "#             labels = inputs.pop(\"labels\")\n",
        "#         else:\n",
        "#             labels = None\n",
        "\n",
        "#         # Sử dụng nn.CrossEntropyLoss() thay vì nn.CrossEntropy\n",
        "#         cross_entropy_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "#         # Chạy mô hình và nhận đầu ra (logits)\n",
        "#         outputs = model(**inputs)\n",
        "\n",
        "#         # Đảm bảo lấy logits từ outputs (mô hình trả về tuple, lấy phần tử đầu tiên là logits)\n",
        "#         logits = outputs\n",
        "\n",
        "#         if labels is None:\n",
        "#             print(\"Labels are None during compute_loss.\")\n",
        "#         if logits is None:\n",
        "#             print(\"Logits are None during compute_loss.\")\n",
        "\n",
        "#         # Tính toán loss\n",
        "#         loss = cross_entropy_loss(logits, labels)\n",
        "\n",
        "#         # Trả về loss và outputs nếu cần\n",
        "#         return (loss, outputs) if return_outputs else loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "iYRbPjlfDSTs"
      },
      "outputs": [],
      "source": [
        "\n",
        "# import wandb\n",
        "\n",
        "# # Khởi tạo wandb\n",
        "# wandb.init(\n",
        "#     project=\"bert-intent-classification\",  # Tên dự án\n",
        "#     name=\"bert_run_3\"                     # Tên phiên chạy\n",
        "# )\n",
        "\n",
        "\n",
        "# # Bước 6: Cài đặt tham số huấn luyện\n",
        "# training_args = TrainingArguments(\n",
        "#     output_dir=\"./result__s\",          # Thư mục lưu kết quả\n",
        "#     eval_strategy=\"epoch\",    # Đánh giá sau mỗi epoch\n",
        "#     learning_rate=2e-4,\n",
        "#     per_device_train_batch_size=128,\n",
        "#     per_device_eval_batch_size=128,\n",
        "#     num_train_epochs=50,\n",
        "#     weight_decay=0.01,\n",
        "#     logging_dir=\"./logs\",\n",
        "#     logging_strategy=\"steps\",\n",
        "#     logging_steps=1,  # Ghi logs mỗi 500 bước huấn luyện\n",
        "#     save_strategy=\"no\",          # Lưu trọng số sau mỗi epoch\n",
        "#     save_total_limit=3,\n",
        "#     label_names = [\"labels\"],\n",
        "#     report_to=\"wandb\",\n",
        "#     run_name=\"bert_run_3\"\n",
        "# )\n",
        "\n",
        "\n",
        "# batch = collate_fn([sample_test_dataset[0]]) # Tạo một batch từ một mẫu đơn lẻ (sample_test_dataset[0]) để kiểm tra xem hàm collate_fn có hoạt động đúng không.\n",
        "# print(batch)\n",
        "\n",
        "# # metrics = trainer.evaluate()\n",
        "# # Mục đích: Chạy giai đoạn evaluation (đánh giá) trên eval_dataset (sample_test_dataset) và tính toán các metrics như:\n",
        "# trainer = TrainerCustom(\n",
        "#     model=model,\n",
        "#     args=training_args,\n",
        "#     train_dataset=sample_train_dataset,\n",
        "#     eval_dataset=sample_test_dataset,\n",
        "#     tokenizer=tokenizer,\n",
        "#     data_collator=collate_fn,\n",
        "# )\n",
        "\n",
        "# metrics = trainer.evaluate()\n",
        "# print(metrics)  # Kiểm tra xem có \"eval_loss\" hay không\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # Bước 7: Tạo Trainer\n",
        "# trainer = TrainerCustom(\n",
        "#     model=model,\n",
        "#     args=training_args,\n",
        "#     train_dataset=sample_train_dataset,\n",
        "#     eval_dataset=sample_test_dataset,\n",
        "#     tokenizer=tokenizer,\n",
        "#     data_collator = collate_fn,\n",
        "# )\n",
        "\n",
        "# # Bước 8: Huấn luyện\n",
        "# trainer.train()\n",
        "\n",
        "# # Kết thúc phiên wandb\n",
        "# wandb.finish()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Vtlo08BaWQhe"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import wandb\n",
        "import os\n",
        "import shutil\n",
        "import time\n",
        "\n",
        "class TrainerCustom(Trainer):\n",
        "    def __init__(self, *args, save_every_n_epochs=10, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        if torch.cuda.is_available():\n",
        "            print(f\"Trainer is running on GPU: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
        "        else:\n",
        "            print(\"Trainer is running on CPU.\")\n",
        "\n",
        "        self.best_eval_loss = float(\"inf\")  # Giá trị loss tốt nhất ban đầu\n",
        "        self.save_every_n_epochs = save_every_n_epochs  # Tần suất lưu lên WandB\n",
        "        self.best_model_info = {\"epoch\": None, \"loss\": None}\n",
        "        self.last_saved_epoch = 0  # Epoch cuối cùng đã lưu Best Model và Last Model\n",
        "        self.executor = ThreadPoolExecutor(max_workers=3)  # Cho phép tối đa 2 luồng song song\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        \"\"\"\n",
        "        How the loss is computed by Trainer. By default, all models return the loss in the first element.\n",
        "\n",
        "        Subclass and override for custom behavior.\n",
        "        \"\"\"\n",
        "\n",
        "        # # Kiểm tra thiết bị của mô hình và dữ liệu\n",
        "        # print(\"Model device:\", next(model.parameters()).device)\n",
        "        # print(\"Input device:\", inputs[\"input_ids\"].device)\n",
        "        if \"labels\" in inputs:\n",
        "            labels = inputs.pop(\"labels\")\n",
        "        else:\n",
        "            labels = None\n",
        "\n",
        "        # Sử dụng nn.CrossEntropyLoss() thay vì nn.CrossEntropy\n",
        "        cross_entropy_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "        # Chạy mô hình và nhận đầu ra (logits)\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "        # Đảm bảo lấy logits từ outputs (mô hình trả về tuple, lấy phần tử đầu tiên là logits)\n",
        "        logits = outputs\n",
        "\n",
        "        if labels is None:\n",
        "            print(\"Labels are None during compute_loss.\")\n",
        "        if logits is None:\n",
        "            print(\"Logits are None during compute_loss.\")\n",
        "\n",
        "        # Tính toán loss\n",
        "        loss = cross_entropy_loss(logits, labels)\n",
        "\n",
        "        # Trả về loss và outputs nếu cần\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "    def async_save_model(self, model_dir, artifact_name, metadata=None):\n",
        "        \"\"\"\n",
        "        Lưu mô hình vào local và đồng bộ lên WandB trong luồng song song.\n",
        "        \"\"\"\n",
        "        def save():\n",
        "            start_time = time.time()\n",
        "            try:\n",
        "                # Xóa tất cả các thư mục tmp_best_model_ trước đó\n",
        "                for folder in os.listdir(\".\"):\n",
        "                    if folder.startswith(\"tmp_best_model_epoch_\") and folder != model_dir:\n",
        "                        shutil.rmtree(folder, ignore_errors=True)\n",
        "                        print(f\"Removed old temporary directory: {folder}\")\n",
        "\n",
        "                # Lưu mô hình vào thư mục tạm\n",
        "                self.save_model(model_dir)\n",
        "\n",
        "                # Đồng bộ lên WandB\n",
        "                artifact = wandb.Artifact(artifact_name, type=\"model\")\n",
        "                artifact.add_dir(model_dir)\n",
        "                if metadata:\n",
        "                    artifact.metadata = metadata\n",
        "                wandb.log_artifact(artifact)\n",
        "            except Exception as e:\n",
        "                print(f\"Error during saving or syncing model {artifact_name}: {e}\")\n",
        "            finally:\n",
        "                # Xóa thư mục tạm hiện tại sau khi đồng bộ\n",
        "                try:\n",
        "                    shutil.rmtree(model_dir, ignore_errors=True)\n",
        "                    print(f\"Successfully removed temporary directory: {model_dir}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error removing temporary directory {model_dir}: {e}\")\n",
        "\n",
        "            elapsed_time = time.time() - start_time\n",
        "            print(f\"Model saved and uploaded to WandB: {artifact_name} in {elapsed_time:.2f} seconds\")\n",
        "\n",
        "        self.executor.submit(save)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def evaluate(self, eval_dataset=None, ignore_keys=None, metric_key_prefix: str = \"eval\"):\n",
        "        metrics = super().evaluate(eval_dataset, ignore_keys, metric_key_prefix)\n",
        "        eval_loss = metrics.get(\"eval_loss\")\n",
        "\n",
        "        # Cập nhật Best Model nếu eval_loss giảm\n",
        "        # Lưu Best Model ngay khi eval_loss giảm (local).\n",
        "        # Chỉ đồng bộ lên WandB mỗi 10 epochs.\n",
        "\n",
        "        if eval_loss is not None and eval_loss < self.best_eval_loss:\n",
        "            print(f\"New best eval_loss: {eval_loss}\")\n",
        "            self.best_eval_loss = eval_loss\n",
        "            self.best_model_info = {\"epoch\": self.state.epoch, \"loss\": eval_loss}\n",
        "\n",
        "            # Log thông tin Best Model lên WandB\n",
        "            wandb.log({\n",
        "                \"best_eval_loss\": self.best_eval_loss,\n",
        "                \"best_model_epoch\": self.best_model_info.get(\"epoch\", -1)\n",
        "            })\n",
        "\n",
        "            # Lưu Best Model vào thư mục tạm (local)\n",
        "            best_model_dir = f\"./tmp_best_model_epoch_{int(self.state.epoch)}\"\n",
        "            self.save_model(best_model_dir)\n",
        "\n",
        "            # Đồng bộ lên WandB mỗi 10 epochs\n",
        "            if int(self.state.epoch) % self.save_every_n_epochs == 0:\n",
        "                artifact_name = f\"best_model_epoch_{int(self.state.epoch)}\"\n",
        "                self.async_save_model(best_model_dir, artifact_name, self.best_model_info)\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def save_last_model(self):\n",
        "        \"\"\"\n",
        "        Lưu Last Model lên WandB sau mỗi N epochs.\n",
        "        \"\"\"\n",
        "        if int(self.state.epoch) % self.save_every_n_epochs == 0 and int(self.state.epoch) != self.last_saved_epoch:\n",
        "            print(f\"Saving Last Model at epoch {self.state.epoch} to WandB...\")\n",
        "            last_model_dir = f\"./tmp_last_model_epoch_{int(self.state.epoch)}\"\n",
        "            artifact_name = f\"last_model_epoch_{int(self.state.epoch)}\"\n",
        "            self.async_save_model(last_model_dir, artifact_name)\n",
        "\n",
        "            # Log thông tin Last Model lên WandB\n",
        "            wandb.log({\n",
        "                \"last_model_epoch\": self.state.epoch\n",
        "            })\n",
        "\n",
        "            # Cập nhật epoch cuối cùng đã lưu\n",
        "            self.last_saved_epoch = int(self.state.epoch)\n",
        "\n",
        "    def train(self, *args, **kwargs):\n",
        "        result = super().train(*args, **kwargs)\n",
        "\n",
        "        # Sau mỗi epoch, lưu Last Model lên WandB\n",
        "        self.save_last_model()\n",
        "        # Chờ tất cả các luồng lưu hoàn thành trước khi kết thúc\n",
        "        self.executor.shutdown(wait=True)\n",
        "\n",
        "        return result\n",
        "\n",
        "\n",
        "# Bước 6: Cài đặt tham số huấn luyện\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./result__s\",          # Thư mục lưu kết quả\n",
        "    eval_strategy=\"epoch\",    # Đánh giá sau mỗi epoch\n",
        "    learning_rate=2e-4,\n",
        "    per_device_train_batch_size=128,\n",
        "    per_device_eval_batch_size=128,\n",
        "    num_train_epochs=30,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=1,  # Ghi logs mỗi 500 bước huấn luyện\n",
        "    save_strategy=\"no\",          # Lưu trọng số sau mỗi epoch\n",
        "    save_total_limit=3,\n",
        "    label_names = [\"labels\"],\n",
        "    report_to=\"wandb\",\n",
        "    run_name=\"bert_run_3\"\n",
        ")\n",
        "\n",
        "\n",
        "import wandb\n",
        "\n",
        "# Khởi tạo wandb\n",
        "wandb.init(\n",
        "    project=\"bert-intent-classification\",  # Tên dự án\n",
        "    name=\"bert_run_3\",                     # Tên phiên chạy\n",
        "    config={\"gpu\": torch.cuda.get_device_name(torch.cuda.current_device()) if torch.cuda.is_available() else \"CPU\"}\n",
        ")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"Trainer is running on GPU: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
        "else:\n",
        "    print(\"Trainer is running on CPU.\")\n",
        "\n",
        "trainer = TrainerCustom(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=sample_train_dataset,\n",
        "    eval_dataset=sample_test_dataset,\n",
        "    data_collator=collate_fn,\n",
        "    save_every_n_epochs=10  # Lưu Best Model và Last Model mỗi 10 epochs\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "\n",
        "wandb.finish()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7qjF1K2EW0Ys",
        "outputId": "113c5363-0b74-4270-8980-197c338b010d"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainer is running on GPU: Tesla T4\n",
            "Trainer is running on GPU: Tesla T4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [30/30 00:11, Epoch 30/30]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.092300</td>\n",
              "      <td>3.176852</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.107200</td>\n",
              "      <td>3.214802</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.183200</td>\n",
              "      <td>3.159883</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.185100</td>\n",
              "      <td>3.071618</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.129700</td>\n",
              "      <td>3.109453</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.106400</td>\n",
              "      <td>3.178693</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.099900</td>\n",
              "      <td>3.318820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.107800</td>\n",
              "      <td>3.426761</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.098300</td>\n",
              "      <td>3.460075</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.065500</td>\n",
              "      <td>3.463956</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.066600</td>\n",
              "      <td>3.450267</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.052800</td>\n",
              "      <td>3.395835</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.053700</td>\n",
              "      <td>3.341796</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.044200</td>\n",
              "      <td>3.329903</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.069600</td>\n",
              "      <td>3.304676</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.060800</td>\n",
              "      <td>3.334413</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.039600</td>\n",
              "      <td>3.418310</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.045700</td>\n",
              "      <td>3.493579</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.043600</td>\n",
              "      <td>3.553579</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.022000</td>\n",
              "      <td>3.585017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.030500</td>\n",
              "      <td>3.602208</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.023800</td>\n",
              "      <td>3.615718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.028400</td>\n",
              "      <td>3.625151</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.025800</td>\n",
              "      <td>3.630121</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.019000</td>\n",
              "      <td>3.632206</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.043800</td>\n",
              "      <td>3.632945</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.019000</td>\n",
              "      <td>3.635459</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.026000</td>\n",
              "      <td>3.637681</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.024200</td>\n",
              "      <td>3.638479</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.025800</td>\n",
              "      <td>3.638705</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New best eval_loss: 3.1768524646759033\n",
            "New best eval_loss: 3.1598830223083496\n",
            "New best eval_loss: 3.071617841720581\n",
            "Saving Last Model at epoch 30.0 to WandB...\n",
            "Removed old temporary directory: tmp_best_model_epoch_4\n",
            "Removed old temporary directory: tmp_best_model_epoch_1\n",
            "Removed old temporary directory: tmp_best_model_epoch_3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./tmp_last_model_epoch_30)... Done. 2.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully removed temporary directory: ./tmp_last_model_epoch_30\n",
            "Model saved and uploaded to WandB: last_model_epoch_30 in 7.39 seconds\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_eval_loss</td><td>▄█▇▁</td></tr><tr><td>best_model_epoch</td><td>▁▆█</td></tr><tr><td>eval/loss</td><td>▂▂▃▂▁▁▂▄▅▆▆▆▅▄▄▄▄▅▆▇▇██████████</td></tr><tr><td>eval/model_preparation_time</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▅▁▂▃▅▇▃▆▇█▇▅▆▆▆▅▅▄▄▅▄▁▁▁▁▁▁▁▁▂▁</td></tr><tr><td>eval/samples_per_second</td><td>▃▇▅▄▂▁▄▂▁▁▁▂▂▂▂▂▂▃▃▃▃▇▇▇▇████▅▇</td></tr><tr><td>eval/steps_per_second</td><td>▃▇▅▄▂▁▄▂▁▁▁▂▂▂▂▂▂▃▃▃▃▇▇▇▇████▅▇</td></tr><tr><td>last_model_epoch</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇█████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>▂▄▆█▅▄▃▆▅▂▃▃▂▂▄▃▁▂▃▁▁▂▂▂▂▄▁▁▂▂</td></tr><tr><td>train/learning_rate</td><td>███▇▇▇▇▆▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>▄▅██▆▅▄▅▄▃▃▂▂▂▃▃▂▂▂▁▁▁▁▁▁▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_eval_loss</td><td>3.07162</td></tr><tr><td>best_model_epoch</td><td>4</td></tr><tr><td>eval/loss</td><td>3.6387</td></tr><tr><td>eval/model_preparation_time</td><td>0.0106</td></tr><tr><td>eval/runtime</td><td>0.0273</td></tr><tr><td>eval/samples_per_second</td><td>183.007</td></tr><tr><td>eval/steps_per_second</td><td>36.601</td></tr><tr><td>last_model_epoch</td><td>30</td></tr><tr><td>total_flos</td><td>0</td></tr><tr><td>train/epoch</td><td>30</td></tr><tr><td>train/global_step</td><td>30</td></tr><tr><td>train/grad_norm</td><td>1.03833</td></tr><tr><td>train/learning_rate</td><td>0</td></tr><tr><td>train/loss</td><td>0.0258</td></tr><tr><td>train_loss</td><td>0.06467</td></tr><tr><td>train_runtime</td><td>11.0985</td></tr><tr><td>train_samples_per_second</td><td>21.625</td></tr><tr><td>train_steps_per_second</td><td>2.703</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">bert_run_3</strong> at: <a href='https://wandb.ai/doanngoccuong_nh/bert-intent-classification/runs/3p1ndst1' target=\"_blank\">https://wandb.ai/doanngoccuong_nh/bert-intent-classification/runs/3p1ndst1</a><br> View project at: <a href='https://wandb.ai/doanngoccuong_nh/bert-intent-classification' target=\"_blank\">https://wandb.ai/doanngoccuong_nh/bert-intent-classification</a><br>Synced 5 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250113_164136-3p1ndst1/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Đúng vậy, trong đoạn mã bạn cung cấp, `trainer.evaluate()` được thực hiện một cách tự động bởi lớp `Trainer` trong thư viện `transformers`. Cụ thể:\n",
        "\n",
        "### Trong TrainingArguments:\n",
        "```python\n",
        "training_args = TrainingArguments(\n",
        "    ...\n",
        "    eval_strategy=\"epoch\",  # Đánh giá sau mỗi epoch\n",
        "    ...\n",
        ")\n",
        "```\n",
        "**`eval_strategy=\"epoch\"`** có nghĩa là quá trình đánh giá (evaluation) sẽ tự động được thực hiện sau mỗi epoch, sử dụng `eval_dataset` mà bạn đã cung cấp trong `TrainerCustom`.\n",
        "\n",
        "### Trong `TrainerCustom`:\n",
        "Trong lớp `TrainerCustom`, phương thức `evaluate()` đã được override. Bên trong, nó:\n",
        "1. Gọi phương thức `super().evaluate()` từ lớp cha `Trainer`, thực hiện việc tính toán loss và các metric.\n",
        "2. Lưu thông tin về Best Model nếu phát hiện `eval_loss` giảm so với trước đó.\n",
        "3. Ghi log kết quả lên WandB.\n",
        "\n",
        "Vì vậy, trong khi huấn luyện (`trainer.train()`), `trainer.evaluate()` được gọi tự động sau mỗi epoch để thực hiện đánh giá và lưu Best Model.\n",
        "\n",
        "---\n",
        "\n",
        "### Kết luận:\n",
        "Bạn không cần gọi riêng `trainer.evaluate()` trong lúc training nếu đã cấu hình `eval_strategy=\"epoch\"`. Tuy nhiên, nếu bạn muốn đánh giá mô hình ở một thời điểm cụ thể ngoài quá trình training (ví dụ, sau khi huấn luyện xong), bạn vẫn có thể gọi `trainer.evaluate()` thủ công."
      ],
      "metadata": {
        "id": "AzcBU9s0jYby"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Bước 9: Đánh giá trên tập kiểm tra\n",
        "# trainer.evaluate()"
      ],
      "metadata": {
        "id": "rC3AmIe0T9co",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        },
        "outputId": "f55316f7-d2ad-4261-a3dd-952c55c1de33"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250113_164136-3p1ndst1</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/doanngoccuong_nh/bert-intent-classification/runs/3p1ndst1' target=\"_blank\">bert_run_3</a></strong> to <a href='https://wandb.ai/doanngoccuong_nh/bert-intent-classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/doanngoccuong_nh/bert-intent-classification' target=\"_blank\">https://wandb.ai/doanngoccuong_nh/bert-intent-classification</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/doanngoccuong_nh/bert-intent-classification/runs/3p1ndst1' target=\"_blank\">https://wandb.ai/doanngoccuong_nh/bert-intent-classification/runs/3p1ndst1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainer is running on GPU: Tesla T4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1/1 : < :]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New best eval_loss: 3.1156165599823\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "int() argument must be a string, a bytes-like object or a real number, not 'NoneType'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-02f5adf0897e>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Evaluate on the test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Finish the WandB session\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-50-5851ae42c571>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;31m# Lưu Best Model vào thư mục tạm (local)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0mbest_model_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"./tmp_best_model_epoch_{int(self.state.epoch)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiq5bRFTmzv5"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "import wandb\n",
        "\n",
        "# 1. Tải mô hình từ artifact trên WandB\n",
        "run = wandb.init(project=\"bert-intent-classification\")  # Tên dự án trong WandB\n",
        "artifact = run.use_artifact('doanngoccuong_nh/bert-intent-classification/last_model_epoch_30:v0', type='model')\n",
        "artifact_dir = artifact.download()\n",
        "\n",
        "# Tải mô hình đã lưu và tokenizer\n",
        "model_path = artifact_dir  # Đường dẫn đến mô hình đã tải\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = torch.load(f\"{model_path}/pytorch_model.bin\")  # Tải mô hình\n",
        "\n",
        "# Chuyển mô hình sang chế độ đánh giá\n",
        "model.eval()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "print(f\"Model loaded and running on device: {device}\")\n",
        "\n",
        "# 2. Xử lý đầu vào\n",
        "sentence = \"What is the weather like today?\"\n",
        "inputs = tokenizer(\n",
        "    sentence,\n",
        "    return_tensors=\"pt\",\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "    max_length=512\n",
        ")\n",
        "\n",
        "# Chuyển đầu vào sang thiết bị phù hợp\n",
        "inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "# 3. Thực hiện dự đoán\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)  # Truyền đầu vào qua mô hình\n",
        "    logits = outputs.logits  # Lấy logits từ đầu ra của mô hình\n",
        "    predicted_class = torch.argmax(logits, dim=1).item()  # Lấy nhãn dự đoán\n",
        "\n",
        "# 4. Mapping nhãn dự đoán sang tên nhãn\n",
        "label_mapping = {0: \"intent_positive\", 1: \"intent_negative\", 2: \"intent_neutral\", 3: \"intent_fallback\", 4: \"silence\"}\n",
        "predicted_label = label_mapping.get(predicted_class, \"Unknown\")\n",
        "\n",
        "# 5. In kết quả dự đoán\n",
        "print(f\"Input sentence: {sentence}\")\n",
        "print(f\"Predicted class ID: {predicted_class}\")\n",
        "print(f\"Predicted label: {predicted_label}\")\n",
        "\n",
        "# Kết thúc phiên WandB\n",
        "wandb.finish()\n"
      ],
      "metadata": {
        "id": "am0boXLrUCXG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 686
        },
        "outputId": "3f81e179-d0c1-4492-eaf2-e9b96463add6"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250113_165003-edtl0kba</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/doanngoccuong_nh/bert-intent-classification/runs/edtl0kba' target=\"_blank\">graceful-serenity-58</a></strong> to <a href='https://wandb.ai/doanngoccuong_nh/bert-intent-classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/doanngoccuong_nh/bert-intent-classification' target=\"_blank\">https://wandb.ai/doanngoccuong_nh/bert-intent-classification</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/doanngoccuong_nh/bert-intent-classification/runs/edtl0kba' target=\"_blank\">https://wandb.ai/doanngoccuong_nh/bert-intent-classification/runs/edtl0kba</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact last_model_epoch_30:v0, 419.95MB. 2 files... \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   2 of 2 files downloaded.  \n",
            "Done. 0:0:1.7\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Unrecognized model in /content/artifacts/last_model_epoch_30:v0. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, align, altclip, audio-spectrogram-transformer, autoformer, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, blip-2, bloom, bridgetower, bros, camembert, canine, chameleon, chinese_clip, chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, clipseg, clvp, code_llama, codegen, cohere, conditional_detr, convbert, convnext, convnextv2, cpmant, ctrl, cvt, dac, data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, decision_transformer, deformable_detr, deit, depth_anything, deta, detr, dinat, dinov2, distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, encodec, encoder-decoder, ernie, ernie_m, esm, falcon, falcon_mamba, fastspeech2_conformer, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, gemma, gemma2, git, glm, glpn, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, granite, granitemoe, graphormer, grounding-dino, groupvit, hiera, hubert, ibert, idefics, idefics2, idefics3, ijepa, imagegpt, informer, instructblip, instructblipvideo, jamba, jetmoe, jukebox, kosmos-2, layoutlm, layoutlmv2, layoutlmv3, led, levit, lilt, llama, llava, llava_next, llava_next_video, llava_onevision, longformer, longt5, luke, lxmert, m2m_100, mamba, mamba2, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mimi, mistral, mixtral, mllama, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, moshi, mpnet, mpt, mra, mt5, musicgen, musicgen_melody, mvp, nat, nemotron, nezha, nllb-moe, nougat, nystromformer, olmo, olmo2, olmoe, omdet-turbo, oneformer, open-llama, openai-gpt, opt, owlv2, owlvit, paligemma, patchtsmixer, patchtst, pegasus, pegasus_x, perceiver, persimmon, phi, phi3, phimoe, pix2struct, pixtral, plbart, poolformer, pop2piano, prophetnet, pvt, pvt_v2, qdqbert, qwen2, qwen2_audio, qwen2_audio_encoder, qwen2_moe, qwen2_vl, rag, realm, recurrent_gemma, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rt_detr, rt_detr_resnet, rwkv, sam, seamless_m4t, seamless_m4t_v2, segformer, seggpt, sew, sew-d, siglip, siglip_vision_model, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, stablelm, starcoder2, superpoint, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, table-transformer, tapas, time_series_transformer, timesformer, timm_backbone, trajectory_transformer, transfo-xl, trocr, tvlt, tvp, udop, umt5, unispeech, unispeech-sat, univnet, upernet, van, video_llava, videomae, vilt, vipllava, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vits, vivit, wav2vec2, wav2vec2-bert, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xmod, yolos, yoso, zamba, zoedepth",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-68bab0dd3567>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Tải mô hình đã lưu và tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0martifact_dir\u001b[0m  \u001b[0;31m# Đường dẫn đến mô hình đã tải\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{model_path}/pytorch_model.bin\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Tải mô hình\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    876\u001b[0m                     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m                     config = AutoConfig.from_pretrained(\n\u001b[0m\u001b[1;32m    879\u001b[0m                         \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m                     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1051\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mCONFIG_MAPPING\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1053\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   1054\u001b[0m             \u001b[0;34mf\"Unrecognized model in {pretrained_model_name_or_path}. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m             \u001b[0;34mf\"Should have a `model_type` key in its {CONFIG_NAME}, or contain one of the following strings \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Unrecognized model in /content/artifacts/last_model_epoch_30:v0. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, align, altclip, audio-spectrogram-transformer, autoformer, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, blip-2, bloom, bridgetower, bros, camembert, canine, chameleon, chinese_clip, chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, clipseg, clvp, code_llama, codegen, cohere, conditional_detr, convbert, convnext, convnextv2, cpmant, ctrl, cvt, dac, data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, decision_transformer, deformable_detr, deit, depth_anything, deta, detr, dinat, dinov2, distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, encodec, encoder-decoder, ernie, ernie_m, esm, falcon, falcon_mamba, fastspeech2_conformer, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, gemma, gemma2, git, glm, glpn, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, granite, granitemoe, graphormer, grounding-dino, groupvit, hiera, hubert, ibert, idefics, idefics2, idefics3, ijepa, imagegpt, informer, instructblip, instructblipvideo, jamba, jetmoe, jukebox, kosmos-2, layoutlm, layoutlmv2, layoutlmv3, led, levit, lilt, llama, llava, llava_next, llava_next_video, llava_onevision, longformer, longt5, luke, lxmer..."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "print(os.listdir(artifact_dir))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSvcjYuTk2T7",
        "outputId": "af384a35-4b20-422d-a26f-f8cb9baf289f"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['model.safetensors', 'training_args.bin']\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "cdfcfcb4a9f349609fc9f52ec0c872c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_eab882737bb74a35ad810005945a8aa9",
              "IPY_MODEL_6acc8d453c9f48549501307caeb745ab",
              "IPY_MODEL_86c4ea6bc9a346d29ff39eea3dd62ca7"
            ],
            "layout": "IPY_MODEL_eded7dc6494e48d2831ff22fafdb505d"
          }
        },
        "eab882737bb74a35ad810005945a8aa9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b11260de16e4993b64673de5ae464f4",
            "placeholder": "​",
            "style": "IPY_MODEL_3779f947e6f643e78b7f8fca00cadb9b",
            "value": "Map: 100%"
          }
        },
        "6acc8d453c9f48549501307caeb745ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bd67655dc69a4b05920866cd9058cc84",
            "max": 120,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e1a93c86301f4b1ea17a4f58671c904d",
            "value": 120
          }
        },
        "86c4ea6bc9a346d29ff39eea3dd62ca7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aeca87675938457caec953fdac04bf77",
            "placeholder": "​",
            "style": "IPY_MODEL_8f28603486904ae8a7f8cb4db0651d6d",
            "value": " 120/120 [00:00&lt;00:00, 2685.59 examples/s]"
          }
        },
        "eded7dc6494e48d2831ff22fafdb505d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b11260de16e4993b64673de5ae464f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3779f947e6f643e78b7f8fca00cadb9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bd67655dc69a4b05920866cd9058cc84": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1a93c86301f4b1ea17a4f58671c904d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "aeca87675938457caec953fdac04bf77": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f28603486904ae8a7f8cb4db0651d6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}