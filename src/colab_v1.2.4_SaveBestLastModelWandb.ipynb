{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLWoGrdRXTSg"
      },
      "source": [
        "## 1. Tải Dữ Liệu từ CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbMKcafkKCPk",
        "outputId": "5883c418-e437-4692-b33c-901e0fb32d78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.11)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.27.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zIVYywvyiq2L"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, TrainingArguments, Trainer, AutoModel\n",
        "import numpy as np\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "import torch.nn as nn\n",
        "import os\n",
        "from typing import List\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\" ## Setup CUDA GPU 1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Kiểm tra GPU khả dụng\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"Number of GPUs:\", torch.cuda.device_count())\n",
        "if torch.cuda.is_available():\n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
        "else:\n",
        "    print(\"No GPU found.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SHnftkneP6B",
        "outputId": "8e2ddfd2-dd72-411e-c1ef-54308c1c9008"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: True\n",
            "Number of GPUs: 1\n",
            "GPU 0: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "def select_gpu():\n",
        "    \"\"\"\n",
        "    Kiểm tra GPU khả dụng và tự động chọn GPU phù hợp.\n",
        "    \"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        num_gpus = torch.cuda.device_count()\n",
        "        print(f\"Number of GPUs available: {num_gpus}\")\n",
        "\n",
        "        # Duyệt qua các GPU khả dụng để tìm GPU ít sử dụng nhất\n",
        "        available_gpus = [torch.cuda.get_device_name(i) for i in range(num_gpus)]\n",
        "        print(\"Available GPUs:\", available_gpus)\n",
        "\n",
        "        for i in range(num_gpus):\n",
        "            try:\n",
        "                # Đặt GPU\n",
        "                os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(i)\n",
        "                device = torch.device(f\"cuda:{i}\")\n",
        "                torch.cuda.set_device(device)\n",
        "                print(f\"Using GPU: {torch.cuda.get_device_name(device.index)}\")\n",
        "                return device\n",
        "            except Exception as e:\n",
        "                print(f\"GPU {i} is not suitable: {e}\")\n",
        "\n",
        "        print(\"No suitable GPU found. Falling back to CPU.\")\n",
        "        return torch.device(\"cpu\")\n",
        "    else:\n",
        "        print(\"No GPUs available. Using CPU.\")\n",
        "        return torch.device(\"cpu\")\n",
        "\n",
        "# Tự động chọn GPU hoặc CPU\n",
        "device = select_gpu()\n",
        "\n",
        "# Kiểm tra lại thiết bị đang sử dụng\n",
        "print(f\"Final selected device: {device}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ge41QgJadXLv",
        "outputId": "f9f3c814-ed41-41f7-c075-ddaf3506297d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of GPUs available: 1\n",
            "Available GPUs: ['Tesla T4']\n",
            "Using GPU: Tesla T4\n",
            "Final selected device: cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "IkfPfDY3iskg"
      },
      "outputs": [],
      "source": [
        "\n",
        "class BERTIntentClassification(nn.Module):\n",
        "\n",
        "\n",
        "    def __init__(self, model_name=\"bert-base-uncased\", num_classes=10, dropout_rate=0.1, cache_dir = \"huggingface\"):\n",
        "        super(BERTIntentClassification, self).__init__()\n",
        "        self.bert = AutoModel.from_pretrained(model_name, cache_dir = cache_dir)\n",
        "        # Get BERT hidden size\n",
        "        hidden_size = self.bert.config.hidden_size\n",
        "        self.ffnn = nn.Sequential(\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.LayerNorm(hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(hidden_size, num_classes)\n",
        "        )\n",
        "\n",
        "\n",
        "    def freeze_bert(self):\n",
        "        for param in self.bert.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "\n",
        "    def get_pooling(self, hidden_state, attention_mask):\n",
        "        \"\"\"\n",
        "        Get mean pooled representation from BERT hidden states\n",
        "        Args:\n",
        "            hidden_state: BERT output containing hidden states\n",
        "        Returns:\n",
        "            pooled_output: Mean pooled representation of the sequence\n",
        "        \"\"\"\n",
        "        # Get last hidden state\n",
        "        last_hidden_state = hidden_state.last_hidden_state  # Shape: [batch_size, seq_len, hidden_size]\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            # Expand attention mask to match hidden state dimensions\n",
        "            attention_mask = attention_mask.unsqueeze(-1)  # [batch_size, seq_len, 1]\n",
        "\n",
        "            # Mask out padding tokens\n",
        "            masked_hidden = last_hidden_state * attention_mask\n",
        "\n",
        "            # Calculate mean (sum / number of actual tokens)\n",
        "            sum_hidden = torch.sum(masked_hidden, dim=1)  # [batch_size, hidden_size]\n",
        "            count_tokens = torch.sum(attention_mask, dim=1)  # [batch_size, 1]\n",
        "            pooled_output = sum_hidden / count_tokens\n",
        "        else:\n",
        "            # If no attention mask, simply take mean of all tokens\n",
        "            pooled_output = torch.mean(last_hidden_state, dim=1)\n",
        "\n",
        "        return pooled_output\n",
        "\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, **kwargs):\n",
        "        \"\"\"\n",
        "        Forward pass of the model\n",
        "        Args:\n",
        "            input_ids: Input token IDs\n",
        "            attention_mask: Attention mask for padding\n",
        "        Returns:\n",
        "            logits: Raw logits for each class\n",
        "        \"\"\"\n",
        "        # Get BERT hidden states\n",
        "        hidden_state = self.bert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "        )\n",
        "\n",
        "        # Get pooled representation\n",
        "        hidden_state_pooling = self.get_pooling(hidden_state=hidden_state, attention_mask=attention_mask)\n",
        "\n",
        "        # Pass through FFNN classifier\n",
        "        logits = self.ffnn(hidden_state_pooling)\n",
        "\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "zypDXvoaivAb"
      },
      "outputs": [],
      "source": [
        "class TrainerCustom(Trainer):\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        \"\"\"\n",
        "        How the loss is computed by Trainer. By default, all models return the loss in the first element.\n",
        "\n",
        "        Subclass and override for custom behavior.\n",
        "        \"\"\"\n",
        "        if \"labels\" in inputs:\n",
        "            labels = inputs.pop(\"labels\")\n",
        "        else:\n",
        "            labels = None\n",
        "\n",
        "        # Sử dụng nn.CrossEntropyLoss() thay vì nn.CrossEntropy\n",
        "        cross_entropy_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "        # Chạy mô hình và nhận đầu ra (logits)\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "        # Đảm bảo lấy logits từ outputs (mô hình trả về tuple, lấy phần tử đầu tiên là logits)\n",
        "        logits = outputs\n",
        "\n",
        "        # Tính toán loss\n",
        "        loss = cross_entropy_loss(logits, labels)\n",
        "\n",
        "        # Trả về loss và outputs nếu cần\n",
        "        return (loss, outputs) if return_outputs else loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zIJIYGcppMk"
      },
      "source": [
        "# 1. Load Dataset and with Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "DQBY8_d5rwlj"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # Bước 1: Tải dữ liệu\n",
        "# # Sử dụng dataset sẵn có từ Hugging Face hoặc tải từ file cục bộ\n",
        "# dataset = load_dataset(\"imdb\", cache_dir = \"huggingface\")  # Ví dụ: Dữ liệu IMDB để phân loại sentiment\n",
        "# # Thay thế trường 'text' thành 'input_ids' trong train_dataset và test_dataset\n",
        "# def preprocess_dataset(dataset):\n",
        "#     return dataset.map(lambda example: {\n",
        "#             \"input_ids\": example['text'],\n",
        "#             \"label\": example['label']\n",
        "#         },\n",
        "#         remove_columns=[\"text\"],\n",
        "#         num_proc=4  # Sử dụng 4 tiến trình song song để xử lý nhanh hơn\n",
        "#     )\n",
        "\n",
        "# train_dataset = preprocess_dataset(dataset[\"train\"])\n",
        "# test_dataset = preprocess_dataset(dataset[\"test\"])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "RBMrLoabi9DJ"
      },
      "outputs": [],
      "source": [
        "# print(train_dataset)\n",
        "# # Truy cập mẫu cụ thể\n",
        "# train_sample = train_dataset[:10]\n",
        "# test_sample = test_dataset[:2]\n",
        "# print(train_sample)\n",
        "\n",
        "\n",
        "# from datasets import Dataset\n",
        "\n",
        "# train_sample = train_dataset[:10]\n",
        "\n",
        "# # Chuyển từ dict về Dataset\n",
        "# train_sample_dataset = Dataset.from_dict(train_sample)\n",
        "# test_sample_dataset = Dataset.from_dict(test_sample)\n",
        "# print(train_sample_dataset)\n",
        "# print(type(train_sample_dataset))\n",
        "# # Output: <class 'datasets.arrow_dataset.Dataset'>\n",
        "\n",
        "\n",
        "# # In thử 1 hàng trong test_sample_dataset\n",
        "# print(\"First row in test_sample_dataset:\")\n",
        "# print(test_sample_dataset[0])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "slaqKCOhjh9U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab58ba35-662c-4a47-f96c-10ac6f51f3ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['label', 'input_ids'],\n",
            "    num_rows: 27\n",
            "})\n",
            "Dataset({\n",
            "    features: ['label', 'input_ids'],\n",
            "    num_rows: 10\n",
            "})\n",
            "First row in test_sample_dataset:\n",
            "{'label': 'Agree', 'input_ids': 'Yes, I want to show you the picture.'}\n"
          ]
        }
      ],
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "def load_csv_dataset(csv_path, text_column, label_column):\n",
        "    \"\"\"\n",
        "    Tải dataset từ file CSV và đổi tên cột.\n",
        "\n",
        "    Args:\n",
        "        csv_path (str): Đường dẫn đến file .csv.\n",
        "        text_column (str): Tên cột chứa văn bản.\n",
        "        label_column (str): Tên cột chứa nhãn.\n",
        "\n",
        "    Returns:\n",
        "        Dataset: Tập dữ liệu đã tải từ file .csv.\n",
        "    \"\"\"\n",
        "    # Tải dữ liệu từ file .csv\n",
        "    dataset = Dataset.from_csv(csv_path)\n",
        "    # Đổi tên cột\n",
        "    dataset = dataset.rename_columns({text_column: \"input_ids\", label_column: \"label\"})\n",
        "    return dataset\n",
        "\n",
        "# Sử dụng hàm\n",
        "csv_path = \"/content/chatbot_intent_data_v1_En.csv\"             # Đường dẫn file CSV\n",
        "text_column = \"input_ids\"       # Cột chứa văn bản\n",
        "label_column = \"label\"        # Cột chứa nhãn\n",
        "\n",
        "# Tải dataset\n",
        "dataset = load_csv_dataset(csv_path, text_column, label_column)\n",
        "\n",
        "# Kiểm tra dữ liệu\n",
        "print(dataset)\n",
        "\n",
        "# Truy cập mẫu cụ thể\n",
        "sample_dataset = dataset.select(range(10))  # Lấy 10 mẫu đầu tiên\n",
        "print(sample_dataset)\n",
        "\n",
        "\n",
        "# In thử 1 hàng trong test_sample_dataset\n",
        "print(\"First row in test_sample_dataset:\")\n",
        "print(sample_dataset[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "1oFci1j5k1UY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49f5f8c9-831e-472f-c77b-24a531369faa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Invalid Samples =====\n",
            "[]\n"
          ]
        }
      ],
      "source": [
        "def check_invalid_samples(dataset):\n",
        "    invalid_samples = []\n",
        "    for idx, sample in enumerate(dataset):\n",
        "        if not isinstance(sample[\"input_ids\"], str) or sample[\"input_ids\"].strip() == \"\":\n",
        "            invalid_samples.append((idx, sample))\n",
        "    return invalid_samples\n",
        "\n",
        "# Kiểm tra dữ liệu không hợp lệ\n",
        "invalid_samples = check_invalid_samples(dataset)\n",
        "print(\"\\n===== Invalid Samples =====\")\n",
        "print(invalid_samples)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "x0-3V7dLlCZp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d136919-e74a-442d-c859-d6e3631be956"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ánh xạ nhãn: {'Agree': 0, 'Decline': 1, 'Fallback': 2, 'Silence': 3, 'Uncertain': 4}\n",
            "Dataset({\n",
            "    features: ['label', 'input_ids'],\n",
            "    num_rows: 27\n",
            "})\n",
            "Dataset({\n",
            "    features: ['label', 'input_ids'],\n",
            "    num_rows: 10\n",
            "})\n",
            "First row in sample_dataset:\n",
            "{'label': 0, 'input_ids': 'Yes, I want to show you the picture.'}\n"
          ]
        }
      ],
      "source": [
        "# Tự động phát hiện nhãn và tạo ánh xạ nhãn\n",
        "def create_label_mapping(dataset_list):\n",
        "    \"\"\"\n",
        "    Tự động phát hiện tất cả các nhãn từ danh sách dataset và ánh xạ chúng thành số nguyên.\n",
        "    \"\"\"\n",
        "    all_labels = set()\n",
        "    for dataset in dataset_list:\n",
        "        all_labels.update(dataset[\"label\"])  # Tập hợp tất cả các nhãn từ dataset\n",
        "\n",
        "    label_to_int = {label: idx for idx, label in enumerate(sorted(all_labels))}\n",
        "    print(f\"Ánh xạ nhãn: {label_to_int}\")\n",
        "    return label_to_int\n",
        "\n",
        "# Hàm chuyển đổi nhãn\n",
        "def preprocess_labels(example, label_to_int):\n",
        "    example[\"label\"] = label_to_int.get(example[\"label\"], -1)  # Gán -1 cho nhãn không hợp lệ\n",
        "    return example\n",
        "\n",
        "# Tạo ánh xạ nhãn\n",
        "label_mapping = create_label_mapping([dataset])\n",
        "\n",
        "# Áp dụng chuyển đổi nhãn\n",
        "dataset = dataset.map(lambda example: preprocess_labels(example, label_mapping))\n",
        "\n",
        "# Kiểm tra kết quả\n",
        "print(dataset)\n",
        "\n",
        "# Truy cập mẫu cụ thể\n",
        "sample_dataset = dataset.select(range(10))  # Lấy 10 mẫu đầu tiên\n",
        "print(sample_dataset)\n",
        "\n",
        "# In thử 1 hàng trong sample_dataset\n",
        "print(\"First row in sample_dataset:\")\n",
        "print(sample_dataset[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "hy_jM2gOk99M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "688d6d21-a578-4af1-b147-85cc6c74d75b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chia dataset: 18 mẫu train, 9 mẫu test\n",
            "Train dataset: Dataset({\n",
            "    features: ['label', 'input_ids'],\n",
            "    num_rows: 18\n",
            "})\n",
            "Test dataset: Dataset({\n",
            "    features: ['label', 'input_ids'],\n",
            "    num_rows: 9\n",
            "})\n",
            "Sample train dataset: Dataset({\n",
            "    features: ['label', 'input_ids'],\n",
            "    num_rows: 8\n",
            "})\n",
            "Sample test dataset: Dataset({\n",
            "    features: ['label', 'input_ids'],\n",
            "    num_rows: 9\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "def split_dataset(dataset, test_size=0.2, seed=42):\n",
        "    \"\"\"\n",
        "    Chia dataset thành tập train và test.\n",
        "\n",
        "    Args:\n",
        "        dataset (Dataset): Tập dữ liệu đầy đủ.\n",
        "        test_size (float): Tỷ lệ dữ liệu test (0.0 - 1.0).\n",
        "        seed (int): Seed để chia dữ liệu ngẫu nhiên.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (train_dataset, test_dataset) - Tập train và test.\n",
        "    \"\"\"\n",
        "    if not (0.0 < test_size < 1.0):\n",
        "        raise ValueError(\"test_size phải nằm trong khoảng (0.0, 1.0)\")\n",
        "    if len(dataset) < 2:\n",
        "        raise ValueError(\"Dataset phải có ít nhất 2 mẫu để chia.\")\n",
        "\n",
        "    train_test_split = dataset.train_test_split(test_size=test_size, seed=seed)\n",
        "    print(f\"Chia dataset: {len(train_test_split['train'])} mẫu train, {len(train_test_split['test'])} mẫu test\")\n",
        "    return train_test_split[\"train\"], train_test_split[\"test\"]\n",
        "\n",
        "# Chia dataset\n",
        "train_dataset, test_dataset = split_dataset(dataset, test_size=0.3)\n",
        "\n",
        "# Kiểm tra dữ liệu\n",
        "print(\"Train dataset:\", train_dataset)\n",
        "print(\"Test dataset:\", test_dataset)\n",
        "\n",
        "# Truy cập mẫu cụ thể\n",
        "sample_train_dataset = train_dataset.select(range(8))  # Lấy 10 mẫu đầu tiên từ train\n",
        "sample_test_dataset = test_dataset.select(range(9))    # Lấy 10 mẫu đầu tiên từ test\n",
        "\n",
        "print(\"Sample train dataset:\", sample_train_dataset)\n",
        "print(\"Sample test dataset:\", sample_test_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCzQIjl_ptbw"
      },
      "source": [
        "# 2. Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "bknqLH2piJMv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94420898-20eb-47ac-fdfc-e42212ca0dc7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Bước 2: Chuẩn bị tokenizer và token hóa dữ liệu\n",
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir = \"huggingface\")\n",
        "model = BERTIntentClassification(\n",
        "    model_name=model_name,\n",
        "    num_classes=5\n",
        ")\n",
        "model.freeze_bert() # Froze Layer BERT\n",
        "max_seq_length = 512\n",
        "\n",
        "\n",
        "def collate_fn(features):\n",
        "    inputs = []\n",
        "    labels = []\n",
        "    for element in features:\n",
        "        inputs.append(element.get(\"input_ids\"))\n",
        "        labels.append(element.get(\"label\"))\n",
        "\n",
        "    labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    token_inputs = tokenizer(\n",
        "        inputs,\n",
        "        add_special_tokens=True,\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=max_seq_length,\n",
        "        return_overflowing_tokens=False,\n",
        "        return_length=False,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "    token_inputs.update({\n",
        "        \"labels\": labels,\n",
        "    })\n",
        "    return token_inputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZldUk54pj1N"
      },
      "source": [
        "# 3. Train Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptK7Cy22p2GK"
      },
      "source": [
        "## 3.1 Log Wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "xZkR3vuFp5uN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1242a1a4-416c-4d2b-f292-9f6d2c4fed68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.19.2)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.25.5)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.10.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.19.2)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.12.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.12.14)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "qux2ABzMp7Ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "622c8ba1-69b0-454e-b0c5-48c0b114beb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (1.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ZZRspV7jp8OT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20b08da5-aae7-4a83-f391-5b49b664a472"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "c8767\n"
          ]
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "# Load biến môi trường từ file .env\n",
        "load_dotenv()\n",
        "\n",
        "# Lấy key từ biến môi trường\n",
        "wandb_api_key = os.getenv(\"WANDB_API_KEY\")\n",
        "print(wandb_api_key[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "215vQ7cOp9oo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "921cb2b6-86fd-4323-8fdc-f772ba36452b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdoanngoccuong\u001b[0m (\u001b[33mdoanngoccuong_nh\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "import wandb\n",
        "import os\n",
        "\n",
        "# Lấy API key từ biến môi trường và đăng nhập\n",
        "wandb.login(key=os.getenv(\"WANDB_API_KEY\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJG_rUYTqwLJ"
      },
      "source": [
        "Cách thiết lập thông qua TrainingArguments\n",
        "Khi sử dụng Trainer, bạn có thể đặt tên dự án trực tiếp trong TrainingArguments bằng cách sử dụng tham số report_to và run_name. Tuy nhiên, để đặt project, bạn cần khởi tạo một phiên wandb trước hoặc truyền cấu hình này thông qua wandb.init().\n",
        "\n",
        "Điều chỉnh TrainingArguments:\n",
        "```python\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results_\",          # Thư mục lưu kết quả\n",
        "    eval_strategy=\"epoch\",           # Đánh giá sau mỗi epoch\n",
        "    learning_rate=2e-4,\n",
        "    per_device_train_batch_size=128,\n",
        "    per_device_eval_batch_size=128,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",            # Thư mục lưu log\n",
        "    logging_strategy=\"steps\",        # Log theo steps\n",
        "    logging_steps=10,                # Log sau mỗi 10 bước\n",
        "    save_strategy=\"epoch\",           # Lưu checkpoint sau mỗi epoch\n",
        "    save_total_limit=3,              # Lưu tối đa 3 checkpoint\n",
        "    report_to=\"wandb\",               # Báo cáo log tới wandb\n",
        "    run_name=\"bert_run_1\"            # Tên phiên chạy trên wandb\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAMgPe3NqAhz"
      },
      "source": [
        "## 3.2 Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3hWcz751mEx"
      },
      "source": [
        "### Ver 1.2.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gs-Z20WNRnE"
      },
      "source": [
        "Dưới đây là bảng tóm tắt chi tiết cách lưu mô hình dựa trên chiến lược được đề xuất:\n",
        "\n",
        "| **Loại Model**    | **Điều Kiện Lưu**                                                                 | **Thư Mục Lưu Trên Local**       | **Số Lượng Lưu Trên Local**        | **Thông Tin Thêm**                              | **Đồng Bộ Lên WandB**                  |\n",
        "|--------------------|-----------------------------------------------------------------------------------|-----------------------------------|------------------------------------|-----------------------------------------------|-----------------------------------------|\n",
        "| **Best Model**     | Khi `eval_loss` giảm                                                             | `output_dir/best_model`           | Chỉ lưu một bản duy nhất           | Lưu thông tin `epoch` và `eval_loss`.          | Có: Artifact `best_model`. Thêm `epoch` và `loss` vào `metadata`. |\n",
        "| **Final Checkpoint** | Sau mỗi epoch (checkpoint cuối của epoch)                                        | `output_dir/checkpoint-epoch-<n>` | Tối đa 3 checkpoint gần nhất       | Không có thông tin đặc biệt.                   | Không đồng bộ (tránh trùng lặp dữ liệu lớn). |\n",
        "| **Custom Checkpoint** (tùy chọn) | Sau một số bước cố định hoặc mốc quan trọng (nếu cần thiết, ví dụ: mỗi 5 epoch) | Tùy chỉnh, ví dụ: `output_dir/checkpoint-step-<n>` | Theo ý muốn, hoặc không giới hạn | Thêm các mốc quan trọng để phân tích sau này. | Tùy chọn (không bắt buộc).              |\n",
        "\n",
        "---\n",
        "\n",
        "### **Chi tiết về bảng**\n",
        "1. **Best Model**:\n",
        "   - Điều kiện: `eval_loss` giảm.\n",
        "   - Chỉ lưu một phiên bản tốt nhất.\n",
        "   - Lưu thông tin epoch và loss để dễ dàng tham khảo hoặc tải xuống sau này.\n",
        "\n",
        "2. **Final Checkpoint**:\n",
        "   - Được lưu sau mỗi epoch.\n",
        "   - Giới hạn số lượng checkpoint lưu trên local để tiết kiệm bộ nhớ (ví dụ: tối đa 3 checkpoint).\n",
        "   - Không lưu thông tin thêm vào checkpoint.\n",
        "\n",
        "3. **Custom Checkpoint** (tùy chọn):\n",
        "   - Có thể sử dụng nếu bạn muốn lưu checkpoint tại các mốc thời gian cụ thể, chẳng hạn như mỗi 5 epoch hoặc sau một số bước huấn luyện (steps).\n",
        "   - Thích hợp khi bạn cần kiểm tra tiến độ huấn luyện chi tiết hơn hoặc muốn lưu backup.\n",
        "\n",
        "---\n",
        "\n",
        "### **Tóm tắt logic**\n",
        "- **Best Model**:\n",
        "  - Lưu vào thư mục cố định (`best_model`).\n",
        "  - Ghi đè khi có `eval_loss` mới tốt hơn.\n",
        "  - Đồng bộ lên WandB.\n",
        "\n",
        "- **Final Checkpoint**:\n",
        "  - Lưu sau mỗi epoch.\n",
        "  - Xóa checkpoint cũ nhất nếu vượt giới hạn `save_total_limit`.\n",
        "  - Không đồng bộ lên WandB (tránh lãng phí không gian lưu trữ).\n",
        "\n",
        "- **Custom Checkpoint**:\n",
        "  - Tùy chọn nếu bạn cần lưu thêm để phục vụ các mục đích cụ thể.\n",
        "\n",
        "Nếu bạn cần thêm bất kỳ chi tiết nào khác, hãy cho mình biết nhé! 😊"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCbkX1KHeT0j"
      },
      "source": [
        "### **Bảng Tóm Tắt: Lưu Best Model và Last Model**\n",
        "\n",
        "| **Loại Model**    | **Khi Nào Cần Lưu**                                                                                         | **Ưu Điểm**                                                                                       | **Hạn Chế**                                                                                      |\n",
        "|--------------------|------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------|\n",
        "| **Best Model**     | - Khi muốn triển khai mô hình tốt nhất với `eval_loss` thấp nhất hoặc `accuracy` cao nhất.                   | - Đảm bảo lưu lại mô hình có hiệu suất tốt nhất trên tập validation.<br>- Phù hợp để triển khai.   | - Không lưu trạng thái đầy đủ (optimizer, scheduler).<br>- Không tiếp tục huấn luyện từ trạng thái này. |\n",
        "| **Last Model**     | - Khi cần tiếp tục huấn luyện (fine-tuning) hoặc khôi phục trạng thái sau khi huấn luyện kết thúc.         | - Lưu đầy đủ trạng thái (weights, optimizer, scheduler).<br>- Phù hợp để tiếp tục huấn luyện.    | - Có thể không phải là mô hình tốt nhất (do overfitting hoặc underfitting).                     |\n",
        "| **Chỉ Lưu Best**   | - Khi chỉ quan tâm đến triển khai mô hình tốt nhất, không cần tiếp tục huấn luyện sau này.                  | - Tiết kiệm tài nguyên lưu trữ.<br>- Tập trung vào mô hình tối ưu cho triển khai.                | - Không thể tiếp tục huấn luyện nếu cần.                                                         |\n",
        "| **Chỉ Lưu Last**   | - Khi muốn đảm bảo khả năng khôi phục trạng thái để tiếp tục huấn luyện.                                    | - Khôi phục hoàn toàn quá trình huấn luyện.<br>- Phù hợp cho fine-tuning hoặc thử nghiệm sau này. | - Không đảm bảo đây là mô hình tốt nhất để triển khai.                                           |\n",
        "| **Lưu Cả Hai**     | - Khi cần cả triển khai mô hình tốt nhất và tiếp tục huấn luyện sau này.                                    | - Kết hợp ưu điểm của cả Best Model và Last Model.<br>- Linh hoạt trong sử dụng.                 | - Tốn thêm tài nguyên lưu trữ và thời gian.                                                     |\n",
        "\n",
        "---\n",
        "\n",
        "### **Chiến Lược Tối Ưu**\n",
        "| **Loại Lưu** | **Tần Suất**                          | **Chiến Lược**                                                                                             |\n",
        "|--------------|---------------------------------------|-----------------------------------------------------------------------------------------------------------|\n",
        "| **Best Model** | Khi `eval_loss` giảm                 | Lưu mỗi lần `eval_loss` giảm để đảm bảo mô hình tốt nhất luôn được lưu.                                    |\n",
        "| **Last Model** | Sau khi huấn luyện kết thúc          | Lưu trạng thái cuối cùng của quá trình huấn luyện (weights + optimizer + scheduler).                      |\n",
        "| **Kết hợp**   | Best Model: Mỗi khi `eval_loss` giảm<br>Last Model: Sau khi kết thúc | Lưu cả Best Model để triển khai và Last Model để tiếp tục huấn luyện khi cần thiết.                      |\n",
        "\n",
        "---\n",
        "\n",
        "### **Lựa Chọn Phù Hợp**\n",
        "- **Dự án triển khai mô hình nhanh**: Lưu **Best Model**.\n",
        "- **Dự án nghiên cứu hoặc fine-tuning tiếp**: Lưu **Last Model**.\n",
        "- **Dự án quy mô lớn, cần cả triển khai và mở rộng**: Lưu **cả hai**.\n",
        "\n",
        "Hãy chọn chiến lược lưu phù hợp với mục tiêu dự án của bạn! 🚀"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLIKjrdaesyG"
      },
      "source": [
        "Thui, ko lưu local nữa, lưu tất trên wandb đi.\n",
        "- Với best model: lưu lên wandb khi loss giảm và đã sau 10 epochs  \n",
        "(Lưu Best Model ngay khi eval_loss giảm ở local, sau 10 epochs thì đồng bộ cái best lên wandb, sau đó xoá các file best ở local).\n",
        "Chỉ đồng bộ lên WandB mỗi 10 epochs.)\n",
        "- Với last model: lưu lên wandb sau mỗi 10 epochs. (lưu local trước -> đồng bộ lên wandb sẽ xoá file local)\n",
        "+, Trong quá trình lưu thì việc training vẫn diễn ra Parallel\n",
        "\n",
        "đều lưu đầy đủ toàn bộ tham số để có thể train thêm từ cả ở best model và last model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "bHYLAOxP3Lea"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "I5n7vK-a9_gM"
      },
      "outputs": [],
      "source": [
        "# class TrainerCustom(Trainer):\n",
        "\n",
        "#     def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "#         \"\"\"\n",
        "#         How the loss is computed by Trainer. By default, all models return the loss in the first element.\n",
        "\n",
        "#         Subclass and override for custom behavior.\n",
        "#         \"\"\"\n",
        "#         if \"labels\" in inputs:\n",
        "#             labels = inputs.pop(\"labels\")\n",
        "#         else:\n",
        "#             labels = None\n",
        "\n",
        "#         # Sử dụng nn.CrossEntropyLoss() thay vì nn.CrossEntropy\n",
        "#         cross_entropy_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "#         # Chạy mô hình và nhận đầu ra (logits)\n",
        "#         outputs = model(**inputs)\n",
        "\n",
        "#         # Đảm bảo lấy logits từ outputs (mô hình trả về tuple, lấy phần tử đầu tiên là logits)\n",
        "#         logits = outputs\n",
        "\n",
        "#         if labels is None:\n",
        "#             print(\"Labels are None during compute_loss.\")\n",
        "#         if logits is None:\n",
        "#             print(\"Logits are None during compute_loss.\")\n",
        "\n",
        "#         # Tính toán loss\n",
        "#         loss = cross_entropy_loss(logits, labels)\n",
        "\n",
        "#         # Trả về loss và outputs nếu cần\n",
        "#         return (loss, outputs) if return_outputs else loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "iYRbPjlfDSTs"
      },
      "outputs": [],
      "source": [
        "\n",
        "# import wandb\n",
        "\n",
        "# # Khởi tạo wandb\n",
        "# wandb.init(\n",
        "#     project=\"bert-intent-classification\",  # Tên dự án\n",
        "#     name=\"bert_run_3\"                     # Tên phiên chạy\n",
        "# )\n",
        "\n",
        "\n",
        "# # Bước 6: Cài đặt tham số huấn luyện\n",
        "# training_args = TrainingArguments(\n",
        "#     output_dir=\"./result__s\",          # Thư mục lưu kết quả\n",
        "#     eval_strategy=\"epoch\",    # Đánh giá sau mỗi epoch\n",
        "#     learning_rate=2e-4,\n",
        "#     per_device_train_batch_size=128,\n",
        "#     per_device_eval_batch_size=128,\n",
        "#     num_train_epochs=50,\n",
        "#     weight_decay=0.01,\n",
        "#     logging_dir=\"./logs\",\n",
        "#     logging_strategy=\"steps\",\n",
        "#     logging_steps=1,  # Ghi logs mỗi 500 bước huấn luyện\n",
        "#     save_strategy=\"no\",          # Lưu trọng số sau mỗi epoch\n",
        "#     save_total_limit=3,\n",
        "#     label_names = [\"labels\"],\n",
        "#     report_to=\"wandb\",\n",
        "#     run_name=\"bert_run_3\"\n",
        "# )\n",
        "\n",
        "\n",
        "# batch = collate_fn([sample_test_dataset[0]]) # Tạo một batch từ một mẫu đơn lẻ (sample_test_dataset[0]) để kiểm tra xem hàm collate_fn có hoạt động đúng không.\n",
        "# print(batch)\n",
        "\n",
        "# # metrics = trainer.evaluate()\n",
        "# # Mục đích: Chạy giai đoạn evaluation (đánh giá) trên eval_dataset (sample_test_dataset) và tính toán các metrics như:\n",
        "# trainer = TrainerCustom(\n",
        "#     model=model,\n",
        "#     args=training_args,\n",
        "#     train_dataset=sample_train_dataset,\n",
        "#     eval_dataset=sample_test_dataset,\n",
        "#     tokenizer=tokenizer,\n",
        "#     data_collator=collate_fn,\n",
        "# )\n",
        "\n",
        "# metrics = trainer.evaluate()\n",
        "# print(metrics)  # Kiểm tra xem có \"eval_loss\" hay không\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # Bước 7: Tạo Trainer\n",
        "# trainer = TrainerCustom(\n",
        "#     model=model,\n",
        "#     args=training_args,\n",
        "#     train_dataset=sample_train_dataset,\n",
        "#     eval_dataset=sample_test_dataset,\n",
        "#     tokenizer=tokenizer,\n",
        "#     data_collator = collate_fn,\n",
        "# )\n",
        "\n",
        "# # Bước 8: Huấn luyện\n",
        "# trainer.train()\n",
        "\n",
        "# # Kết thúc phiên wandb\n",
        "# wandb.finish()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Vtlo08BaWQhe"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import wandb\n",
        "import os\n",
        "import shutil\n",
        "import time\n",
        "\n",
        "class TrainerCustom(Trainer):\n",
        "    def __init__(self, *args, save_every_n_epochs=10, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        if torch.cuda.is_available():\n",
        "            print(f\"Trainer is running on GPU: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
        "        else:\n",
        "            print(\"Trainer is running on CPU.\")\n",
        "\n",
        "        self.best_eval_loss = float(\"inf\")  # Giá trị loss tốt nhất ban đầu\n",
        "        self.save_every_n_epochs = save_every_n_epochs  # Tần suất lưu lên WandB\n",
        "        self.best_model_info = {\"epoch\": None, \"loss\": None}\n",
        "        self.last_saved_epoch = 0  # Epoch cuối cùng đã lưu Best Model và Last Model\n",
        "        self.executor = ThreadPoolExecutor(max_workers=3)  # Cho phép tối đa 2 luồng song song\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        \"\"\"\n",
        "        How the loss is computed by Trainer. By default, all models return the loss in the first element.\n",
        "\n",
        "        Subclass and override for custom behavior.\n",
        "        \"\"\"\n",
        "\n",
        "        # Kiểm tra thiết bị của mô hình và dữ liệu\n",
        "        print(\"Model device:\", next(model.parameters()).device)\n",
        "        print(\"Input device:\", inputs[\"input_ids\"].device)\n",
        "        if \"labels\" in inputs:\n",
        "            labels = inputs.pop(\"labels\")\n",
        "        else:\n",
        "            labels = None\n",
        "\n",
        "        # Sử dụng nn.CrossEntropyLoss() thay vì nn.CrossEntropy\n",
        "        cross_entropy_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "        # Chạy mô hình và nhận đầu ra (logits)\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "        # Đảm bảo lấy logits từ outputs (mô hình trả về tuple, lấy phần tử đầu tiên là logits)\n",
        "        logits = outputs\n",
        "\n",
        "        if labels is None:\n",
        "            print(\"Labels are None during compute_loss.\")\n",
        "        if logits is None:\n",
        "            print(\"Logits are None during compute_loss.\")\n",
        "\n",
        "        # Tính toán loss\n",
        "        loss = cross_entropy_loss(logits, labels)\n",
        "\n",
        "        # Trả về loss và outputs nếu cần\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "    def async_save_model(self, model_dir, artifact_name, metadata=None):\n",
        "        \"\"\"\n",
        "        Lưu mô hình vào local và đồng bộ lên WandB trong luồng song song.\n",
        "        \"\"\"\n",
        "        def save():\n",
        "            start_time = time.time()\n",
        "            try:\n",
        "                # Xóa tất cả các thư mục tmp_best_model_ trước đó\n",
        "                for folder in os.listdir(\".\"):\n",
        "                    if folder.startswith(\"tmp_best_model_epoch_\") and folder != model_dir:\n",
        "                        shutil.rmtree(folder, ignore_errors=True)\n",
        "                        print(f\"Removed old temporary directory: {folder}\")\n",
        "\n",
        "                # Lưu mô hình vào thư mục tạm\n",
        "                self.save_model(model_dir)\n",
        "\n",
        "                # Đồng bộ lên WandB\n",
        "                artifact = wandb.Artifact(artifact_name, type=\"model\")\n",
        "                artifact.add_dir(model_dir)\n",
        "                if metadata:\n",
        "                    artifact.metadata = metadata\n",
        "                wandb.log_artifact(artifact)\n",
        "            except Exception as e:\n",
        "                print(f\"Error during saving or syncing model {artifact_name}: {e}\")\n",
        "            finally:\n",
        "                # Xóa thư mục tạm hiện tại sau khi đồng bộ\n",
        "                try:\n",
        "                    shutil.rmtree(model_dir, ignore_errors=True)\n",
        "                    print(f\"Successfully removed temporary directory: {model_dir}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error removing temporary directory {model_dir}: {e}\")\n",
        "\n",
        "            elapsed_time = time.time() - start_time\n",
        "            print(f\"Model saved and uploaded to WandB: {artifact_name} in {elapsed_time:.2f} seconds\")\n",
        "\n",
        "        self.executor.submit(save)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def evaluate(self, eval_dataset=None, ignore_keys=None, metric_key_prefix: str = \"eval\"):\n",
        "        metrics = super().evaluate(eval_dataset, ignore_keys, metric_key_prefix)\n",
        "        eval_loss = metrics.get(\"eval_loss\")\n",
        "\n",
        "        # Cập nhật Best Model nếu eval_loss giảm\n",
        "        # Lưu Best Model ngay khi eval_loss giảm (local).\n",
        "        # Chỉ đồng bộ lên WandB mỗi 10 epochs.\n",
        "\n",
        "        if eval_loss is not None and eval_loss < self.best_eval_loss:\n",
        "            print(f\"New best eval_loss: {eval_loss}\")\n",
        "            self.best_eval_loss = eval_loss\n",
        "            self.best_model_info = {\"epoch\": self.state.epoch, \"loss\": eval_loss}\n",
        "\n",
        "            # Log thông tin Best Model lên WandB\n",
        "            wandb.log({\n",
        "                \"best_eval_loss\": self.best_eval_loss,\n",
        "                \"best_model_epoch\": self.best_model_info.get(\"epoch\", -1)\n",
        "            })\n",
        "\n",
        "            # Lưu Best Model vào thư mục tạm (local)\n",
        "            best_model_dir = f\"./tmp_best_model_epoch_{int(self.state.epoch)}\"\n",
        "            self.save_model(best_model_dir)\n",
        "\n",
        "            # Đồng bộ lên WandB mỗi 10 epochs\n",
        "            if int(self.state.epoch) % self.save_every_n_epochs == 0:\n",
        "                artifact_name = f\"best_model_epoch_{int(self.state.epoch)}\"\n",
        "                self.async_save_model(best_model_dir, artifact_name, self.best_model_info)\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def save_last_model(self):\n",
        "        \"\"\"\n",
        "        Lưu Last Model lên WandB sau mỗi N epochs.\n",
        "        \"\"\"\n",
        "        if int(self.state.epoch) % self.save_every_n_epochs == 0 and int(self.state.epoch) != self.last_saved_epoch:\n",
        "            print(f\"Saving Last Model at epoch {self.state.epoch} to WandB...\")\n",
        "            last_model_dir = f\"./tmp_last_model_epoch_{int(self.state.epoch)}\"\n",
        "            artifact_name = f\"last_model_epoch_{int(self.state.epoch)}\"\n",
        "            self.async_save_model(last_model_dir, artifact_name)\n",
        "\n",
        "            # Log thông tin Last Model lên WandB\n",
        "            wandb.log({\n",
        "                \"last_model_epoch\": self.state.epoch\n",
        "            })\n",
        "\n",
        "            # Cập nhật epoch cuối cùng đã lưu\n",
        "            self.last_saved_epoch = int(self.state.epoch)\n",
        "\n",
        "    def train(self, *args, **kwargs):\n",
        "        result = super().train(*args, **kwargs)\n",
        "\n",
        "        # Sau mỗi epoch, lưu Last Model lên WandB\n",
        "        self.save_last_model()\n",
        "        # Chờ tất cả các luồng lưu hoàn thành trước khi kết thúc\n",
        "        self.executor.shutdown(wait=True)\n",
        "\n",
        "        return result\n",
        "\n",
        "\n",
        "# Bước 6: Cài đặt tham số huấn luyện\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./result__s\",          # Thư mục lưu kết quả\n",
        "    eval_strategy=\"epoch\",    # Đánh giá sau mỗi epoch\n",
        "    learning_rate=2e-4,\n",
        "    per_device_train_batch_size=128,\n",
        "    per_device_eval_batch_size=128,\n",
        "    num_train_epochs=50,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=1,  # Ghi logs mỗi 500 bước huấn luyện\n",
        "    save_strategy=\"no\",          # Lưu trọng số sau mỗi epoch\n",
        "    save_total_limit=3,\n",
        "    label_names = [\"labels\"],\n",
        "    report_to=\"wandb\",\n",
        "    run_name=\"bert_run_3\"\n",
        ")\n",
        "\n",
        "\n",
        "import wandb\n",
        "\n",
        "# Khởi tạo wandb\n",
        "wandb.init(\n",
        "    project=\"bert-intent-classification\",  # Tên dự án\n",
        "    name=\"bert_run_3\",                     # Tên phiên chạy\n",
        "    config={\"gpu\": torch.cuda.get_device_name(torch.cuda.current_device()) if torch.cuda.is_available() else \"CPU\"}\n",
        ")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"Trainer is running on GPU: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
        "else:\n",
        "    print(\"Trainer is running on CPU.\")\n",
        "\n",
        "trainer = TrainerCustom(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=sample_train_dataset,\n",
        "    eval_dataset=sample_test_dataset,\n",
        "    data_collator=collate_fn,\n",
        "    save_every_n_epochs=10  # Lưu Best Model và Last Model mỗi 10 epochs\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "wandb.finish()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7qjF1K2EW0Ys",
        "outputId": "71d5827b-d426-4aaf-95d0-125065e8b2aa"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250113_155341-ybo3fi9m</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/doanngoccuong_nh/bert-intent-classification/runs/ybo3fi9m' target=\"_blank\">bert_run_3</a></strong> to <a href='https://wandb.ai/doanngoccuong_nh/bert-intent-classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/doanngoccuong_nh/bert-intent-classification' target=\"_blank\">https://wandb.ai/doanngoccuong_nh/bert-intent-classification</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/doanngoccuong_nh/bert-intent-classification/runs/ybo3fi9m' target=\"_blank\">https://wandb.ai/doanngoccuong_nh/bert-intent-classification/runs/ybo3fi9m</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainer is running on GPU: Tesla T4\n",
            "Trainer is running on GPU: Tesla T4\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [50/50 00:07, Epoch 50/50]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.504333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.508570</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.512193</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.514808</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.518519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.519195</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.520812</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.521818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.524044</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.525590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.527027</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.528899</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.530765</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.532099</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.532809</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.533924</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.537579</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.541591</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.543557</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.545702</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.547019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.550053</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.552818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.555238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.557191</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.559164</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.561520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.564117</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.567102</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.570376</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.573700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.575893</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.578464</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.580347</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.581907</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.583474</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.585416</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.586971</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.589063</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.591239</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.593091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.594595</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.596463</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.598260</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.599729</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.600888</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.601807</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.602536</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.603000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.603219</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "New best eval_loss: 3.5043325424194336\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Saving Last Model at epoch 50.0 to WandB...\n",
            "Removed old temporary directory: tmp_best_model_epoch_1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./tmp_last_model_epoch_50)... Done. 5.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully removed temporary directory: ./tmp_last_model_epoch_50\n",
            "Model saved and uploaded to WandB: last_model_epoch_50 in 13.87 seconds\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_eval_loss</td><td>▁</td></tr><tr><td>best_model_epoch</td><td>▁</td></tr><tr><td>eval/loss</td><td>▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇██████</td></tr><tr><td>eval/runtime</td><td>▂▁▁▁▁▂▁▁▁▁▁▁▁▁▂▃▄▄▁▁▁▄▄▂▁▁▁▂▁█▆▂▃▄▁▁▂▂▂▂</td></tr><tr><td>eval/samples_per_second</td><td>▅▇▇▇▇▄▆▇▇▆▇▇▇▆▄▃▃▇█▇▃▃▅▇▆█▆█▁▃▅▃▃▄▇▇▆▆▆▆</td></tr><tr><td>eval/steps_per_second</td><td>▅▇▇▇▇▄▇▆▇▇▇▇▇▇▆▄▃▃▇██▃▃▅▆▇█▆▇▁▂▅▃▃▃█▆▆▆▆</td></tr><tr><td>last_model_epoch</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>▁▁▁▂▂▃▃▃▂▂▂▃▂▃▄█▃▂▄▅▄▂▄▅▃▅▁▃▃▅▃▄▂▂▄▃▃▂▃▁</td></tr><tr><td>train/learning_rate</td><td>████▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_eval_loss</td><td>3.50433</td></tr><tr><td>best_model_epoch</td><td>1</td></tr><tr><td>eval/loss</td><td>3.60322</td></tr><tr><td>eval/runtime</td><td>0.0371</td></tr><tr><td>eval/samples_per_second</td><td>242.832</td></tr><tr><td>eval/steps_per_second</td><td>26.981</td></tr><tr><td>last_model_epoch</td><td>50</td></tr><tr><td>total_flos</td><td>0</td></tr><tr><td>train/epoch</td><td>50</td></tr><tr><td>train/global_step</td><td>50</td></tr><tr><td>train/grad_norm</td><td>0.0</td></tr><tr><td>train/learning_rate</td><td>0</td></tr><tr><td>train/loss</td><td>0</td></tr><tr><td>train_loss</td><td>0.0</td></tr><tr><td>train_runtime</td><td>7.7718</td></tr><tr><td>train_samples_per_second</td><td>51.468</td></tr><tr><td>train_steps_per_second</td><td>6.434</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">bert_run_3</strong> at: <a href='https://wandb.ai/doanngoccuong_nh/bert-intent-classification/runs/ybo3fi9m' target=\"_blank\">https://wandb.ai/doanngoccuong_nh/bert-intent-classification/runs/ybo3fi9m</a><br> View project at: <a href='https://wandb.ai/doanngoccuong_nh/bert-intent-classification' target=\"_blank\">https://wandb.ai/doanngoccuong_nh/bert-intent-classification</a><br>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250113_155341-ybo3fi9m/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Bước 9: Đánh giá trên tập kiểm tra\n",
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "rC3AmIe0T9co",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "outputId": "3649d606-a0b3-4bc2-b680-94d2f7d58c40"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1/1 : < :]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "Error",
          "evalue": "You must call wandb.init() before wandb.log()",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-d56dd7597d91>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Bước 9: Đánh giá trên tập kiểm tra\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-23-8962e5ce1ff0>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_key_prefix\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"eval\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_key_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0meval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"eval_loss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4072\u001b[0m         )\n\u001b[1;32m   4073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4074\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4075\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4076\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mDebugOption\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPU_METRICS_DEBUG\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mlog\u001b[0;34m(self, logs, start_time)\u001b[0m\n\u001b[1;32m   3568\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"step\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3569\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3570\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_log\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3572\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prepare_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer_callback.py\u001b[0m in \u001b[0;36mon_log\u001b[0;34m(self, args, state, control, logs)\u001b[0m\n\u001b[1;32m    510\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_log\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerState\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerControl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_log\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"on_log\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_prediction_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerState\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerControl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer_callback.py\u001b[0m in \u001b[0;36mcall_event\u001b[0;34m(self, event, args, state, control, **kwargs)\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m             result = getattr(callback, event)(\n\u001b[0m\u001b[1;32m    520\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m                 \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/integrations/integration_utils.py\u001b[0m in \u001b[0;36mon_log\u001b[0;34m(self, args, state, control, model, logs, **kwargs)\u001b[0m\n\u001b[1;32m    968\u001b[0m             \u001b[0mnon_scalar_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msingle_value_scalars\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    969\u001b[0m             \u001b[0mnon_scalar_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrewrite_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnon_scalar_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 970\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mnon_scalar_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train/global_step\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/preinit.py\u001b[0m in \u001b[0;36mpreinit_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m ) -> Callable:\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpreinit_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"You must call wandb.init() before {name}()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mpreinit_wrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mError\u001b[0m: You must call wandb.init() before wandb.log()"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiq5bRFTmzv5"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"What is the weather like today?\"\n",
        "\n",
        "\n",
        "inputs = tokenizer(\n",
        "    sentence,\n",
        "    return_tensors=\"pt\",\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "    max_length=512\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "am0boXLrUCXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()  # Đặt mô hình ở chế độ đánh giá (không tính gradient)\n",
        "with torch.no_grad():  # Không cần tính gradient\n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "    predicted_class = torch.argmax(logits, dim=1).item()  # Lấy nhãn dự đoán\n",
        "    print(f\"Predicted class: {predicted_class}\")\n"
      ],
      "metadata": {
        "id": "yB4bStFOUDkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTTbVHd551js"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}