{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLWoGrdRXTSg"
      },
      "source": [
        "## 1. Tải Dữ Liệu từ CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbMKcafkKCPk",
        "outputId": "862eb7e6-18e0-4a0b-ea52-2abaf074492a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.27.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zIVYywvyiq2L"
      },
      "outputs": [],
      "source": [
        "\n",
        "from transformers import AutoTokenizer, TrainingArguments, Trainer, AutoModel\n",
        "import numpy as np\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "import torch.nn as nn\n",
        "import os\n",
        "from typing import List\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\" ## Setup CUDA GPU 1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SHnftkneP6B",
        "outputId": "f636162f-b47e-4de5-a307-57da63ce1541"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: True\n",
            "Number of GPUs: 1\n",
            "GPU 0: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Kiểm tra GPU khả dụng\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"Number of GPUs:\", torch.cuda.device_count())\n",
        "if torch.cuda.is_available():\n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
        "else:\n",
        "    print(\"No GPU found.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ge41QgJadXLv",
        "outputId": "0534528f-1114-4a7e-9497-d5df6f6f318f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of GPUs available: 1\n",
            "Available GPUs: ['Tesla T4']\n",
            "Using GPU: Tesla T4\n",
            "Final selected device: cuda:0\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "def select_gpu():\n",
        "    \"\"\"\n",
        "    Kiểm tra GPU khả dụng và tự động chọn GPU phù hợp.\n",
        "    \"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        num_gpus = torch.cuda.device_count()\n",
        "        print(f\"Number of GPUs available: {num_gpus}\")\n",
        "\n",
        "        # Duyệt qua các GPU khả dụng để tìm GPU ít sử dụng nhất\n",
        "        available_gpus = [torch.cuda.get_device_name(i) for i in range(num_gpus)]\n",
        "        print(\"Available GPUs:\", available_gpus)\n",
        "\n",
        "        for i in range(num_gpus):\n",
        "            try:\n",
        "                # Đặt GPU\n",
        "                os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(i)\n",
        "                device = torch.device(f\"cuda:{i}\")\n",
        "                torch.cuda.set_device(device)\n",
        "                print(f\"Using GPU: {torch.cuda.get_device_name(device.index)}\")\n",
        "                return device\n",
        "            except Exception as e:\n",
        "                print(f\"GPU {i} is not suitable: {e}\")\n",
        "\n",
        "        print(\"No suitable GPU found. Falling back to CPU.\")\n",
        "        return torch.device(\"cpu\")\n",
        "    else:\n",
        "        print(\"No GPUs available. Using CPU.\")\n",
        "        return torch.device(\"cpu\")\n",
        "\n",
        "# Tự động chọn GPU hoặc CPU\n",
        "device = select_gpu()\n",
        "\n",
        "# Kiểm tra lại thiết bị đang sử dụng\n",
        "print(f\"Final selected device: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "IkfPfDY3iskg"
      },
      "outputs": [],
      "source": [
        "\n",
        "class BERTIntentClassification(nn.Module):\n",
        "\n",
        "\n",
        "    def __init__(self, model_name=\"bert-base-uncased\", num_classes=10, dropout_rate=0.1, cache_dir = \"huggingface\"):\n",
        "        super(BERTIntentClassification, self).__init__()\n",
        "        self.bert = AutoModel.from_pretrained(model_name, cache_dir = cache_dir)\n",
        "        # Get BERT hidden size\n",
        "        hidden_size = self.bert.config.hidden_size\n",
        "        self.ffnn = nn.Sequential(\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.LayerNorm(hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(hidden_size, num_classes)\n",
        "        )\n",
        "\n",
        "\n",
        "    def freeze_bert(self):\n",
        "        for param in self.bert.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "\n",
        "    def get_pooling(self, hidden_state, attention_mask):\n",
        "        \"\"\"\n",
        "        Get mean pooled representation from BERT hidden states\n",
        "        Args:\n",
        "            hidden_state: BERT output containing hidden states\n",
        "        Returns:\n",
        "            pooled_output: Mean pooled representation of the sequence\n",
        "        \"\"\"\n",
        "        # Get last hidden state\n",
        "        last_hidden_state = hidden_state.last_hidden_state  # Shape: [batch_size, seq_len, hidden_size]\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            # Expand attention mask to match hidden state dimensions\n",
        "            attention_mask = attention_mask.unsqueeze(-1)  # [batch_size, seq_len, 1]\n",
        "\n",
        "            # Mask out padding tokens\n",
        "            masked_hidden = last_hidden_state * attention_mask\n",
        "\n",
        "            # Calculate mean (sum / number of actual tokens)\n",
        "            sum_hidden = torch.sum(masked_hidden, dim=1)  # [batch_size, hidden_size]\n",
        "            count_tokens = torch.sum(attention_mask, dim=1)  # [batch_size, 1]\n",
        "            pooled_output = sum_hidden / count_tokens\n",
        "        else:\n",
        "            # If no attention mask, simply take mean of all tokens\n",
        "            pooled_output = torch.mean(last_hidden_state, dim=1)\n",
        "\n",
        "        return pooled_output\n",
        "\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, **kwargs):\n",
        "        \"\"\"\n",
        "        Forward pass of the model\n",
        "        Args:\n",
        "            input_ids: Input token IDs\n",
        "            attention_mask: Attention mask for padding\n",
        "        Returns:\n",
        "            logits: Raw logits for each class\n",
        "        \"\"\"\n",
        "        # Get BERT hidden states\n",
        "        hidden_state = self.bert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "        )\n",
        "\n",
        "        # Get pooled representation\n",
        "        hidden_state_pooling = self.get_pooling(hidden_state=hidden_state, attention_mask=attention_mask)\n",
        "\n",
        "        # Pass through FFNN classifier\n",
        "        logits = self.ffnn(hidden_state_pooling)\n",
        "\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "zypDXvoaivAb"
      },
      "outputs": [],
      "source": [
        "class TrainerCustom(Trainer):\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        \"\"\"\n",
        "        How the loss is computed by Trainer. By default, all models return the loss in the first element.\n",
        "\n",
        "        Subclass and override for custom behavior.\n",
        "        \"\"\"\n",
        "        if \"labels\" in inputs:\n",
        "            labels = inputs.pop(\"labels\")\n",
        "        else:\n",
        "            labels = None\n",
        "\n",
        "        # Sử dụng nn.CrossEntropyLoss() thay vì nn.CrossEntropy\n",
        "        cross_entropy_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "        # Chạy mô hình và nhận đầu ra (logits)\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "        # Đảm bảo lấy logits từ outputs (mô hình trả về tuple, lấy phần tử đầu tiên là logits)\n",
        "        logits = outputs\n",
        "\n",
        "        # Tính toán loss\n",
        "        loss = cross_entropy_loss(logits, labels)\n",
        "\n",
        "        # Trả về loss và outputs nếu cần\n",
        "        return (loss, outputs) if return_outputs else loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zIJIYGcppMk"
      },
      "source": [
        "# 1. Load Dataset and with Dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjW1FC-ibueZ"
      },
      "source": [
        "\n",
        "**Cách kết hợp question và answer:**\n",
        "\n",
        "1. **Ghép nối trực tiếp:** Bạn có thể kết hợp câu hỏi và câu trả lời thành một chuỗi duy nhất, sử dụng một ký tự đặc biệt hoặc dấu phân cách để tách biệt chúng. Ví dụ:\n",
        "\n",
        "   ```python\n",
        "   combined_text = question + \" [SEP] \" + answer\n",
        "   ```\n",
        "\n",
        "   Trong đó, `[SEP]` là một token đặc biệt thường được sử dụng trong các mô hình như BERT để phân tách các đoạn văn bản khác nhau.\n",
        "\n",
        "2. **Sử dụng token đặc biệt:** Một số mô hình hỗ trợ các token đặc biệt để đánh dấu bắt đầu và kết thúc của câu hỏi và câu trả lời. Ví dụ:\n",
        "\n",
        "   ```python\n",
        "   combined_text = \"[CLS] \" + question + \" [SEP] \" + answer + \" [SEP]\"\n",
        "   ```\n",
        "\n",
        "   - `[CLS]`: Token đánh dấu bắt đầu chuỗi (thường dùng trong BERT).\n",
        "   - `[SEP]`: Token phân tách giữa các phần của chuỗi.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyd8l8VPdgpg"
      },
      "source": [
        "Ví dụ thực tế về chuỗi chuẩn:\n",
        "\n",
        "Một câu/đoạn duy nhất:\n",
        "```\n",
        "[CLS] This is the first sentence. [SEP]\n",
        "```\n",
        "Hai câu/đoạn (ví dụ: câu hỏi và trả lời):\n",
        "```\n",
        "[CLS] What is your name? [SEP] My name is John. [SEP]\n",
        "```\n",
        "Nhiều câu/đoạn (3 đoạn):\n",
        "```\n",
        "[CLS] Question 1 [SEP] Answer 1 [SEP] Extra information [SEP]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSOI6zlGfQV_"
      },
      "source": [
        "```\n",
        "                          input_ids  intent\n",
        "0  [CLS] Cậu có muốn tiếp tục không? [SEP]  silence\n",
        "1                          [CLS] [SEP]  silence\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### xỬ LÝ VẤN ĐỀ BỊ THỪA: CLS, SEP CỦA CÁC VERSION TRƯỚC + BERT SẼ HOẠT ĐỘNG TỐT VỚI CHỮ THƯỜNG: lowercase và - question: {question}. answer: {}\n",
        "- dùng lowercase với BERT mà\n",
        "\n",
        "\n",
        "Việc thêm `[CLS]` và `[SEP]` theo cách thủ công như bạn thực hiện là **không cần thiết** và có thể **không tương thích** với cách mà tokenizer của BERT được thiết kế hoạt động. Cụ thể:\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Hoạt động mặc định của BERT Tokenizer**\n",
        "- **`[CLS]`**: Là token đặc biệt được thêm tự động vào đầu mỗi câu bởi tokenizer. BERT sử dụng token này để biểu diễn toàn bộ ngữ cảnh của câu.\n",
        "- **`[SEP]`**: Là token đặc biệt được sử dụng để phân tách các câu khi có nhiều đoạn văn trong một input (e.g., `text` và `text_pair`).\n",
        "\n",
        "Khi bạn sử dụng `tokenizer` với `text` và `text_pair`, tokenizer **tự động thêm `[CLS]` và `[SEP]`** vào đầu và cuối các câu, nên bạn không cần phải thêm thủ công.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Những vấn đề khi thêm `[CLS]` và `[SEP]` thủ công**\n",
        "- **Redundant Tokens**: Bạn đang thêm `[CLS]` và `[SEP]` bằng tay, nhưng sau đó tokenizer lại thêm **lần nữa**, dẫn đến chuỗi input bị thừa các token đặc biệt. Ví dụ:\n",
        "\n",
        "  ```python\n",
        "  input_text = \"[CLS] Cậu có muốn học thêm về các cụm từ khác không? [SEP] Không, tớ không muốn học thêm đâu. [SEP]\"\n",
        "  tokenizer(input_text)\n",
        "  # Tokenizer sẽ thêm [CLS] và [SEP] lần nữa -> Dẫn đến lỗi hoặc input không chính xác.\n",
        "  ```\n",
        "\n",
        "- **Không tuân theo pretraining**: BERT được huấn luyện trước với cơ chế tự động thêm `[CLS]` và `[SEP]`. Nếu bạn thêm bằng tay, mô hình có thể xử lý input không chính xác vì không phù hợp với cách mà mô hình đã được huấn luyện trước đó.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "sDE8LWW7CIDP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. **Loại Bỏ `[CLS]` và `[SEP]`:**\n",
        "   - Tokenizer của BERT sẽ tự động thêm `[CLS]` ở đầu chuỗi và `[SEP]` giữa các đoạn văn bản (và ở cuối nếu cần).\n",
        "   - Điều này giúp tránh dư thừa token.\n",
        "\n",
        "2. **Chuẩn Hóa Chuỗi:**\n",
        "   - Chuyển toàn bộ chuỗi về **lowercase** để nhất quán với tokenizer (đặc biệt với BERT mặc định sử dụng lowercase).\n",
        "\n",
        "3. **Định Dạng Chuỗi:**\n",
        "   - Sử dụng định dạng chuẩn:\n",
        "     ```plaintext\n",
        "     question: {nội dung câu hỏi}. answer: {nội dung câu trả lời}.\n",
        "     ```\n"
      ],
      "metadata": {
        "id": "8muRMiTQLxE3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHXznY2KbtzB",
        "outputId": "497cd90c-72bb-443b-be43-d69a8702d02d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['input_ids', 'label'],\n",
            "    num_rows: 15694\n",
            "})\n",
            "Dataset({\n",
            "    features: ['input_ids', 'label'],\n",
            "    num_rows: 20\n",
            "})\n",
            "First row in dataset:\n",
            "{'input_ids': '[CLS] Được rồi, bây giờ chúng ta sẽ chơi một trò chơi! Hãy kể tên nhiều từ thuộc cùng 1 chủ đề nhé. Chủ đề lần này là hành động bắt đầu bằng từ \"eat food\". Tớ ví dụ nhé, \"eat pizza\", đến lượt cậu nhé [SEP] Tớ ăn cơm. [SEP]', 'label': 'intent_positive'}\n",
            "{'input_ids': '[CLS] Cậu có muốn chơi tiếp không? [SEP]', 'label': 'silence'}\n"
          ]
        }
      ],
      "source": [
        "# import pandas as pd\n",
        "# from datasets import Dataset\n",
        "\n",
        "# def combine_with_special_tokens(row, text_columns, cls_token=\"[CLS]\", sep_token=\"[SEP]\"):\n",
        "#     \"\"\"\n",
        "#     Thêm các token đặc biệt vào chuỗi kết hợp từ các cột văn bản.\n",
        "\n",
        "#     Args:\n",
        "#         row (pd.Series): Dòng dữ liệu từ DataFrame.\n",
        "#         text_columns (list): Danh sách các cột văn bản cần kết hợp.\n",
        "#         cls_token (str): Token bắt đầu câu.\n",
        "#         sep_token (str): Token phân cách.\n",
        "\n",
        "#     Returns:\n",
        "#         str: Chuỗi văn bản đã thêm token đặc biệt.\n",
        "#     \"\"\"\n",
        "#     tokens = [cls_token]  # Thêm [CLS] đầu tiên\n",
        "\n",
        "#     # Thêm nội dung từ các cột văn bản\n",
        "#     for col in text_columns:\n",
        "#         if pd.notna(row[col]) and row[col].strip():  # Kiểm tra không rỗng\n",
        "#             tokens.append(row[col].strip())\n",
        "#             tokens.append(sep_token)  # Thêm [SEP] sau mỗi đoạn\n",
        "\n",
        "#     # Nếu không có nội dung nào được thêm, chỉ giữ lại [CLS] và [SEP]\n",
        "#     if len(tokens) == 1:\n",
        "#         tokens.append(sep_token)\n",
        "\n",
        "#     return \" \".join(tokens)\n",
        "\n",
        "# def load_xlsx_dataset(xlsx_path, text_columns, label_column, cls_token=\"[CLS]\", sep_token=\"[SEP]\"):\n",
        "#     \"\"\"\n",
        "#     Tải dataset từ file Excel (.xlsx) và xử lý dữ liệu.\n",
        "\n",
        "#     Args:\n",
        "#         xlsx_path (str): Đường dẫn đến file .xlsx.\n",
        "#         text_columns (list): Danh sách các cột cần ghép để tạo văn bản đầu vào.\n",
        "#         label_column (str): Tên cột chứa nhãn.\n",
        "#         cls_token (str): Token bắt đầu câu.\n",
        "#         sep_token (str): Token phân cách.\n",
        "\n",
        "#     Returns:\n",
        "#         Dataset: Tập dữ liệu đã xử lý.\n",
        "#     \"\"\"\n",
        "#     # Đọc file Excel bằng pandas\n",
        "#     df = pd.read_excel(xlsx_path)\n",
        "\n",
        "#     # Kiểm tra các cột cần thiết\n",
        "#     for col in text_columns + [label_column]:\n",
        "#         if col not in df.columns:\n",
        "#             raise ValueError(f\"Missing required column: {col}\")\n",
        "\n",
        "#     # Ghép các cột text lại thành một chuỗi duy nhất với token đặc biệt\n",
        "#     df[\"input_ids\"] = df.apply(lambda row: combine_with_special_tokens(row, text_columns, cls_token, sep_token), axis=1)\n",
        "\n",
        "#     # Đổi tên cột nhãn\n",
        "#     df = df.rename(columns={label_column: \"label\"})\n",
        "\n",
        "#     # Chuyển đổi DataFrame thành Dataset\n",
        "#     dataset = Dataset.from_pandas(df[[\"input_ids\", \"label\"]])\n",
        "\n",
        "#     return dataset\n",
        "\n",
        "# # Sử dụng hàm\n",
        "# xlsx_path = \"/content/processed_data_example_v4_15000Data.xlsx\"  # Đường dẫn file Excel\n",
        "# text_columns = [\"robot\", \"user_answer\"]  # Các cột cần ghép\n",
        "# label_column = \"user_intent\"  # Cột chứa nhãn\n",
        "\n",
        "# # Tải dataset từ Excel\n",
        "# dataset = load_xlsx_dataset(xlsx_path, text_columns, label_column)\n",
        "\n",
        "# # Kiểm tra dữ liệu\n",
        "# print(dataset)\n",
        "\n",
        "# # Lấy 10 mẫu đầu tiên\n",
        "# sample_dataset = dataset.select(range(20))\n",
        "# print(sample_dataset)\n",
        "\n",
        "# # In thử một hàng\n",
        "# print(\"First row in dataset:\")\n",
        "# print(sample_dataset[0])\n",
        "# print(sample_dataset[11])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "\n",
        "def combine_with_format(row, question_col, answer_col):\n",
        "    \"\"\"\n",
        "    Kết hợp dữ liệu từ cột câu hỏi và câu trả lời theo định dạng chuẩn.\n",
        "\n",
        "    Args:\n",
        "        row (pd.Series): Dòng dữ liệu từ DataFrame.\n",
        "        question_col (str): Tên cột chứa câu hỏi.\n",
        "        answer_col (str): Tên cột chứa câu trả lời.\n",
        "\n",
        "    Returns:\n",
        "        str: Chuỗi định dạng chuẩn \"question: {câu hỏi}. answer: {câu trả lời}.\"\n",
        "    \"\"\"\n",
        "    question = row[question_col].strip().lower() if pd.notna(row[question_col]) else \"\"\n",
        "    answer = row[answer_col].strip().lower() if pd.notna(row[answer_col]) else \"\"\n",
        "    return f\"question: {question}. answer: {answer}.\"\n",
        "\n",
        "def load_xlsx_dataset_v3(xlsx_path, question_col, answer_col, label_col):\n",
        "    \"\"\"\n",
        "    Tải và xử lý dữ liệu từ file Excel, chuyển đổi định dạng đầu vào.\n",
        "\n",
        "    Args:\n",
        "        xlsx_path (str): Đường dẫn đến file Excel.\n",
        "        question_col (str): Tên cột chứa câu hỏi.\n",
        "        answer_col (str): Tên cột chứa câu trả lời.\n",
        "        label_col (str): Tên cột chứa nhãn.\n",
        "\n",
        "    Returns:\n",
        "        Dataset: Dữ liệu đã xử lý.\n",
        "    \"\"\"\n",
        "    # Đọc file Excel\n",
        "    df = pd.read_excel(xlsx_path)\n",
        "\n",
        "    # Kiểm tra các cột\n",
        "    for col in [question_col, answer_col, label_col]:\n",
        "        if col not in df.columns:\n",
        "            raise ValueError(f\"Missing required column: {col}\")\n",
        "\n",
        "    # Kết hợp văn bản\n",
        "    df[\"input_ids\"] = df.apply(lambda row: combine_with_format(row, question_col, answer_col), axis=1)\n",
        "\n",
        "    # Giữ nguyên nhãn không ánh xạ\n",
        "    df = df.rename(columns={label_col: \"label\"})\n",
        "\n",
        "    # Chuyển đổi DataFrame thành Dataset\n",
        "    dataset = Dataset.from_pandas(df[[\"input_ids\", \"label\"]])\n",
        "\n",
        "    return dataset\n",
        "\n",
        "# Sử dụng hàm\n",
        "xlsx_path = \"/content/processed_data_example_v4_15000Data.xlsx\"  # File dữ liệu\n",
        "question_col = \"robot\"  # Cột câu hỏi\n",
        "answer_col = \"user_answer\"  # Cột câu trả lời\n",
        "label_col = \"user_intent\"  # Cột nhãn\n",
        "\n",
        "# Tải dataset từ Excel\n",
        "dataset = load_xlsx_dataset_v3(xlsx_path, question_col, answer_col, label_col)\n",
        "\n",
        "# Kiểm tra dữ liệu\n",
        "print(dataset)\n",
        "\n",
        "# Lấy 10 mẫu đầu tiên\n",
        "sample_dataset = dataset.select(range(10))\n",
        "print(sample_dataset)\n",
        "\n",
        "# In thử một hàng\n",
        "print(\"First row in dataset:\")\n",
        "print(sample_dataset[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Ab9NvyrEWZX",
        "outputId": "ba7eeac8-5a08-4c83-9e4c-5c742e31b9e0"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['input_ids', 'label'],\n",
            "    num_rows: 15694\n",
            "})\n",
            "Dataset({\n",
            "    features: ['input_ids', 'label'],\n",
            "    num_rows: 10\n",
            "})\n",
            "First row in dataset:\n",
            "{'input_ids': 'question: được rồi, bây giờ chúng ta sẽ chơi một trò chơi! hãy kể tên nhiều từ thuộc cùng 1 chủ đề nhé. chủ đề lần này là hành động bắt đầu bằng từ \"eat food\". tớ ví dụ nhé, \"eat pizza\", đến lượt cậu nhé. answer: tớ ăn cơm..', 'label': 'intent_positive'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1oFci1j5k1UY",
        "outputId": "1ffd30e5-cffd-46ad-fa16-223320f945aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Invalid Samples =====\n",
            "[]\n"
          ]
        }
      ],
      "source": [
        "def check_invalid_samples(dataset):\n",
        "    invalid_samples = []\n",
        "    for idx, sample in enumerate(dataset):\n",
        "        if not isinstance(sample[\"input_ids\"], str) or sample[\"input_ids\"].strip() == \"\":\n",
        "            invalid_samples.append((idx, sample))\n",
        "    return invalid_samples\n",
        "\n",
        "# Kiểm tra dữ liệu không hợp lệ\n",
        "invalid_samples = check_invalid_samples(dataset)\n",
        "print(\"\\n===== Invalid Samples =====\")\n",
        "print(invalid_samples)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240,
          "referenced_widgets": [
            "ab53a6542f784faeb5940f8a593e34c0",
            "02e82d56873c4b908f20d8c17279f4bf",
            "d31163b2d1004ff0bd67f262d2231783",
            "915eea616078437e850131c73c9e48fc",
            "6f9df60a6b404375b9e8042d1957dfa3",
            "89d77fee09354e818ddeb591f00812fc",
            "884fcf6db2b642648fd6532edf0f375c",
            "59f098f6f3b14d6ab6b7787f34fd177e",
            "212d65d42a814563b66513fb39abd94e",
            "a276388a7f00445ca3b24c40bb5d1981",
            "708722b714de4bc9beff893c7c7ad30c"
          ]
        },
        "id": "x0-3V7dLlCZp",
        "outputId": "37a2d291-04fb-4fa9-fc36-46faf9f7bfac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ánh xạ nhãn: {'intent_fallback': 0, 'intent_learn_more': 1, 'intent_negative': 2, 'intent_neutral': 3, 'intent_positive': 4, 'silence': 5}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/15694 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ab53a6542f784faeb5940f8a593e34c0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['input_ids', 'label'],\n",
            "    num_rows: 15694\n",
            "})\n",
            "Dataset({\n",
            "    features: ['input_ids', 'label'],\n",
            "    num_rows: 10\n",
            "})\n",
            "First row in sample_dataset:\n",
            "{'input_ids': 'question: tốt lắm! cậu có thể kể thêm từ nào khác không?. answer: tớ ăn bánh mì..', 'label': 4}\n"
          ]
        }
      ],
      "source": [
        "# Tự động phát hiện nhãn và tạo ánh xạ nhãn\n",
        "def create_label_mapping(dataset_list):\n",
        "    \"\"\"\n",
        "    Tự động phát hiện tất cả các nhãn từ danh sách dataset và ánh xạ chúng thành số nguyên.\n",
        "    \"\"\"\n",
        "    all_labels = set()\n",
        "    for dataset in dataset_list:\n",
        "        all_labels.update(dataset[\"label\"])  # Tập hợp tất cả các nhãn từ dataset\n",
        "\n",
        "    label_to_int = {label: idx for idx, label in enumerate(sorted(all_labels))}\n",
        "    print(f\"Ánh xạ nhãn: {label_to_int}\")\n",
        "    return label_to_int\n",
        "\n",
        "# Hàm chuyển đổi nhãn\n",
        "def preprocess_labels(example, label_to_int):\n",
        "    example[\"label\"] = label_to_int.get(example[\"label\"], -1)  # Gán -1 cho nhãn không hợp lệ\n",
        "    return example\n",
        "\n",
        "# Tạo ánh xạ nhãn\n",
        "label_mapping = create_label_mapping([dataset])\n",
        "\n",
        "# Áp dụng chuyển đổi nhãn\n",
        "dataset = dataset.map(lambda example: preprocess_labels(example, label_mapping))\n",
        "\n",
        "# Kiểm tra kết quả\n",
        "print(dataset)\n",
        "\n",
        "# Truy cập mẫu cụ thể\n",
        "sample_dataset = dataset.select(range(10))  # Lấy 10 mẫu đầu tiên\n",
        "print(sample_dataset)\n",
        "\n",
        "# In thử 1 hàng trong sample_dataset\n",
        "print(\"First row in sample_dataset:\")\n",
        "print(sample_dataset[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "hy_jM2gOk99M"
      },
      "outputs": [],
      "source": [
        "# def split_dataset(dataset, test_size=0.2, seed=42):\n",
        "#     \"\"\"\n",
        "#     Chia dataset thành tập train và test.\n",
        "\n",
        "#     Args:\n",
        "#         dataset (Dataset): Tập dữ liệu đầy đủ.\n",
        "#         test_size (float): Tỷ lệ dữ liệu test (0.0 - 1.0).\n",
        "#         seed (int): Seed để chia dữ liệu ngẫu nhiên.\n",
        "\n",
        "#     Returns:\n",
        "#         tuple: (train_dataset, test_dataset) - Tập train và test.\n",
        "#     \"\"\"\n",
        "#     if not (0.0 < test_size < 1.0):\n",
        "#         raise ValueError(\"test_size phải nằm trong khoảng (0.0, 1.0)\")\n",
        "#     if len(dataset) < 2:\n",
        "#         raise ValueError(\"Dataset phải có ít nhất 2 mẫu để chia.\")\n",
        "\n",
        "#     train_test_split = dataset.train_test_split(test_size=test_size, seed=seed)\n",
        "#     print(f\"Chia dataset: {len(train_test_split['train'])} mẫu train, {len(train_test_split['test'])} mẫu test\")\n",
        "#     return train_test_split[\"train\"], train_test_split[\"test\"]\n",
        "\n",
        "# # Chia dataset\n",
        "# train_dataset, test_dataset = split_dataset(dataset, test_size=0.3)\n",
        "\n",
        "# # Kiểm tra dữ liệu\n",
        "# print(\"Train dataset:\", train_dataset)\n",
        "# print(\"Test dataset:\", test_dataset)\n",
        "\n",
        "# # Truy cập mẫu cụ thể\n",
        "# sample_train_dataset = train_dataset.select(range(8))  # Lấy 10 mẫu đầu tiên từ train\n",
        "# sample_test_dataset = test_dataset.select(range(5))    # Lấy 10 mẫu đầu tiên từ test\n",
        "\n",
        "# print(\"Sample train dataset:\", sample_train_dataset)\n",
        "# print(\"Sample test dataset:\", sample_test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_dataset(dataset, test_size=0.2, valid_size=0.2, seed=42):\n",
        "    \"\"\"\n",
        "    Chia dataset thành tập train, valid, và test.\n",
        "\n",
        "    Args:\n",
        "        dataset (Dataset): Tập dữ liệu đầy đủ.\n",
        "        test_size (float): Tỷ lệ dữ liệu test (0.0 - 1.0).\n",
        "        valid_size (float): Tỷ lệ dữ liệu valid so với tập train ban đầu (0.0 - 1.0).\n",
        "        seed (int): Seed để chia dữ liệu ngẫu nhiên.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (train_dataset, valid_dataset, test_dataset) - Tập train, valid, và test.\n",
        "    \"\"\"\n",
        "    if not (0.0 < test_size < 1.0):\n",
        "        raise ValueError(\"test_size phải nằm trong khoảng (0.0, 1.0)\")\n",
        "    if not (0.0 < valid_size < 1.0):\n",
        "        raise ValueError(\"valid_size phải nằm trong khoảng (0.0, 1.0)\")\n",
        "    if len(dataset) < 3:\n",
        "        raise ValueError(\"Dataset phải có ít nhất 3 mẫu để chia.\")\n",
        "\n",
        "    # Chia tập train và test\n",
        "    train_test_split = dataset.train_test_split(test_size=test_size, seed=seed, shuffle=False)\n",
        "    train_dataset = train_test_split[\"train\"]\n",
        "    test_dataset = train_test_split[\"test\"]\n",
        "\n",
        "    # Chia tập train thành train và valid\n",
        "    valid_split = train_dataset.train_test_split(test_size=valid_size, seed=seed)\n",
        "    train_dataset = valid_split[\"train\"]\n",
        "    valid_dataset = valid_split[\"test\"]\n",
        "\n",
        "    print(f\"Chia dataset: {len(train_dataset)} mẫu train, {len(valid_dataset)} mẫu valid, {len(test_dataset)} mẫu test\")\n",
        "    return train_dataset, valid_dataset, test_dataset\n",
        "\n",
        "\n",
        "# Chia dataset\n",
        "train_dataset, valid_dataset, test_dataset = split_dataset(dataset, test_size=0.2, valid_size=0.2)\n",
        "\n",
        "# Kiểm tra dữ liệu\n",
        "print(\"Train dataset:\", train_dataset)\n",
        "print(\"Valid dataset:\", valid_dataset)\n",
        "print(\"Test dataset:\", test_dataset)\n",
        "\n",
        "# Truy cập mẫu cụ thể\n",
        "sample_train_dataset = train_dataset.select(range(8))  # Lấy 8 mẫu đầu tiên từ train\n",
        "sample_valid_dataset = valid_dataset.select(range(5))  # Lấy 5 mẫu đầu tiên từ valid\n",
        "sample_test_dataset = test_dataset.select(range(5))    # Lấy 5 mẫu đầu tiên từ test\n",
        "\n",
        "print(\"Sample train dataset:\", sample_train_dataset)\n",
        "print(\"Sample valid dataset:\", sample_valid_dataset)\n",
        "print(\"Sample test dataset:\", sample_test_dataset)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5maHogdB1pho",
        "outputId": "26bce46e-c8c2-4558-bc6c-d771d7b78590"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chia dataset: 10044 mẫu train, 2511 mẫu valid, 3139 mẫu test\n",
            "Train dataset: Dataset({\n",
            "    features: ['input_ids', 'label'],\n",
            "    num_rows: 10044\n",
            "})\n",
            "Valid dataset: Dataset({\n",
            "    features: ['input_ids', 'label'],\n",
            "    num_rows: 2511\n",
            "})\n",
            "Test dataset: Dataset({\n",
            "    features: ['input_ids', 'label'],\n",
            "    num_rows: 3139\n",
            "})\n",
            "Sample train dataset: Dataset({\n",
            "    features: ['input_ids', 'label'],\n",
            "    num_rows: 8\n",
            "})\n",
            "Sample valid dataset: Dataset({\n",
            "    features: ['input_ids', 'label'],\n",
            "    num_rows: 5\n",
            "})\n",
            "Sample test dataset: Dataset({\n",
            "    features: ['input_ids', 'label'],\n",
            "    num_rows: 5\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "    print(sample_test_dataset[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WU8e9Orx4flS",
        "outputId": "c6228a87-dce4-4016-f7b4-f0ec345bd990"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': \"question: cậu có muốn học thêm về các cụm từ khác không? maybe something like 'run fast'?. answer: không, tớ không muốn học thêm đâu..\", 'label': 2}\n",
            "{'input_ids': \"question: cậu có hiểu cách sử dụng 'jump now' không? do you feel confident using it?. answer: tớ không biết, có thể tớ cần thêm thời gian..\", 'label': 3}\n",
            "{'input_ids': \"question: cậu có thể nói lại câu 'jump now' không? can you repeat it?. answer: tớ không chắc lắm, nhưng tớ sẽ thử..\", 'label': 3}\n",
            "{'input_ids': \"question: cậu có muốn tìm hiểu thêm về các động từ hành động không? like 'jump'?. answer: có, tớ muốn biết thêm về các động từ khác..\", 'label': 1}\n",
            "{'input_ids': \"question: cậu có muốn học cách kết hợp 'jump now' với các câu khác không? for example, 'jump now and have fun!'. answer: ừ, tớ muốn học thêm về cách kết hợp..\", 'label': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCzQIjl_ptbw"
      },
      "source": [
        "# 2. Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24GkcP7XhXn3",
        "outputId": "e613576f-899f-4468-d850-e17465263b94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'intent_fallback': 0, 'intent_learn_more': 1, 'intent_negative': 2, 'intent_neutral': 3, 'intent_positive': 4, 'silence': 5}\n",
            "Number of unique labels: 6\n"
          ]
        }
      ],
      "source": [
        "# Calculate the number of unique labels\n",
        "print(label_mapping)\n",
        "number_label = len(label_mapping)\n",
        "print(\"Number of unique labels:\", number_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "bknqLH2piJMv"
      },
      "outputs": [],
      "source": [
        "# Bước 2: Chuẩn bị tokenizer và token hóa dữ liệu\n",
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir = \"huggingface\")\n",
        "model = BERTIntentClassification(\n",
        "    model_name=model_name,\n",
        "    num_classes=6\n",
        ")\n",
        "model.freeze_bert() # Froze Layer BERT\n",
        "max_seq_length = 512\n",
        "\n",
        "\n",
        "def collate_fn(features):\n",
        "    inputs = []\n",
        "    labels = []\n",
        "    for element in features:\n",
        "        inputs.append(element.get(\"input_ids\"))\n",
        "        labels.append(element.get(\"label\"))\n",
        "\n",
        "    labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    token_inputs = tokenizer(\n",
        "        inputs,                           # Chuỗi đầu vào (sau khi ghép nối từ `robot` và `user_answer`).\n",
        "        add_special_tokens=True,          # Tự động thêm các token đặc biệt như [CLS], [SEP] vào chuỗi đầu vào.\n",
        "        truncation=True,                  # Cắt bớt chuỗi nếu độ dài vượt quá `max_length`.\n",
        "        padding=True,                     # Thêm padding (với token [PAD]) để đảm bảo tất cả các chuỗi có độ dài bằng nhau.\n",
        "        max_length=max_seq_length,        # Độ dài tối đa của chuỗi sau khi token hóa (ví dụ: 512).\n",
        "        return_overflowing_tokens=False,  # Không trả về các đoạn token vượt quá `max_length`.\n",
        "        return_length=False,              # Không trả về độ dài của từng chuỗi token đã tạo ra.\n",
        "        return_tensors=\"pt\",              # Trả về đầu ra dưới dạng tensor của PyTorch (định dạng này cần thiết để sử dụng với mô hình).\n",
        "    )\n",
        "\n",
        "    token_inputs.update({\n",
        "        \"labels\": labels,\n",
        "    })\n",
        "    return token_inputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZldUk54pj1N"
      },
      "source": [
        "# 3. Train Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptK7Cy22p2GK"
      },
      "source": [
        "## 3.1 Log Wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZkR3vuFp5uN",
        "outputId": "2e52a43d-956f-4649-b64e-ca4577732bac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.19.3)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.25.5)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.10.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.19.2)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.12.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.12.14)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qux2ABzMp7Ec",
        "outputId": "d4029e49-56ba-4ec9-b0ee-fb19a476aece"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (1.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZRspV7jp8OT",
        "outputId": "a26396d0-0bd8-4c48-fc5a-96adc54d2653"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "c8767\n"
          ]
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "# Load biến môi trường từ file .env\n",
        "load_dotenv()\n",
        "\n",
        "# Lấy key từ biến môi trường\n",
        "wandb_api_key = os.getenv(\"WANDB_API_KEY\")\n",
        "print(wandb_api_key[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "215vQ7cOp9oo",
        "outputId": "b347a7a6-e607-49b6-cbd4-afb527705cce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ],
      "source": [
        "import wandb\n",
        "import os\n",
        "\n",
        "# Lấy API key từ biến môi trường và đăng nhập\n",
        "wandb.login(key=os.getenv(\"WANDB_API_KEY\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJG_rUYTqwLJ"
      },
      "source": [
        "Cách thiết lập thông qua TrainingArguments\n",
        "Khi sử dụng Trainer, bạn có thể đặt tên dự án trực tiếp trong TrainingArguments bằng cách sử dụng tham số report_to và run_name. Tuy nhiên, để đặt project, bạn cần khởi tạo một phiên wandb trước hoặc truyền cấu hình này thông qua wandb.init().\n",
        "\n",
        "Điều chỉnh TrainingArguments:\n",
        "```python\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results_\",          # Thư mục lưu kết quả\n",
        "    eval_strategy=\"epoch\",           # Đánh giá sau mỗi epoch\n",
        "    learning_rate=2e-4,\n",
        "    per_device_train_batch_size=128,\n",
        "    per_device_eval_batch_size=128,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",            # Thư mục lưu log\n",
        "    logging_strategy=\"steps\",        # Log theo steps\n",
        "    logging_steps=10,                # Log sau mỗi 10 bước\n",
        "    save_strategy=\"epoch\",           # Lưu checkpoint sau mỗi epoch\n",
        "    save_total_limit=3,              # Lưu tối đa 3 checkpoint\n",
        "    report_to=\"wandb\",               # Báo cáo log tới wandb\n",
        "    run_name=\"bert_run_1\"            # Tên phiên chạy trên wandb\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAMgPe3NqAhz"
      },
      "source": [
        "## 3.2 Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3hWcz751mEx"
      },
      "source": [
        "### Ver 1.2.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLIKjrdaesyG"
      },
      "source": [
        "Thui, ko lưu local nữa, lưu tất trên wandb đi.\n",
        "- Với best model: lưu lên wandb khi loss giảm và đã sau 10 epochs  \n",
        "(Lưu Best Model ngay khi eval_loss giảm ở local, sau 10 epochs thì đồng bộ cái best lên wandb, sau đó xoá các file best ở local).\n",
        "Chỉ đồng bộ lên WandB mỗi 10 epochs.)\n",
        "- Với last model: lưu lên wandb sau mỗi 10 epochs. (lưu local trước -> đồng bộ lên wandb sẽ xoá file local)\n",
        "+, Trong quá trình lưu thì việc training vẫn diễn ra Parallel\n",
        "\n",
        "đều lưu đầy đủ toàn bộ tham số để có thể train thêm từ cả ở best model và last model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import wandb\n",
        "import os\n",
        "import shutil\n",
        "import time\n",
        "\n",
        "class TrainerCustom(Trainer):\n",
        "    def __init__(self, *args, save_every_n_epochs=10, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        if torch.cuda.is_available():\n",
        "            print(f\"Trainer is running on GPU: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
        "        else:\n",
        "            print(\"Trainer is running on CPU.\")\n",
        "\n",
        "        self.best_eval_loss = float(\"inf\")  # Giá trị loss tốt nhất ban đầu\n",
        "        self.save_every_n_epochs = save_every_n_epochs  # Tần suất lưu lên WandB\n",
        "        self.best_model_info = {\"epoch\": None, \"loss\": None}\n",
        "        self.last_saved_epoch = 0  # Epoch cuối cùng đã lưu Best Model và Last Model\n",
        "        self.executor = ThreadPoolExecutor(max_workers=3)  # Cho phép tối đa 2 luồng song song\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        \"\"\"\n",
        "        How the loss is computed by Trainer. By default, all models return the loss in the first element.\n",
        "\n",
        "        Subclass and override for custom behavior.\n",
        "        \"\"\"\n",
        "\n",
        "        # # Kiểm tra thiết bị của mô hình và dữ liệu\n",
        "        # print(\"Model device:\", next(model.parameters()).device)\n",
        "        # print(\"Input device:\", inputs[\"input_ids\"].device)\n",
        "        if \"labels\" in inputs:\n",
        "            labels = inputs.pop(\"labels\")\n",
        "        else:\n",
        "            labels = None\n",
        "\n",
        "        # Sử dụng nn.CrossEntropyLoss() thay vì nn.CrossEntropy\n",
        "        cross_entropy_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "        # Chạy mô hình và nhận đầu ra (logits)\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "        # Đảm bảo lấy logits từ outputs (mô hình trả về tuple, lấy phần tử đầu tiên là logits)\n",
        "        logits = outputs\n",
        "\n",
        "        if labels is None:\n",
        "            print(\"Labels are None during compute_loss.\")\n",
        "        if logits is None:\n",
        "            print(\"Logits are None during compute_loss.\")\n",
        "\n",
        "        # Tính toán loss\n",
        "        loss = cross_entropy_loss(logits, labels)\n",
        "\n",
        "        # Trả về loss và outputs nếu cần\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "    def async_save_model(self, model_dir, artifact_name, metadata=None):\n",
        "        \"\"\"\n",
        "        Lưu mô hình vào local và đồng bộ lên WandB trong luồng song song.\n",
        "        \"\"\"\n",
        "        def save():\n",
        "            start_time = time.time()\n",
        "            try:\n",
        "                # Xóa tất cả các thư mục tmp_best_model_ trước đó\n",
        "                for folder in os.listdir(\".\"):\n",
        "                    if folder.startswith(\"tmp_best_model_epoch_\") and folder != model_dir:\n",
        "                        shutil.rmtree(folder, ignore_errors=True)\n",
        "                        print(f\"Removed old temporary directory: {folder}\")\n",
        "\n",
        "                # Lưu mô hình vào thư mục tạm\n",
        "                self.save_model(model_dir)\n",
        "\n",
        "                # Đồng bộ lên WandB\n",
        "                artifact = wandb.Artifact(artifact_name, type=\"model\")\n",
        "                artifact.add_dir(model_dir)\n",
        "                if metadata:\n",
        "                    artifact.metadata = metadata\n",
        "                wandb.log_artifact(artifact)\n",
        "            except Exception as e:\n",
        "                print(f\"Error during saving or syncing model {artifact_name}: {e}\")\n",
        "            finally:\n",
        "                # Xóa thư mục tạm hiện tại sau khi đồng bộ\n",
        "                try:\n",
        "                    shutil.rmtree(model_dir, ignore_errors=True)\n",
        "                    print(f\"Successfully removed temporary directory: {model_dir}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error removing temporary directory {model_dir}: {e}\")\n",
        "\n",
        "            elapsed_time = time.time() - start_time\n",
        "            print(f\"Model saved and uploaded to WandB: {artifact_name} in {elapsed_time:.2f} seconds\")\n",
        "\n",
        "        self.executor.submit(save)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def evaluate(self, eval_dataset=None, ignore_keys=None, metric_key_prefix: str = \"eval\"):\n",
        "        metrics = super().evaluate(eval_dataset, ignore_keys, metric_key_prefix)\n",
        "        eval_loss = metrics.get(\"eval_loss\")\n",
        "\n",
        "        # Cập nhật Best Model nếu eval_loss giảm\n",
        "        # Lưu Best Model ngay khi eval_loss giảm (local).\n",
        "        # Đồng bộ wandb ngay\n",
        "\n",
        "        if eval_loss is not None and eval_loss < self.best_eval_loss:\n",
        "            print(f\"New best eval_loss: {eval_loss}\")\n",
        "            self.best_eval_loss = eval_loss\n",
        "            self.best_model_info = {\"epoch\": self.state.epoch, \"loss\": eval_loss}\n",
        "\n",
        "            # Log thông tin Best Model lên WandB\n",
        "            wandb.log({\n",
        "                \"best_eval_loss\": self.best_eval_loss,\n",
        "                \"best_model_epoch\": self.best_model_info.get(\"epoch\", -1)\n",
        "            })\n",
        "\n",
        "            # Lưu Best Model vào thư mục tạm (local)\n",
        "            best_model_dir = f\"./tmp_best_model_epoch_{int(self.state.epoch)}\"\n",
        "            self.save_model(best_model_dir)\n",
        "\n",
        "            # Đồng bộ lên WandB ngay\n",
        "            artifact_name = f\"best_model_epoch_{int(self.state.epoch)}\"\n",
        "            self.async_save_model(best_model_dir, artifact_name, self.best_model_info)\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    # def save_last_model(self):\n",
        "    #     \"\"\"\n",
        "    #     Lưu Last Model lên WandB sau mỗi N epochs.\n",
        "    #     \"\"\"\n",
        "    #     if int(self.state.epoch) % self.save_every_n_epochs == 0 and int(self.state.epoch) != self.last_saved_epoch:\n",
        "    #         print(f\"Saving Last Model at epoch {self.state.epoch} to WandB...\")\n",
        "    #         last_model_dir = f\"./tmp_last_model_epoch_{int(self.state.epoch)}\"\n",
        "    #         artifact_name = f\"last_model_epoch_{int(self.state.epoch)}\"\n",
        "    #         self.async_save_model(last_model_dir, artifact_name)\n",
        "\n",
        "    #         # Log thông tin Last Model lên WandB\n",
        "    #         wandb.log({\n",
        "    #             \"last_model_epoch\": self.state.epoch\n",
        "    #         })\n",
        "\n",
        "    #         # Cập nhật epoch cuối cùng đã lưu\n",
        "    #         self.last_saved_epoch = int(self.state.epoch)\n",
        "\n",
        "    def save_last_model(self):\n",
        "        \"\"\"\n",
        "        Lưu Last Model (bao gồm trạng thái optimizer, scheduler) lên WandB sau mỗi N epochs.\n",
        "        \"\"\"\n",
        "        if int(self.state.epoch) % self.save_every_n_epochs == 0 and int(self.state.epoch) != self.last_saved_epoch:\n",
        "            print(f\"Saving Last Model at epoch {self.state.epoch} to WandB...\")\n",
        "\n",
        "            # Thư mục tạm lưu checkpoint\n",
        "            last_model_dir = f\"./tmp_last_model_epoch_{int(self.state.epoch)}\"\n",
        "            os.makedirs(last_model_dir, exist_ok=True)\n",
        "\n",
        "            # Lưu đầy đủ trạng thái mô hình (checkpoint)\n",
        "            self.save_model(last_model_dir)\n",
        "\n",
        "            # Đường dẫn tệp trainer_state.json\n",
        "            trainer_state_path = os.path.join(last_model_dir, \"trainer_state.json\")\n",
        "            self.state.save_to_json(trainer_state_path)  # Lưu trạng thái trainer\n",
        "\n",
        "            # Đồng bộ checkpoint lên WandB\n",
        "            artifact_name = f\"last_model_epoch_{int(self.state.epoch)}\"\n",
        "            metadata = {\n",
        "                \"epoch\": int(self.state.epoch),\n",
        "                \"last_eval_loss\": self.state.best_metric if hasattr(self.state, \"best_metric\") else \"N/A\",\n",
        "            }\n",
        "            self.async_save_model(last_model_dir, artifact_name, metadata)\n",
        "\n",
        "            # Cập nhật epoch cuối cùng đã lưu\n",
        "            self.last_saved_epoch = int(self.state.epoch)\n",
        "\n",
        "\n",
        "    def train(self, *args, **kwargs):\n",
        "        result = super().train(*args, **kwargs)\n",
        "\n",
        "        # Sau mỗi epoch, lưu Last Model lên WandB\n",
        "        self.save_last_model()\n",
        "        # Chờ tất cả các luồng lưu hoàn thành trước khi kết thúc\n",
        "        self.executor.shutdown(wait=True)\n",
        "\n",
        "        return result\n",
        "\n",
        "\n",
        "# Bước 6: Cài đặt tham số huấn luyện\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./result__s\",          # Thư mục lưu kết quả\n",
        "    eval_strategy=\"epoch\",    # Đánh giá sau mỗi epoch\n",
        "    learning_rate=2e-4,\n",
        "    per_device_train_batch_size=128,\n",
        "    per_device_eval_batch_size=128,\n",
        "    num_train_epochs=10,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=1,  # Ghi logs mỗi 500 bước huấn luyện\n",
        "    save_strategy=\"no\",          # Lưu trọng số sau mỗi epoch\n",
        "    save_total_limit=3,\n",
        "    label_names = [\"labels\"],\n",
        "    report_to=\"wandb\",\n",
        "    run_name=\"bert_run_3\"\n",
        ")\n",
        "\n",
        "\n",
        "import wandb\n",
        "\n",
        "# Khởi tạo wandb\n",
        "wandb.init(\n",
        "    project=\"bertIntentClassification\",  # Tên dự án\n",
        "    name=\"bert_10000Data_1epoch\",                     # Tên phiên chạy\n",
        "    config={\"gpu\": torch.cuda.get_device_name(torch.cuda.current_device()) if torch.cuda.is_available() else \"CPU\"}\n",
        ")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"Trainer is running on GPU: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
        "else:\n",
        "    print(\"Trainer is running on CPU.\")\n",
        "\n",
        "trainer = TrainerCustom(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=valid_dataset,\n",
        "    data_collator=collate_fn,\n",
        "    save_every_n_epochs=3  # Lưu Best Model và Last Model mỗi 10 epochs\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "\n",
        "wandb.finish()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vKSFKgok4Ve_",
        "outputId": "f057f3b2-3b55-40eb-a55a-af535556f189"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainer is running on GPU: Tesla T4\n",
            "Trainer is running on GPU: Tesla T4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='790' max='790' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [790/790 13:03, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.827600</td>\n",
              "      <td>0.795858</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.708100</td>\n",
              "      <td>0.702948</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.673800</td>\n",
              "      <td>0.597507</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.768500</td>\n",
              "      <td>0.566722</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.681100</td>\n",
              "      <td>0.536791</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.609000</td>\n",
              "      <td>0.529781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.369300</td>\n",
              "      <td>0.513642</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.612300</td>\n",
              "      <td>0.500463</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.452800</td>\n",
              "      <td>0.482030</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.588400</td>\n",
              "      <td>0.475604</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New best eval_loss: 0.7958577275276184\n",
            "Removed old temporary directory: tmp_best_model_epoch_1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./tmp_best_model_epoch_1)... Done. 3.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully removed temporary directory: ./tmp_best_model_epoch_1\n",
            "Model saved and uploaded to WandB: best_model_epoch_1 in 10.23 seconds\n",
            "New best eval_loss: 0.7029483318328857\n",
            "Removed old temporary directory: tmp_best_model_epoch_2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./tmp_best_model_epoch_2)... Done. 6.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully removed temporary directory: ./tmp_best_model_epoch_2\n",
            "Model saved and uploaded to WandB: best_model_epoch_2 in 20.34 seconds\n",
            "New best eval_loss: 0.5975072979927063\n",
            "Removed old temporary directory: tmp_best_model_epoch_3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./tmp_best_model_epoch_3)... Done. 3.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully removed temporary directory: ./tmp_best_model_epoch_3\n",
            "Model saved and uploaded to WandB: best_model_epoch_3 in 12.90 seconds\n",
            "New best eval_loss: 0.5667217373847961\n",
            "Removed old temporary directory: tmp_best_model_epoch_4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./tmp_best_model_epoch_4)... Done. 12.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully removed temporary directory: ./tmp_best_model_epoch_4\n",
            "Model saved and uploaded to WandB: best_model_epoch_4 in 18.80 seconds\n",
            "New best eval_loss: 0.5367911458015442\n",
            "Removed old temporary directory: tmp_best_model_epoch_5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./tmp_best_model_epoch_5)... Done. 2.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully removed temporary directory: ./tmp_best_model_epoch_5\n",
            "Model saved and uploaded to WandB: best_model_epoch_5 in 12.39 seconds\n",
            "New best eval_loss: 0.5297806859016418\n",
            "Removed old temporary directory: tmp_best_model_epoch_6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./tmp_best_model_epoch_6)... Done. 7.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully removed temporary directory: ./tmp_best_model_epoch_6\n",
            "Model saved and uploaded to WandB: best_model_epoch_6 in 17.45 seconds\n",
            "New best eval_loss: 0.5136417150497437\n",
            "Removed old temporary directory: tmp_best_model_epoch_7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./tmp_best_model_epoch_7)... Done. 3.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully removed temporary directory: ./tmp_best_model_epoch_7\n",
            "Model saved and uploaded to WandB: best_model_epoch_7 in 12.95 seconds\n",
            "New best eval_loss: 0.5004633665084839\n",
            "Removed old temporary directory: tmp_best_model_epoch_8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./tmp_best_model_epoch_8)... Done. 7.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully removed temporary directory: ./tmp_best_model_epoch_8\n",
            "Model saved and uploaded to WandB: best_model_epoch_8 in 17.33 seconds\n",
            "New best eval_loss: 0.48203015327453613\n",
            "Removed old temporary directory: tmp_best_model_epoch_9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./tmp_best_model_epoch_9)... Done. 6.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully removed temporary directory: ./tmp_best_model_epoch_9\n",
            "Model saved and uploaded to WandB: best_model_epoch_9 in 19.04 seconds\n",
            "New best eval_loss: 0.47560402750968933\n",
            "Removed old temporary directory: tmp_best_model_epoch_10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./tmp_best_model_epoch_10)... Done. 2.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully removed temporary directory: ./tmp_best_model_epoch_10\n",
            "Model saved and uploaded to WandB: best_model_epoch_10 in 8.46 seconds\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_eval_loss</td><td>█▆▄▃▂▂▂▂▁▁</td></tr><tr><td>best_model_epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>eval/loss</td><td>█▆▄▃▂▂▂▂▁▁</td></tr><tr><td>eval/runtime</td><td>▁█▄▄▄▄▄▅▄▅</td></tr><tr><td>eval/samples_per_second</td><td>█▁▅▅▄▅▄▄▅▄</td></tr><tr><td>eval/steps_per_second</td><td>█▁▅▅▄▅▄▄▄▄</td></tr><tr><td>test/loss</td><td>▁▁▁</td></tr><tr><td>test/runtime</td><td>█▁█</td></tr><tr><td>test/samples_per_second</td><td>▁█▁</td></tr><tr><td>test/steps_per_second</td><td>▁█▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇█████</td></tr><tr><td>train/grad_norm</td><td>▁▁▁▄▄▅▂▄▆▅▄▄▂▃▃▆▅▂▁▄▃▃▄▃▂▄▄█▃▂▃▂▃▃▂▂▃▅▃▂</td></tr><tr><td>train/learning_rate</td><td>█████▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▄▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▁▁▁▁</td></tr><tr><td>train/loss</td><td>█▆▄▄▃▃▃▃▃▃▃▂▂▂▃▂▂▂▂▂▂▃▃▂▂▂▂▂▂▂▂▂▂▂▃▂▂▂▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_eval_loss</td><td>0.4756</td></tr><tr><td>best_model_epoch</td><td>10</td></tr><tr><td>eval/loss</td><td>0.4756</td></tr><tr><td>eval/runtime</td><td>13.8414</td></tr><tr><td>eval/samples_per_second</td><td>181.412</td></tr><tr><td>eval/steps_per_second</td><td>1.445</td></tr><tr><td>test/loss</td><td>0.63098</td></tr><tr><td>test/runtime</td><td>10.6348</td></tr><tr><td>test/samples_per_second</td><td>295.163</td></tr><tr><td>test/steps_per_second</td><td>2.351</td></tr><tr><td>total_flos</td><td>0</td></tr><tr><td>train/epoch</td><td>10</td></tr><tr><td>train/global_step</td><td>790</td></tr><tr><td>train/grad_norm</td><td>5.48938</td></tr><tr><td>train/learning_rate</td><td>0</td></tr><tr><td>train/loss</td><td>0.5884</td></tr><tr><td>train_loss</td><td>0.68839</td></tr><tr><td>train_runtime</td><td>784.0412</td></tr><tr><td>train_samples_per_second</td><td>128.106</td></tr><tr><td>train_steps_per_second</td><td>1.008</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">bert_10000Data_1epoch</strong> at: <a href='https://wandb.ai/doanngoccuong_nh/bertIntentClassification/runs/zztnx9uv' target=\"_blank\">https://wandb.ai/doanngoccuong_nh/bertIntentClassification/runs/zztnx9uv</a><br> View project at: <a href='https://wandb.ai/doanngoccuong_nh/bertIntentClassification' target=\"_blank\">https://wandb.ai/doanngoccuong_nh/bertIntentClassification</a><br>Synced 5 W&B file(s), 0 media file(s), 22 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250115_075347-zztnx9uv/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test"
      ],
      "metadata": {
        "id": "yuP4R0do3Qpm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test"
      ],
      "metadata": {
        "id": "aWTycJpa398K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Khắc phục lỗi không đồng nhất số lượng nhãn và dự đoán\n",
        "# test_results = trainer.predict(test_dataset)\n",
        "\n",
        "# # Cắt nhãn và dự đoán theo độ dài nhỏ nhất\n",
        "# predictions = test_results.predictions.argmax(axis=-1)\n",
        "# labels = test_results.label_ids\n",
        "\n",
        "# min_length = min(len(labels), len(predictions))\n",
        "# labels = labels[:min_length]\n",
        "# predictions = predictions[:min_length]\n",
        "\n",
        "# # Báo cáo hiệu suất\n",
        "# from sklearn.metrics import classification_report\n",
        "# report = classification_report(\n",
        "#     labels,\n",
        "#     predictions,\n",
        "#     target_names=[\"intent_fallback\", \"intent_learn_more\", \"intent_negative\", \"intent_neutral\", \"intent_positive\", \"silence\"]\n",
        "# )\n",
        "# print(\"\\nClassification Report:\\n\", report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "id": "sA6GGPCJ3_NI",
        "outputId": "6ec03add-d2c7-4b45-e1b8-0a8fc32ed328"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Report:\n",
            "                    precision    recall  f1-score   support\n",
            "\n",
            "  intent_fallback       0.15      0.11      0.12       481\n",
            "intent_learn_more       0.14      0.14      0.14       509\n",
            "  intent_negative       0.17      0.20      0.18       544\n",
            "   intent_neutral       0.15      0.17      0.16       536\n",
            "  intent_positive       0.16      0.17      0.17       551\n",
            "          silence       0.16      0.15      0.15       493\n",
            "\n",
            "         accuracy                           0.16      3114\n",
            "        macro avg       0.16      0.16      0.15      3114\n",
            "     weighted avg       0.16      0.16      0.16      3114\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
        "import wandb\n",
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "\n",
        "\n",
        "\n",
        "# 1. Tải mô hình từ artifact trên WandB\n",
        "run = wandb.init(project=\"bert-intent-classification\")  # Tên dự án trong WandB\n",
        "artifact = run.use_artifact('doanngoccuong_nh/bertIntentClassification/best_model_epoch_9:v1', type='model')\n",
        "artifact_dir = artifact.download()\n",
        "print(\"Files in artifact_dir:\", os.listdir(artifact_dir))\n",
        "\n",
        "# Đường dẫn tệp cấu hình\n",
        "config_path = os.path.join(artifact_dir, \"config.json\")\n",
        "\n",
        "# Kiểm tra và cập nhật tệp config.json\n",
        "config = {\n",
        "    \"model_type\": \"bert\",\n",
        "    \"hidden_size\": 768,\n",
        "    \"num_attention_heads\": 12,\n",
        "    \"num_hidden_layers\": 12,\n",
        "    \"vocab_size\": 30522\n",
        "}\n",
        "with open(config_path, \"w\") as f:\n",
        "    json.dump(config, f, indent=4)\n",
        "print(f\"Config.json updated at {config_path}\")\n",
        "\n",
        "\n",
        "# 2. Create config.json if not available\n",
        "config = AutoConfig.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Tải tokenizer từ mô hình gốc\n",
        "original_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Lưu các tệp cần thiết vào artifact_dir\n",
        "original_tokenizer.save_pretrained(artifact_dir)\n",
        "\n",
        "print(f\"Tokenizer files saved to {artifact_dir}\")\n",
        "\n",
        "# 4. Tải mô hình đã lưu và tokenizer\n",
        "model_path = artifact_dir  # Đường dẫn đến mô hình đã tải\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "# model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "\n",
        "\n",
        "\n",
        "# Khởi tạo mô hình trống\n",
        "from safetensors.torch import load_file\n",
        "model = BERTIntentClassification(model_name=\"bert-base-uncased\", num_classes=6)\n",
        "weights_path = os.path.join(artifact_dir, \"model.safetensors\") # Đường dẫn đến tệp `model.safetensors`\n",
        "state_dict = load_file(weights_path) # Tải trọng số vào mô hình\n",
        "model.load_state_dict(state_dict)\n",
        "model.eval()\n",
        "\n",
        "\n",
        "# Chuyển mô hình sang chế độ đánh giá\n",
        "model.eval()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "print(f\"Model loaded and running on device: {device}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Bước 2: Chuẩn bị tokenizer và token hóa dữ liệu\n",
        "max_seq_length = 512\n",
        "\n",
        "# def preprocess_input(question, answer, tokenizer, max_seq_length):\n",
        "#     \"\"\"\n",
        "#     Tiền xử lý dữ liệu đầu vào bằng cách ghép nối câu hỏi và câu trả lời với các token đặc biệt.\n",
        "#     \"\"\"\n",
        "#     input_text = f\"[CLS] {question.strip()} [SEP] {answer.strip()} [SEP]\"\n",
        "#     inputs = tokenizer(\n",
        "#         input_text,\n",
        "#         return_tensors=\"pt\",\n",
        "#         truncation=True,\n",
        "#         padding=True,\n",
        "#         max_length=max_seq_length\n",
        "#     )\n",
        "#     return inputs\n",
        "\n",
        "def preprocess_input(question, answer, tokenizer, max_seq_length):\n",
        "    \"\"\"\n",
        "    Tiền xử lý dữ liệu đầu vào bằng cách kết hợp câu hỏi và câu trả lời theo định dạng chuẩn.\n",
        "    \"\"\"\n",
        "    # Kết hợp câu hỏi và câu trả lời theo định dạng chuẩn\n",
        "    input_text = f\"question: {question.strip().lower()}. answer: {answer.strip().lower()}.\"\n",
        "\n",
        "    # Tokenize dữ liệu đầu vào\n",
        "    # inputs = tokenizer(\n",
        "    #     input_text,\n",
        "    #     return_tensors=\"pt\",  # Trả về tensor PyTorch\n",
        "    #     truncation=True,      # Cắt chuỗi nếu vượt quá độ dài tối đa\n",
        "    #     padding=\"max_length\", # Thêm padding để đạt độ dài tối đa\n",
        "    #     max_length=max_seq_length  # Độ dài tối đa\n",
        "    # )\n",
        "\n",
        "    # return inputs\n",
        "\n",
        "    # Tokenize dữ liệu đầu vào\n",
        "    token_inputs = tokenizer(\n",
        "        inputs,                           # Chuỗi đầu vào (sau khi ghép nối từ `robot` và `user_answer`).\n",
        "        add_special_tokens=True,          # Tự động thêm các token đặc biệt như [CLS], [SEP] vào chuỗi đầu vào.\n",
        "        truncation=True,                  # Cắt bớt chuỗi nếu độ dài vượt quá `max_length`.\n",
        "        padding=True,                     # Thêm padding (với token [PAD]) để đảm bảo tất cả các chuỗi có độ dài bằng nhau.\n",
        "        max_length=max_seq_length,        # Độ dài tối đa của chuỗi sau khi token hóa (ví dụ: 512).\n",
        "        return_overflowing_tokens=False,  # Không trả về các đoạn token vượt quá `max_length`.\n",
        "        return_length=False,              # Không trả về độ dài của từng chuỗi token đã tạo ra.\n",
        "        return_tensors=\"pt\",              # Trả về đầu ra dưới dạng tensor của PyTorch (định dạng này cần thiết để sử dụng với mô hình).\n",
        "    )\n",
        "\n",
        "    return token_inputs\n",
        "\n",
        "# 6. Khởi tạo biến lưu kết quả\n",
        "results = []\n",
        "correct_predictions = 0\n",
        "\n",
        "def map_label(pred_class, label_mapping):\n",
        "    return label_mapping.get(pred_class, f\"Unknown (Class ID: {pred_class})\")\n",
        "\n",
        "# Cập nhật label_mapping từ thông tin huấn luyện\n",
        "label_mapping = {\n",
        "    0: \"intent_fallback\",\n",
        "    1: \"intent_learn_more\",\n",
        "    2: \"intent_negative\",\n",
        "    3: \"intent_neutral\",\n",
        "    4: \"intent_positive\",\n",
        "    5: \"silence\"\n",
        "}\n",
        "\n",
        "# 7. Thực hiện inference trên từng dòng dữ liệu\n",
        "import os\n",
        "import pandas as pd\n",
        "import shutil\n",
        "import torch\n",
        "\n",
        "def process_and_update_file(input_file, output_file, model, tokenizer, label_mapping, max_seq_length, device, num_rows=None):\n",
        "    \"\"\"\n",
        "    Processes an input Excel file, performs inference, and updates the file with predicted results.\n",
        "\n",
        "    Args:\n",
        "        input_file (str): Path to the input Excel file.\n",
        "        output_file (str): Path to the output Excel file.\n",
        "        model: The trained model for inference.\n",
        "        tokenizer: Tokenizer for preprocessing.\n",
        "        label_mapping (dict): Mapping from class index to label.\n",
        "        max_seq_length (int): Maximum sequence length for the tokenizer.\n",
        "        device: PyTorch device (e.g., 'cpu' or 'cuda').\n",
        "        num_rows (int, optional): Number of rows to process. Default is None (process all rows).\n",
        "    \"\"\"\n",
        "    # Sao chép file gốc nếu file output chưa tồn tại\n",
        "    if not os.path.exists(output_file):\n",
        "        shutil.copy(input_file, output_file)\n",
        "        print(f\"File copied from {input_file} to {output_file}\")\n",
        "\n",
        "    # Đọc dữ liệu từ file output\n",
        "    data = pd.read_excel(output_file)\n",
        "\n",
        "    # Giới hạn số dòng nếu cần\n",
        "    if num_rows is not None:\n",
        "        data = data.head(num_rows)\n",
        "        print(f\"Processing only the first {num_rows} rows.\")\n",
        "\n",
        "    # Xử lý inference và thêm cột mới\n",
        "    results = []\n",
        "    correct_predictions = 0\n",
        "\n",
        "\n",
        "\n",
        "    for idx, row in data.iterrows():\n",
        "        question = row[\"robot\"]\n",
        "        answer = row[\"user_answer\"] if not pd.isna(row[\"user_answer\"]) else \"\"\n",
        "        true_intent = row[\"user_intent\"]\n",
        "\n",
        "        # Tiền xử lý đầu vào\n",
        "        inputs = preprocess_input(question, answer, tokenizer, max_seq_length)\n",
        "        print(\"input processed:\", inputs)\n",
        "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "        # Thực hiện dự đoán\n",
        "        start_time = time.time()\n",
        "        with torch.no_grad():\n",
        "            logits = model(**inputs)  # Custom model directly returns logits\n",
        "            predicted_class = torch.argmax(logits, dim=1).item()\n",
        "            predicted_label = map_label(predicted_class, label_mapping)\n",
        "        end_time = time.time()\n",
        "\n",
        "        # Calculate response time\n",
        "        response_time = end_time - start_time\n",
        "\n",
        "        # Kiểm tra đúng sai\n",
        "        is_correct = (predicted_label == true_intent)\n",
        "        if is_correct:\n",
        "            correct_predictions += 1\n",
        "\n",
        "        # Lưu kết quả\n",
        "        results.append({\n",
        "            \"predicted_intent\": predicted_label,\n",
        "            \"is_correct\": is_correct,\n",
        "            \"model_response_time\": response_time\n",
        "        })\n",
        "\n",
        "        print(f\"Question: {question}\")\n",
        "        print(f\"Answer: {answer}\")\n",
        "        print(f\"Inputs: {inputs}\")\n",
        "        print(f\"Logits: {logits}\")\n",
        "        print(f\"Predicted class: {predicted_class}\")\n",
        "        print(f\"Predicted label: {predicted_label}\")\n",
        "\n",
        "    # Tạo DataFrame từ kết quả\n",
        "    results_df = pd.DataFrame(results)\n",
        "\n",
        "    # Thêm cột vào DataFrame ban đầu\n",
        "    data[\"predicted_intent\"] = results_df[\"predicted_intent\"]\n",
        "    data[\"is_correct\"] = results_df[\"is_correct\"]\n",
        "    data[\"model_response_time\"] = results_df[\"model_response_time\"]\n",
        "\n",
        "    # Ghi kết quả trở lại file Excel\n",
        "    with pd.ExcelWriter(output_file, engine=\"openpyxl\", mode=\"w\") as writer:\n",
        "        data.to_excel(writer, index=False)\n",
        "\n",
        "    # Tính accuracy\n",
        "    accuracy = correct_predictions / len(data)\n",
        "    print(f\"Accuracy: {accuracy:.2%}\")\n",
        "    print(f\"Evaluation results saved to {output_file}\")\n",
        "\n",
        "# Định nghĩa các tham số cần thiết\n",
        "input_file = \"/content/test1_processed_TEST_500to1000Phrase.xlsx\"\n",
        "output_file = \"evaluation_results.xlsx\"\n",
        "num_rows = 5000  # Số lượng dòng muốn đánh giá\n",
        "# model = ...  # Model đã huấn luyện\n",
        "# tokenizer = ...  # Tokenizer tương ứng\n",
        "# label_mapping = {0: \"intent_A\", 1: \"intent_B\", 2: \"intent_C\"}  # Mapping nhãn\n",
        "# max_seq_length = 128  # Độ dài tối đa\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Gọi hàm để xử lý và cập nhật file\n",
        "# Định nghĩa các tham số cần thiết\n",
        "\n",
        "\n",
        "# Gọi hàm để xử lý và giới hạn số dòng\n",
        "process_and_update_file(input_file, output_file, model, tokenizer, label_mapping, max_seq_length, device, num_rows=num_rows)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ubs29075ISNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 3500 Test (Ko Shuffle): 75.95%\n",
        "\n",
        "\n",
        "2. Đánh giá trên bộ 600 câu được tạo bởi 50 cụm mới hoàn toàn:\n",
        "- Với BERT cũ: Để bình thường và CLS, SEP bị double => Accuracy: 68.16% (10 epochs).\n",
        "Hôm qua (best epoch 90/100 epochs) đánh giá trên data mới toanh này chỉ đạt 46.67%\n",
        "\n",
        "- Khi Chuyển sang: lowercase và question: {question}. answer: {answer} độ chính xác lên : 69.65% (10 epochs).\n"
      ],
      "metadata": {
        "id": "NybtfYCr_LYt"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ab53a6542f784faeb5940f8a593e34c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_02e82d56873c4b908f20d8c17279f4bf",
              "IPY_MODEL_d31163b2d1004ff0bd67f262d2231783",
              "IPY_MODEL_915eea616078437e850131c73c9e48fc"
            ],
            "layout": "IPY_MODEL_6f9df60a6b404375b9e8042d1957dfa3"
          }
        },
        "02e82d56873c4b908f20d8c17279f4bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_89d77fee09354e818ddeb591f00812fc",
            "placeholder": "​",
            "style": "IPY_MODEL_884fcf6db2b642648fd6532edf0f375c",
            "value": "Map: 100%"
          }
        },
        "d31163b2d1004ff0bd67f262d2231783": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_59f098f6f3b14d6ab6b7787f34fd177e",
            "max": 15694,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_212d65d42a814563b66513fb39abd94e",
            "value": 15694
          }
        },
        "915eea616078437e850131c73c9e48fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a276388a7f00445ca3b24c40bb5d1981",
            "placeholder": "​",
            "style": "IPY_MODEL_708722b714de4bc9beff893c7c7ad30c",
            "value": " 15694/15694 [00:00&lt;00:00, 21381.93 examples/s]"
          }
        },
        "6f9df60a6b404375b9e8042d1957dfa3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "89d77fee09354e818ddeb591f00812fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "884fcf6db2b642648fd6532edf0f375c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "59f098f6f3b14d6ab6b7787f34fd177e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "212d65d42a814563b66513fb39abd94e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a276388a7f00445ca3b24c40bb5d1981": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "708722b714de4bc9beff893c7c7ad30c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}