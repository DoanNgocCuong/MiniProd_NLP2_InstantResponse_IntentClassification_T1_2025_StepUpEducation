{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python \n",
    "from transformers import AutoTokenizer, TrainingArguments, Trainer, AutoModel\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\" ## Setup CUDA GPU 1\n",
    "\n",
    "\n",
    "class BERTIntentClassification(nn.Module):\n",
    "    \n",
    "    \n",
    "    def __init__(self, model_name=\"bert-base-uncased\", num_classes=10, dropout_rate=0.1, cache_dir = \"huggingface\"):\n",
    "        super(BERTIntentClassification, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name, cache_dir = cache_dir)\n",
    "        # Get BERT hidden size\n",
    "        hidden_size = self.bert.config.hidden_size\n",
    "        self.ffnn = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "\n",
    "    \n",
    "    def freeze_bert(self):\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "\n",
    "    def get_pooling(self, hidden_state, attention_mask):\n",
    "        \"\"\"\n",
    "        Get mean pooled representation from BERT hidden states\n",
    "        Args:\n",
    "            hidden_state: BERT output containing hidden states\n",
    "        Returns:\n",
    "            pooled_output: Mean pooled representation of the sequence\n",
    "        \"\"\"\n",
    "        # Get last hidden state\n",
    "        last_hidden_state = hidden_state.last_hidden_state  # Shape: [batch_size, seq_len, hidden_size]\n",
    "        \n",
    "        if attention_mask is not None:\n",
    "            # Expand attention mask to match hidden state dimensions\n",
    "            attention_mask = attention_mask.unsqueeze(-1)  # [batch_size, seq_len, 1]\n",
    "            \n",
    "            # Mask out padding tokens\n",
    "            masked_hidden = last_hidden_state * attention_mask\n",
    "            \n",
    "            # Calculate mean (sum / number of actual tokens)\n",
    "            sum_hidden = torch.sum(masked_hidden, dim=1)  # [batch_size, hidden_size]\n",
    "            count_tokens = torch.sum(attention_mask, dim=1)  # [batch_size, 1]\n",
    "            pooled_output = sum_hidden / count_tokens\n",
    "        else:\n",
    "            # If no attention mask, simply take mean of all tokens\n",
    "            pooled_output = torch.mean(last_hidden_state, dim=1)\n",
    "        \n",
    "        return pooled_output\n",
    "        \n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, **kwargs):\n",
    "        \"\"\"\n",
    "        Forward pass of the model\n",
    "        Args:\n",
    "            input_ids: Input token IDs\n",
    "            attention_mask: Attention mask for padding\n",
    "        Returns:\n",
    "            logits: Raw logits for each class\n",
    "        \"\"\"\n",
    "        # Get BERT hidden states\n",
    "        hidden_state = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "        \n",
    "        # Get pooled representation\n",
    "        hidden_state_pooling = self.get_pooling(hidden_state=hidden_state, attention_mask=attention_mask)\n",
    "        \n",
    "        # Pass through FFNN classifier\n",
    "        logits = self.ffnn(hidden_state_pooling)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "\n",
    "\n",
    "class TrainerCustom(Trainer):\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        \"\"\"\n",
    "        How the loss is computed by Trainer. By default, all models return the loss in the first element.\n",
    "\n",
    "        Subclass and override for custom behavior.\n",
    "        \"\"\"\n",
    "        if \"labels\" in inputs:\n",
    "            labels = inputs.pop(\"labels\")\n",
    "        else:\n",
    "            labels = None\n",
    "        \n",
    "        # Sử dụng nn.CrossEntropyLoss() thay vì nn.CrossEntropy\n",
    "        cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Chạy mô hình và nhận đầu ra (logits)\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        # Đảm bảo lấy logits từ outputs (mô hình trả về tuple, lấy phần tử đầu tiên là logits)\n",
    "        logits = outputs\n",
    "        \n",
    "        # Tính toán loss\n",
    "        loss = cross_entropy_loss(logits, labels)\n",
    "        \n",
    "        # Trả về loss và outputs nếu cần\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "# Bước 1: Tải dữ liệu\n",
    "# Sử dụng dataset sẵn có từ Hugging Face hoặc tải từ file cục bộ\n",
    "dataset = load_dataset(\"imdb\", cache_dir = \"huggingface\")  # Ví dụ: Dữ liệu IMDB để phân loại sentiment\n",
    "# Thay thế trường 'text' thành 'input_ids' trong train_dataset và test_dataset\n",
    "def preprocess_dataset(dataset):\n",
    "    return dataset.map(lambda example: {\n",
    "            \"input_ids\": example['text'],\n",
    "            \"label\": example['label']\n",
    "        }, \n",
    "        remove_columns=[\"text\"],\n",
    "        num_proc=4  # Sử dụng 4 tiến trình song song để xử lý nhanh hơn\n",
    "    )\n",
    "\n",
    "train_dataset = preprocess_dataset(dataset[\"train\"])\n",
    "test_dataset = preprocess_dataset(dataset[\"test\"])\n",
    "\n",
    "\n",
    "# Bước 2: Chuẩn bị tokenizer và token hóa dữ liệu\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir = \"huggingface\")\n",
    "model = BERTIntentClassification(\n",
    "    model_name=model_name,\n",
    "    num_classes=2\n",
    ")\n",
    "model.freeze_bert() # Froze Layer BERT\n",
    "max_seq_length = 512\n",
    "\n",
    "\n",
    "def collate_fn(features):\n",
    "    inputs = []\n",
    "    labels = []\n",
    "    for element in features:\n",
    "        inputs.append(element.get(\"input_ids\"))\n",
    "        labels.append(element.get(\"label\"))\n",
    "    \n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    \n",
    "    token_inputs = tokenizer(\n",
    "        inputs,\n",
    "        add_special_tokens=True,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=max_seq_length,\n",
    "        return_overflowing_tokens=False,\n",
    "        return_length=False,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    token_inputs.update({\n",
    "        \"labels\": labels,\n",
    "    })\n",
    "    return token_inputs\n",
    "\n",
    "# Bước 6: Cài đặt tham số huấn luyện\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",          # Thư mục lưu kết quả\n",
    "    eval_strategy=\"epoch\",    # Đánh giá sau mỗi epoch\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=128,\n",
    "    per_device_eval_batch_size=128,\n",
    "    num_train_epochs=50,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=None,\n",
    "    logging_strategy = \"epoch\",\n",
    "    save_strategy=\"epoch\",          # Lưu trọng số sau mỗi epoch\n",
    "    save_total_limit=3,\n",
    ")\n",
    "\n",
    "# Bước 7: Tạo Trainer\n",
    "trainer = TrainerCustom(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator = collate_fn,\n",
    ")\n",
    "\n",
    "# Bước 8: Huấn luyện\n",
    "trainer.train()\n",
    "\n",
    "# Bước 9: Đánh giá trên tập kiểm tra\n",
    "trainer.evaluate()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
