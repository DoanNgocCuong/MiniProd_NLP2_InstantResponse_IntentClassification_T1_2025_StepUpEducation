{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLWoGrdRXTSg"
      },
      "source": [
        "## 1. Tải Dữ Liệu từ CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbMKcafkKCPk",
        "outputId": "56cd1612-565d-4f0b-e7d8-93d7eda6f11d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.11)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.27.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "zIVYywvyiq2L"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, TrainingArguments, Trainer, AutoModel\n",
        "import numpy as np\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "import torch.nn as nn\n",
        "import os\n",
        "from typing import List\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\" ## Setup CUDA GPU 1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Kiểm tra GPU khả dụng\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"Number of GPUs:\", torch.cuda.device_count())\n",
        "if torch.cuda.is_available():\n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
        "else:\n",
        "    print(\"No GPU found.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SHnftkneP6B",
        "outputId": "d7bcb28c-5ef5-4fcc-8b47-69afe8fc6648"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: True\n",
            "Number of GPUs: 1\n",
            "GPU 0: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "def select_gpu():\n",
        "    \"\"\"\n",
        "    Kiểm tra GPU khả dụng và tự động chọn GPU phù hợp.\n",
        "    \"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        num_gpus = torch.cuda.device_count()\n",
        "        print(f\"Number of GPUs available: {num_gpus}\")\n",
        "\n",
        "        # Duyệt qua các GPU khả dụng để tìm GPU ít sử dụng nhất\n",
        "        available_gpus = [torch.cuda.get_device_name(i) for i in range(num_gpus)]\n",
        "        print(\"Available GPUs:\", available_gpus)\n",
        "\n",
        "        for i in range(num_gpus):\n",
        "            try:\n",
        "                # Đặt GPU\n",
        "                os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(i)\n",
        "                device = torch.device(f\"cuda:{i}\")\n",
        "                torch.cuda.set_device(device)\n",
        "                print(f\"Using GPU: {torch.cuda.get_device_name(device.index)}\")\n",
        "                return device\n",
        "            except Exception as e:\n",
        "                print(f\"GPU {i} is not suitable: {e}\")\n",
        "\n",
        "        print(\"No suitable GPU found. Falling back to CPU.\")\n",
        "        return torch.device(\"cpu\")\n",
        "    else:\n",
        "        print(\"No GPUs available. Using CPU.\")\n",
        "        return torch.device(\"cpu\")\n",
        "\n",
        "# Tự động chọn GPU hoặc CPU\n",
        "device = select_gpu()\n",
        "\n",
        "# Kiểm tra lại thiết bị đang sử dụng\n",
        "print(f\"Final selected device: {device}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ge41QgJadXLv",
        "outputId": "f6353b1e-f476-4319-ea33-2b75d0e3c208"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of GPUs available: 1\n",
            "Available GPUs: ['Tesla T4']\n",
            "Using GPU: Tesla T4\n",
            "Final selected device: cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "IkfPfDY3iskg"
      },
      "outputs": [],
      "source": [
        "\n",
        "class BERTIntentClassification(nn.Module):\n",
        "\n",
        "\n",
        "    def __init__(self, model_name=\"bert-base-uncased\", num_classes=10, dropout_rate=0.1, cache_dir = \"huggingface\"):\n",
        "        super(BERTIntentClassification, self).__init__()\n",
        "        self.bert = AutoModel.from_pretrained(model_name, cache_dir = cache_dir)\n",
        "        # Get BERT hidden size\n",
        "        hidden_size = self.bert.config.hidden_size\n",
        "        self.ffnn = nn.Sequential(\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.LayerNorm(hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(hidden_size, num_classes)\n",
        "        )\n",
        "\n",
        "\n",
        "    def freeze_bert(self):\n",
        "        for param in self.bert.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "\n",
        "    def get_pooling(self, hidden_state, attention_mask):\n",
        "        \"\"\"\n",
        "        Get mean pooled representation from BERT hidden states\n",
        "        Args:\n",
        "            hidden_state: BERT output containing hidden states\n",
        "        Returns:\n",
        "            pooled_output: Mean pooled representation of the sequence\n",
        "        \"\"\"\n",
        "        # Get last hidden state\n",
        "        last_hidden_state = hidden_state.last_hidden_state  # Shape: [batch_size, seq_len, hidden_size]\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            # Expand attention mask to match hidden state dimensions\n",
        "            attention_mask = attention_mask.unsqueeze(-1)  # [batch_size, seq_len, 1]\n",
        "\n",
        "            # Mask out padding tokens\n",
        "            masked_hidden = last_hidden_state * attention_mask\n",
        "\n",
        "            # Calculate mean (sum / number of actual tokens)\n",
        "            sum_hidden = torch.sum(masked_hidden, dim=1)  # [batch_size, hidden_size]\n",
        "            count_tokens = torch.sum(attention_mask, dim=1)  # [batch_size, 1]\n",
        "            pooled_output = sum_hidden / count_tokens\n",
        "        else:\n",
        "            # If no attention mask, simply take mean of all tokens\n",
        "            pooled_output = torch.mean(last_hidden_state, dim=1)\n",
        "\n",
        "        return pooled_output\n",
        "\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, **kwargs):\n",
        "        \"\"\"\n",
        "        Forward pass of the model\n",
        "        Args:\n",
        "            input_ids: Input token IDs\n",
        "            attention_mask: Attention mask for padding\n",
        "        Returns:\n",
        "            logits: Raw logits for each class\n",
        "        \"\"\"\n",
        "        # Get BERT hidden states\n",
        "        hidden_state = self.bert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "        )\n",
        "\n",
        "        # Get pooled representation\n",
        "        hidden_state_pooling = self.get_pooling(hidden_state=hidden_state, attention_mask=attention_mask)\n",
        "\n",
        "        # Pass through FFNN classifier\n",
        "        logits = self.ffnn(hidden_state_pooling)\n",
        "\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "zypDXvoaivAb"
      },
      "outputs": [],
      "source": [
        "class TrainerCustom(Trainer):\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        \"\"\"\n",
        "        How the loss is computed by Trainer. By default, all models return the loss in the first element.\n",
        "\n",
        "        Subclass and override for custom behavior.\n",
        "        \"\"\"\n",
        "        if \"labels\" in inputs:\n",
        "            labels = inputs.pop(\"labels\")\n",
        "        else:\n",
        "            labels = None\n",
        "\n",
        "        # Sử dụng nn.CrossEntropyLoss() thay vì nn.CrossEntropy\n",
        "        cross_entropy_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "        # Chạy mô hình và nhận đầu ra (logits)\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "        # Đảm bảo lấy logits từ outputs (mô hình trả về tuple, lấy phần tử đầu tiên là logits)\n",
        "        logits = outputs\n",
        "\n",
        "        # Tính toán loss\n",
        "        loss = cross_entropy_loss(logits, labels)\n",
        "\n",
        "        # Trả về loss và outputs nếu cần\n",
        "        return (loss, outputs) if return_outputs else loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zIJIYGcppMk"
      },
      "source": [
        "# 1. Load Dataset and with Dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Cách kết hợp question và answer:**\n",
        "\n",
        "1. **Ghép nối trực tiếp:** Bạn có thể kết hợp câu hỏi và câu trả lời thành một chuỗi duy nhất, sử dụng một ký tự đặc biệt hoặc dấu phân cách để tách biệt chúng. Ví dụ:\n",
        "\n",
        "   ```python\n",
        "   combined_text = question + \" [SEP] \" + answer\n",
        "   ```\n",
        "\n",
        "   Trong đó, `[SEP]` là một token đặc biệt thường được sử dụng trong các mô hình như BERT để phân tách các đoạn văn bản khác nhau.\n",
        "\n",
        "2. **Sử dụng token đặc biệt:** Một số mô hình hỗ trợ các token đặc biệt để đánh dấu bắt đầu và kết thúc của câu hỏi và câu trả lời. Ví dụ:\n",
        "\n",
        "   ```python\n",
        "   combined_text = \"[CLS] \" + question + \" [SEP] \" + answer + \" [SEP]\"\n",
        "   ```\n",
        "\n",
        "   - `[CLS]`: Token đánh dấu bắt đầu chuỗi (thường dùng trong BERT).\n",
        "   - `[SEP]`: Token phân tách giữa các phần của chuỗi.\n",
        "\n"
      ],
      "metadata": {
        "id": "CjW1FC-ibueZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ví dụ thực tế về chuỗi chuẩn:\n",
        "\n",
        "Một câu/đoạn duy nhất:\n",
        "```\n",
        "[CLS] This is the first sentence. [SEP]\n",
        "```\n",
        "Hai câu/đoạn (ví dụ: câu hỏi và trả lời):\n",
        "```\n",
        "[CLS] What is your name? [SEP] My name is John. [SEP]\n",
        "```\n",
        "Nhiều câu/đoạn (3 đoạn):\n",
        "```\n",
        "[CLS] Question 1 [SEP] Answer 1 [SEP] Extra information [SEP]\n",
        "```"
      ],
      "metadata": {
        "id": "hyd8l8VPdgpg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "                          input_ids  intent\n",
        "0  [CLS] Cậu có muốn tiếp tục không? [SEP]  silence\n",
        "1                          [CLS] [SEP]  silence\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "mSOI6zlGfQV_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "\n",
        "def combine_with_special_tokens(row, text_columns, cls_token=\"[CLS]\", sep_token=\"[SEP]\"):\n",
        "    \"\"\"\n",
        "    Thêm các token đặc biệt vào chuỗi kết hợp từ các cột văn bản.\n",
        "\n",
        "    Args:\n",
        "        row (pd.Series): Dòng dữ liệu từ DataFrame.\n",
        "        text_columns (list): Danh sách các cột văn bản cần kết hợp.\n",
        "        cls_token (str): Token bắt đầu câu.\n",
        "        sep_token (str): Token phân cách.\n",
        "\n",
        "    Returns:\n",
        "        str: Chuỗi văn bản đã thêm token đặc biệt.\n",
        "    \"\"\"\n",
        "    tokens = [cls_token]  # Thêm [CLS] đầu tiên\n",
        "\n",
        "    # Thêm nội dung từ các cột văn bản\n",
        "    for col in text_columns:\n",
        "        if pd.notna(row[col]) and row[col].strip():  # Kiểm tra không rỗng\n",
        "            tokens.append(row[col].strip())\n",
        "            tokens.append(sep_token)  # Thêm [SEP] sau mỗi đoạn\n",
        "\n",
        "    # Nếu không có nội dung nào được thêm, chỉ giữ lại [CLS] và [SEP]\n",
        "    if len(tokens) == 1:\n",
        "        tokens.append(sep_token)\n",
        "\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "def load_xlsx_dataset(xlsx_path, text_columns, label_column, cls_token=\"[CLS]\", sep_token=\"[SEP]\"):\n",
        "    \"\"\"\n",
        "    Tải dataset từ file Excel (.xlsx) và xử lý dữ liệu.\n",
        "\n",
        "    Args:\n",
        "        xlsx_path (str): Đường dẫn đến file .xlsx.\n",
        "        text_columns (list): Danh sách các cột cần ghép để tạo văn bản đầu vào.\n",
        "        label_column (str): Tên cột chứa nhãn.\n",
        "        cls_token (str): Token bắt đầu câu.\n",
        "        sep_token (str): Token phân cách.\n",
        "\n",
        "    Returns:\n",
        "        Dataset: Tập dữ liệu đã xử lý.\n",
        "    \"\"\"\n",
        "    # Đọc file Excel bằng pandas\n",
        "    df = pd.read_excel(xlsx_path)\n",
        "\n",
        "    # Kiểm tra các cột cần thiết\n",
        "    for col in text_columns + [label_column]:\n",
        "        if col not in df.columns:\n",
        "            raise ValueError(f\"Missing required column: {col}\")\n",
        "\n",
        "    # Ghép các cột text lại thành một chuỗi duy nhất với token đặc biệt\n",
        "    df[\"input_ids\"] = df.apply(lambda row: combine_with_special_tokens(row, text_columns, cls_token, sep_token), axis=1)\n",
        "\n",
        "    # Đổi tên cột nhãn\n",
        "    df = df.rename(columns={label_column: \"label\"})\n",
        "\n",
        "    # Chuyển đổi DataFrame thành Dataset\n",
        "    dataset = Dataset.from_pandas(df[[\"input_ids\", \"label\"]])\n",
        "\n",
        "    return dataset\n",
        "\n",
        "# Sử dụng hàm\n",
        "xlsx_path = \"/content/processed_data_example_v2.xlsx\"  # Đường dẫn file Excel\n",
        "text_columns = [\"question\", \"answer\"]  # Các cột cần ghép\n",
        "label_column = \"intent\"  # Cột chứa nhãn\n",
        "\n",
        "# Tải dataset từ Excel\n",
        "dataset = load_xlsx_dataset(xlsx_path, text_columns, label_column)\n",
        "\n",
        "# Kiểm tra dữ liệu\n",
        "print(dataset)\n",
        "\n",
        "# Lấy 10 mẫu đầu tiên\n",
        "sample_dataset = dataset.select(range(20))\n",
        "print(sample_dataset)\n",
        "\n",
        "# In thử một hàng\n",
        "print(\"First row in dataset:\")\n",
        "print(sample_dataset[0])\n",
        "print(sample_dataset[11])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHXznY2KbtzB",
        "outputId": "a76f7796-a013-42c6-f990-11b03bfbc5e0"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['input_ids', 'label'],\n",
            "    num_rows: 120\n",
            "})\n",
            "Dataset({\n",
            "    features: ['input_ids', 'label'],\n",
            "    num_rows: 20\n",
            "})\n",
            "First row in dataset:\n",
            "{'input_ids': \"[CLS] Cậu có thể kể tên một số hành động bắt đầu bằng từ 'play' không? [SEP] Tớ có thể nói 'play football' và 'play basketball'. [SEP]\", 'label': 'intent_positive'}\n",
            "{'input_ids': '[CLS] Cậu có muốn tiếp tục không? [SEP]', 'label': 'silence'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "1oFci1j5k1UY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54890123-28ed-4915-8b81-d8f166972eb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Invalid Samples =====\n",
            "[]\n"
          ]
        }
      ],
      "source": [
        "def check_invalid_samples(dataset):\n",
        "    invalid_samples = []\n",
        "    for idx, sample in enumerate(dataset):\n",
        "        if not isinstance(sample[\"input_ids\"], str) or sample[\"input_ids\"].strip() == \"\":\n",
        "            invalid_samples.append((idx, sample))\n",
        "    return invalid_samples\n",
        "\n",
        "# Kiểm tra dữ liệu không hợp lệ\n",
        "invalid_samples = check_invalid_samples(dataset)\n",
        "print(\"\\n===== Invalid Samples =====\")\n",
        "print(invalid_samples)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "x0-3V7dLlCZp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240,
          "referenced_widgets": [
            "3c50229b7c484c738b70960585d9a192",
            "040fd1fe0cc2463f951c1f69640b3bc7",
            "45c366bdf5c14a238d20ca955f213ab7",
            "dd8529db4f1d4712a6e09cdff2aa9a04",
            "23ea11595dc146279338e8aaa2c67c9b",
            "3081da01e0f54970aa6a3c4e54b8cb06",
            "40ccfe508ef444d59da1d85da527c681",
            "e0e6fa43efeb4980a685e398965fddd1",
            "b1ef581b00aa4983a7d96e3932e173b9",
            "8dde65697b5e4f07b22780df3b2ed994",
            "cb1314888ee5473799a49e1784559726"
          ]
        },
        "outputId": "6f3059da-bef6-4ab4-e9d1-c4ece47f2e2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ánh xạ nhãn: {'intent_fallback': 0, 'intent_learn_more': 1, 'intent_negative': 2, 'intent_neutral': 3, 'intent_positive': 4, 'silence': 5}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/120 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3c50229b7c484c738b70960585d9a192"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['input_ids', 'label'],\n",
            "    num_rows: 120\n",
            "})\n",
            "Dataset({\n",
            "    features: ['input_ids', 'label'],\n",
            "    num_rows: 10\n",
            "})\n",
            "First row in sample_dataset:\n",
            "{'input_ids': \"[CLS] Cậu có biết thêm từ nào khác không? [SEP] Tớ biết 'play games' nữa. [SEP]\", 'label': 4}\n"
          ]
        }
      ],
      "source": [
        "# Tự động phát hiện nhãn và tạo ánh xạ nhãn\n",
        "def create_label_mapping(dataset_list):\n",
        "    \"\"\"\n",
        "    Tự động phát hiện tất cả các nhãn từ danh sách dataset và ánh xạ chúng thành số nguyên.\n",
        "    \"\"\"\n",
        "    all_labels = set()\n",
        "    for dataset in dataset_list:\n",
        "        all_labels.update(dataset[\"label\"])  # Tập hợp tất cả các nhãn từ dataset\n",
        "\n",
        "    label_to_int = {label: idx for idx, label in enumerate(sorted(all_labels))}\n",
        "    print(f\"Ánh xạ nhãn: {label_to_int}\")\n",
        "    return label_to_int\n",
        "\n",
        "# Hàm chuyển đổi nhãn\n",
        "def preprocess_labels(example, label_to_int):\n",
        "    example[\"label\"] = label_to_int.get(example[\"label\"], -1)  # Gán -1 cho nhãn không hợp lệ\n",
        "    return example\n",
        "\n",
        "# Tạo ánh xạ nhãn\n",
        "label_mapping = create_label_mapping([dataset])\n",
        "\n",
        "# Áp dụng chuyển đổi nhãn\n",
        "dataset = dataset.map(lambda example: preprocess_labels(example, label_mapping))\n",
        "\n",
        "# Kiểm tra kết quả\n",
        "print(dataset)\n",
        "\n",
        "# Truy cập mẫu cụ thể\n",
        "sample_dataset = dataset.select(range(10))  # Lấy 10 mẫu đầu tiên\n",
        "print(sample_dataset)\n",
        "\n",
        "# In thử 1 hàng trong sample_dataset\n",
        "print(\"First row in sample_dataset:\")\n",
        "print(sample_dataset[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "hy_jM2gOk99M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cc7fc39-a476-4fa5-c8d2-2942ab9e1f7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chia dataset: 84 mẫu train, 36 mẫu test\n",
            "Train dataset: Dataset({\n",
            "    features: ['input_ids', 'label'],\n",
            "    num_rows: 84\n",
            "})\n",
            "Test dataset: Dataset({\n",
            "    features: ['input_ids', 'label'],\n",
            "    num_rows: 36\n",
            "})\n",
            "Sample train dataset: Dataset({\n",
            "    features: ['input_ids', 'label'],\n",
            "    num_rows: 8\n",
            "})\n",
            "Sample test dataset: Dataset({\n",
            "    features: ['input_ids', 'label'],\n",
            "    num_rows: 5\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "def split_dataset(dataset, test_size=0.2, seed=42):\n",
        "    \"\"\"\n",
        "    Chia dataset thành tập train và test.\n",
        "\n",
        "    Args:\n",
        "        dataset (Dataset): Tập dữ liệu đầy đủ.\n",
        "        test_size (float): Tỷ lệ dữ liệu test (0.0 - 1.0).\n",
        "        seed (int): Seed để chia dữ liệu ngẫu nhiên.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (train_dataset, test_dataset) - Tập train và test.\n",
        "    \"\"\"\n",
        "    if not (0.0 < test_size < 1.0):\n",
        "        raise ValueError(\"test_size phải nằm trong khoảng (0.0, 1.0)\")\n",
        "    if len(dataset) < 2:\n",
        "        raise ValueError(\"Dataset phải có ít nhất 2 mẫu để chia.\")\n",
        "\n",
        "    train_test_split = dataset.train_test_split(test_size=test_size, seed=seed)\n",
        "    print(f\"Chia dataset: {len(train_test_split['train'])} mẫu train, {len(train_test_split['test'])} mẫu test\")\n",
        "    return train_test_split[\"train\"], train_test_split[\"test\"]\n",
        "\n",
        "# Chia dataset\n",
        "train_dataset, test_dataset = split_dataset(dataset, test_size=0.3)\n",
        "\n",
        "# Kiểm tra dữ liệu\n",
        "print(\"Train dataset:\", train_dataset)\n",
        "print(\"Test dataset:\", test_dataset)\n",
        "\n",
        "# Truy cập mẫu cụ thể\n",
        "sample_train_dataset = train_dataset.select(range(8))  # Lấy 10 mẫu đầu tiên từ train\n",
        "sample_test_dataset = test_dataset.select(range(5))    # Lấy 10 mẫu đầu tiên từ test\n",
        "\n",
        "print(\"Sample train dataset:\", sample_train_dataset)\n",
        "print(\"Sample test dataset:\", sample_test_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCzQIjl_ptbw"
      },
      "source": [
        "# 2. Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the number of unique labels\n",
        "print(label_mapping)\n",
        "number_label = len(label_mapping)\n",
        "print(\"Number of unique labels:\", number_label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24GkcP7XhXn3",
        "outputId": "131a0b14-0d0a-451f-adbb-cdb99978d2d9"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'intent_fallback': 0, 'intent_learn_more': 1, 'intent_negative': 2, 'intent_neutral': 3, 'intent_positive': 4, 'silence': 5}\n",
            "Number of unique labels: 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "bknqLH2piJMv"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Bước 2: Chuẩn bị tokenizer và token hóa dữ liệu\n",
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir = \"huggingface\")\n",
        "model = BERTIntentClassification(\n",
        "    model_name=model_name,\n",
        "    num_classes=6\n",
        ")\n",
        "model.freeze_bert() # Froze Layer BERT\n",
        "max_seq_length = 512\n",
        "\n",
        "\n",
        "def collate_fn(features):\n",
        "    inputs = []\n",
        "    labels = []\n",
        "    for element in features:\n",
        "        inputs.append(element.get(\"input_ids\"))\n",
        "        labels.append(element.get(\"label\"))\n",
        "\n",
        "    labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    token_inputs = tokenizer(\n",
        "        inputs,\n",
        "        add_special_tokens=True,\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=max_seq_length,\n",
        "        return_overflowing_tokens=False,\n",
        "        return_length=False,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "    token_inputs.update({\n",
        "        \"labels\": labels,\n",
        "    })\n",
        "    return token_inputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZldUk54pj1N"
      },
      "source": [
        "# 3. Train Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptK7Cy22p2GK"
      },
      "source": [
        "## 3.1 Log Wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "xZkR3vuFp5uN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "264cc742-ca1b-44e4-a76f-e549f784a9cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.19.3)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.25.5)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.10.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.19.2)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.12.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.12.14)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "qux2ABzMp7Ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf74c8cc-07a3-4318-c561-853d743e4f7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (1.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "ZZRspV7jp8OT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "139aa6fa-97ce-49f2-f3d2-915dd14ff8c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "c8767\n"
          ]
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "# Load biến môi trường từ file .env\n",
        "load_dotenv()\n",
        "\n",
        "# Lấy key từ biến môi trường\n",
        "wandb_api_key = os.getenv(\"WANDB_API_KEY\")\n",
        "print(wandb_api_key[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "215vQ7cOp9oo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4bc27e9-57ce-424a-f075-be20753540fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "import wandb\n",
        "import os\n",
        "\n",
        "# Lấy API key từ biến môi trường và đăng nhập\n",
        "wandb.login(key=os.getenv(\"WANDB_API_KEY\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJG_rUYTqwLJ"
      },
      "source": [
        "Cách thiết lập thông qua TrainingArguments\n",
        "Khi sử dụng Trainer, bạn có thể đặt tên dự án trực tiếp trong TrainingArguments bằng cách sử dụng tham số report_to và run_name. Tuy nhiên, để đặt project, bạn cần khởi tạo một phiên wandb trước hoặc truyền cấu hình này thông qua wandb.init().\n",
        "\n",
        "Điều chỉnh TrainingArguments:\n",
        "```python\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results_\",          # Thư mục lưu kết quả\n",
        "    eval_strategy=\"epoch\",           # Đánh giá sau mỗi epoch\n",
        "    learning_rate=2e-4,\n",
        "    per_device_train_batch_size=128,\n",
        "    per_device_eval_batch_size=128,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",            # Thư mục lưu log\n",
        "    logging_strategy=\"steps\",        # Log theo steps\n",
        "    logging_steps=10,                # Log sau mỗi 10 bước\n",
        "    save_strategy=\"epoch\",           # Lưu checkpoint sau mỗi epoch\n",
        "    save_total_limit=3,              # Lưu tối đa 3 checkpoint\n",
        "    report_to=\"wandb\",               # Báo cáo log tới wandb\n",
        "    run_name=\"bert_run_1\"            # Tên phiên chạy trên wandb\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAMgPe3NqAhz"
      },
      "source": [
        "## 3.2 Train"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from concurrent.futures import ThreadPoolExecutor\n",
        "# import wandb\n",
        "# import os\n",
        "# import shutil\n",
        "# import time\n",
        "\n",
        "# class TrainerCustom(Trainer):\n",
        "#     def __init__(self, *args, save_every_n_epochs=10, **kwargs):\n",
        "#         super().__init__(*args, **kwargs)\n",
        "#         if torch.cuda.is_available():\n",
        "#             print(f\"Trainer is running on GPU: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
        "#         else:\n",
        "#             print(\"Trainer is running on CPU.\")\n",
        "\n",
        "#         self.best_eval_loss = float(\"inf\")  # Giá trị loss tốt nhất ban đầu\n",
        "#         self.save_every_n_epochs = save_every_n_epochs  # Tần suất lưu lên WandB\n",
        "#         self.best_model_info = {\"epoch\": None, \"loss\": None}\n",
        "#         self.last_saved_epoch = 0  # Epoch cuối cùng đã lưu Best Model và Last Model\n",
        "#         self.executor = ThreadPoolExecutor(max_workers=3)  # Cho phép tối đa 2 luồng song song\n",
        "\n",
        "#     def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "#         \"\"\"\n",
        "#         How the loss is computed by Trainer. By default, all models return the loss in the first element.\n",
        "\n",
        "#         Subclass and override for custom behavior.\n",
        "#         \"\"\"\n",
        "\n",
        "#         # # Kiểm tra thiết bị của mô hình và dữ liệu\n",
        "#         # print(\"Model device:\", next(model.parameters()).device)\n",
        "#         # print(\"Input device:\", inputs[\"input_ids\"].device)\n",
        "#         if \"labels\" in inputs:\n",
        "#             labels = inputs.pop(\"labels\")\n",
        "#         else:\n",
        "#             labels = None\n",
        "\n",
        "#         # Sử dụng nn.CrossEntropyLoss() thay vì nn.CrossEntropy\n",
        "#         cross_entropy_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "#         # Chạy mô hình và nhận đầu ra (logits)\n",
        "#         outputs = model(**inputs)\n",
        "\n",
        "#         # Đảm bảo lấy logits từ outputs (mô hình trả về tuple, lấy phần tử đầu tiên là logits)\n",
        "#         logits = outputs\n",
        "\n",
        "#         if labels is None:\n",
        "#             print(\"Labels are None during compute_loss.\")\n",
        "#         if logits is None:\n",
        "#             print(\"Logits are None during compute_loss.\")\n",
        "\n",
        "#         # Tính toán loss\n",
        "#         loss = cross_entropy_loss(logits, labels)\n",
        "\n",
        "#         # Trả về loss và outputs nếu cần\n",
        "#         return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "#     def async_save_model(self, model_dir, artifact_name, metadata=None):\n",
        "#         \"\"\"\n",
        "#         Lưu mô hình vào local và đồng bộ lên WandB trong luồng song song.\n",
        "#         \"\"\"\n",
        "#         def save():\n",
        "#             start_time = time.time()\n",
        "#             try:\n",
        "#                 # Xóa tất cả các thư mục tmp_best_model_ trước đó\n",
        "#                 for folder in os.listdir(\".\"):\n",
        "#                     if folder.startswith(\"tmp_best_model_epoch_\") and folder != model_dir:\n",
        "#                         shutil.rmtree(folder, ignore_errors=True)\n",
        "#                         print(f\"Removed old temporary directory: {folder}\")\n",
        "\n",
        "#                 # Lưu mô hình vào thư mục tạm\n",
        "#                 self.save_model(model_dir)\n",
        "\n",
        "#                 # Đồng bộ lên WandB\n",
        "#                 artifact = wandb.Artifact(artifact_name, type=\"model\")\n",
        "#                 artifact.add_dir(model_dir)\n",
        "#                 if metadata:\n",
        "#                     artifact.metadata = metadata\n",
        "#                 wandb.log_artifact(artifact)\n",
        "#             except Exception as e:\n",
        "#                 print(f\"Error during saving or syncing model {artifact_name}: {e}\")\n",
        "#             finally:\n",
        "#                 # Xóa thư mục tạm hiện tại sau khi đồng bộ\n",
        "#                 try:\n",
        "#                     shutil.rmtree(model_dir, ignore_errors=True)\n",
        "#                     print(f\"Successfully removed temporary directory: {model_dir}\")\n",
        "#                 except Exception as e:\n",
        "#                     print(f\"Error removing temporary directory {model_dir}: {e}\")\n",
        "\n",
        "#             elapsed_time = time.time() - start_time\n",
        "#             print(f\"Model saved and uploaded to WandB: {artifact_name} in {elapsed_time:.2f} seconds\")\n",
        "\n",
        "#         self.executor.submit(save)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#     def evaluate(self, eval_dataset=None, ignore_keys=None, metric_key_prefix: str = \"eval\"):\n",
        "#         metrics = super().evaluate(eval_dataset, ignore_keys, metric_key_prefix)\n",
        "#         eval_loss = metrics.get(\"eval_loss\")\n",
        "\n",
        "#         # Cập nhật Best Model nếu eval_loss giảm\n",
        "#         # Lưu Best Model ngay khi eval_loss giảm (local).\n",
        "#         # Chỉ đồng bộ lên WandB mỗi 10 epochs.\n",
        "\n",
        "#         if eval_loss is not None and eval_loss < self.best_eval_loss:\n",
        "#             print(f\"New best eval_loss: {eval_loss}\")\n",
        "#             self.best_eval_loss = eval_loss\n",
        "#             self.best_model_info = {\"epoch\": self.state.epoch, \"loss\": eval_loss}\n",
        "\n",
        "#             # Log thông tin Best Model lên WandB\n",
        "#             wandb.log({\n",
        "#                 \"best_eval_loss\": self.best_eval_loss,\n",
        "#                 \"best_model_epoch\": self.best_model_info.get(\"epoch\", -1)\n",
        "#             })\n",
        "\n",
        "#             # Lưu Best Model vào thư mục tạm (local)\n",
        "#             best_model_dir = f\"./tmp_best_model_epoch_{int(self.state.epoch)}\"\n",
        "#             self.save_model(best_model_dir)\n",
        "\n",
        "#             # Đồng bộ lên WandB mỗi 10 epochs\n",
        "#             if int(self.state.epoch) % self.save_every_n_epochs == 0:\n",
        "#                 artifact_name = f\"best_model_epoch_{int(self.state.epoch)}\"\n",
        "#                 self.async_save_model(best_model_dir, artifact_name, self.best_model_info)\n",
        "\n",
        "#         return metrics\n",
        "\n",
        "#     def save_last_model(self):\n",
        "#         \"\"\"\n",
        "#         Lưu Last Model lên WandB sau mỗi N epochs.\n",
        "#         \"\"\"\n",
        "#         if int(self.state.epoch) % self.save_every_n_epochs == 0 and int(self.state.epoch) != self.last_saved_epoch:\n",
        "#             print(f\"Saving Last Model at epoch {self.state.epoch} to WandB...\")\n",
        "#             last_model_dir = f\"./tmp_last_model_epoch_{int(self.state.epoch)}\"\n",
        "#             artifact_name = f\"last_model_epoch_{int(self.state.epoch)}\"\n",
        "#             self.async_save_model(last_model_dir, artifact_name)\n",
        "\n",
        "#             # Log thông tin Last Model lên WandB\n",
        "#             wandb.log({\n",
        "#                 \"last_model_epoch\": self.state.epoch\n",
        "#             })\n",
        "\n",
        "#             # Cập nhật epoch cuối cùng đã lưu\n",
        "#             self.last_saved_epoch = int(self.state.epoch)\n",
        "\n",
        "#     def train(self, *args, **kwargs):\n",
        "#         result = super().train(*args, **kwargs)\n",
        "\n",
        "#         # Sau mỗi epoch, lưu Last Model lên WandB\n",
        "#         self.save_last_model()\n",
        "#         # Chờ tất cả các luồng lưu hoàn thành trước khi kết thúc\n",
        "#         self.executor.shutdown(wait=True)\n",
        "\n",
        "#         return result\n",
        "\n",
        "\n",
        "# # Bước 6: Cài đặt tham số huấn luyện\n",
        "# training_args = TrainingArguments(\n",
        "#     output_dir=\"./result__s\",          # Thư mục lưu kết quả\n",
        "#     eval_strategy=\"epoch\",    # Đánh giá sau mỗi epoch\n",
        "#     learning_rate=2e-4,\n",
        "#     per_device_train_batch_size=128,\n",
        "#     per_device_eval_batch_size=128,\n",
        "#     num_train_epochs=30,\n",
        "#     weight_decay=0.01,\n",
        "#     logging_dir=\"./logs\",\n",
        "#     logging_strategy=\"steps\",\n",
        "#     logging_steps=1,  # Ghi logs mỗi 500 bước huấn luyện\n",
        "#     save_strategy=\"no\",          # Lưu trọng số sau mỗi epoch\n",
        "#     save_total_limit=3,\n",
        "#     label_names = [\"labels\"],\n",
        "#     report_to=\"wandb\",\n",
        "#     run_name=\"bert_run_3\"\n",
        "# )\n",
        "\n",
        "\n",
        "# import wandb\n",
        "\n",
        "# # Khởi tạo wandb\n",
        "# wandb.init(\n",
        "#     project=\"bert-intent-classification\",  # Tên dự án\n",
        "#     name=\"bert_run_3\",                     # Tên phiên chạy\n",
        "#     config={\"gpu\": torch.cuda.get_device_name(torch.cuda.current_device()) if torch.cuda.is_available() else \"CPU\"}\n",
        "# )\n",
        "\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# model = model.to(device)\n",
        "# if torch.cuda.is_available():\n",
        "#     print(f\"Trainer is running on GPU: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
        "# else:\n",
        "#     print(\"Trainer is running on CPU.\")\n",
        "\n",
        "# trainer = TrainerCustom(\n",
        "#     model=model,\n",
        "#     args=training_args,\n",
        "#     train_dataset=sample_train_dataset,\n",
        "#     eval_dataset=sample_test_dataset,\n",
        "#     data_collator=collate_fn,\n",
        "#     save_every_n_epochs=10  # Lưu Best Model và Last Model mỗi 10 epochs\n",
        "# )\n",
        "\n",
        "# trainer.train()\n",
        "\n",
        "\n",
        "# wandb.finish()\n"
      ],
      "metadata": {
        "id": "rSkdmXg6ttvC"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 Continue Train"
      ],
      "metadata": {
        "id": "VWu_0pfhtJ6S"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3hWcz751mEx"
      },
      "source": [
        "### Ver 1.2.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLIKjrdaesyG"
      },
      "source": [
        "Thui, ko lưu local nữa, lưu tất trên wandb đi.\n",
        "- Với best model: lưu lên wandb khi loss giảm và đã sau 10 epochs  \n",
        "(Lưu Best Model ngay khi eval_loss giảm ở local, sau 10 epochs thì đồng bộ cái best lên wandb, sau đó xoá các file best ở local).\n",
        "Chỉ đồng bộ lên WandB mỗi 10 epochs.)\n",
        "- Với last model: lưu lên wandb sau mỗi 10 epochs. (lưu local trước -> đồng bộ lên wandb sẽ xoá file local)\n",
        "+, Trong quá trình lưu thì việc training vẫn diễn ra Parallel\n",
        "\n",
        "đều lưu đầy đủ toàn bộ tham số để có thể train thêm từ cả ở best model và last model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import wandb\n",
        "import os\n",
        "import shutil\n",
        "import time\n",
        "\n",
        "class TrainerCustom(Trainer):\n",
        "    def __init__(self, *args, save_every_n_epochs=10, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        if torch.cuda.is_available():\n",
        "            print(f\"Trainer is running on GPU: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
        "        else:\n",
        "            print(\"Trainer is running on CPU.\")\n",
        "\n",
        "        self.best_eval_loss = float(\"inf\")  # Giá trị loss tốt nhất ban đầu\n",
        "        self.save_every_n_epochs = save_every_n_epochs  # Tần suất lưu lên WandB\n",
        "        self.best_model_info = {\"epoch\": None, \"loss\": None}\n",
        "        self.last_saved_epoch = 0  # Epoch cuối cùng đã lưu Best Model và Last Model\n",
        "        self.executor = ThreadPoolExecutor(max_workers=3)  # Cho phép tối đa 2 luồng song song\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        \"\"\"\n",
        "        How the loss is computed by Trainer. By default, all models return the loss in the first element.\n",
        "\n",
        "        Subclass and override for custom behavior.\n",
        "        \"\"\"\n",
        "\n",
        "        # # Kiểm tra thiết bị của mô hình và dữ liệu\n",
        "        # print(\"Model device:\", next(model.parameters()).device)\n",
        "        # print(\"Input device:\", inputs[\"input_ids\"].device)\n",
        "        if \"labels\" in inputs:\n",
        "            labels = inputs.pop(\"labels\")\n",
        "        else:\n",
        "            labels = None\n",
        "\n",
        "        # Sử dụng nn.CrossEntropyLoss() thay vì nn.CrossEntropy\n",
        "        cross_entropy_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "        # Chạy mô hình và nhận đầu ra (logits)\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "        # Đảm bảo lấy logits từ outputs (mô hình trả về tuple, lấy phần tử đầu tiên là logits)\n",
        "        logits = outputs\n",
        "\n",
        "        if labels is None:\n",
        "            print(\"Labels are None during compute_loss.\")\n",
        "        if logits is None:\n",
        "            print(\"Logits are None during compute_loss.\")\n",
        "\n",
        "        # Tính toán loss\n",
        "        loss = cross_entropy_loss(logits, labels)\n",
        "\n",
        "        # Trả về loss và outputs nếu cần\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "    def async_save_model(self, model_dir, artifact_name, metadata=None):\n",
        "        \"\"\"\n",
        "        Lưu mô hình vào local và đồng bộ lên WandB trong luồng song song.\n",
        "        \"\"\"\n",
        "        def save():\n",
        "            start_time = time.time()\n",
        "            try:\n",
        "                # Xóa tất cả các thư mục tmp_best_model_ trước đó\n",
        "                for folder in os.listdir(\".\"):\n",
        "                    if folder.startswith(\"tmp_best_model_epoch_\") and folder != model_dir:\n",
        "                        shutil.rmtree(folder, ignore_errors=True)\n",
        "                        print(f\"Removed old temporary directory: {folder}\")\n",
        "\n",
        "                # Lưu mô hình vào thư mục tạm\n",
        "                self.save_model(model_dir)\n",
        "\n",
        "                # Đồng bộ lên WandB\n",
        "                artifact = wandb.Artifact(artifact_name, type=\"model\")\n",
        "                artifact.add_dir(model_dir)\n",
        "                if metadata:\n",
        "                    artifact.metadata = metadata\n",
        "                wandb.log_artifact(artifact)\n",
        "            except Exception as e:\n",
        "                print(f\"Error during saving or syncing model {artifact_name}: {e}\")\n",
        "            finally:\n",
        "                # Xóa thư mục tạm hiện tại sau khi đồng bộ\n",
        "                try:\n",
        "                    shutil.rmtree(model_dir, ignore_errors=True)\n",
        "                    print(f\"Successfully removed temporary directory: {model_dir}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error removing temporary directory {model_dir}: {e}\")\n",
        "\n",
        "            elapsed_time = time.time() - start_time\n",
        "            print(f\"Model saved and uploaded to WandB: {artifact_name} in {elapsed_time:.2f} seconds\")\n",
        "\n",
        "        self.executor.submit(save)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def evaluate(self, eval_dataset=None, ignore_keys=None, metric_key_prefix: str = \"eval\"):\n",
        "        metrics = super().evaluate(eval_dataset, ignore_keys, metric_key_prefix)\n",
        "        eval_loss = metrics.get(\"eval_loss\")\n",
        "\n",
        "        # Cập nhật Best Model nếu eval_loss giảm\n",
        "        # Lưu Best Model ngay khi eval_loss giảm (local).\n",
        "        # Chỉ đồng bộ lên WandB mỗi 10 epochs.\n",
        "\n",
        "        if eval_loss is not None and eval_loss < self.best_eval_loss:\n",
        "            print(f\"New best eval_loss: {eval_loss}\")\n",
        "            self.best_eval_loss = eval_loss\n",
        "            self.best_model_info = {\"epoch\": self.state.epoch, \"loss\": eval_loss}\n",
        "\n",
        "            # Log thông tin Best Model lên WandB\n",
        "            wandb.log({\n",
        "                \"best_eval_loss\": self.best_eval_loss,\n",
        "                \"best_model_epoch\": self.best_model_info.get(\"epoch\", -1)\n",
        "            })\n",
        "\n",
        "            # Lưu Best Model vào thư mục tạm (local)\n",
        "            best_model_dir = f\"./tmp_best_model_epoch_{int(self.state.epoch)}\"\n",
        "            self.save_model(best_model_dir)\n",
        "\n",
        "            # Đồng bộ lên WandB mỗi 10 epochs\n",
        "            if int(self.state.epoch) % self.save_every_n_epochs == 0:\n",
        "                artifact_name = f\"best_model_epoch_{int(self.state.epoch)}\"\n",
        "                self.async_save_model(best_model_dir, artifact_name, self.best_model_info)\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def save_last_model(self):\n",
        "        \"\"\"\n",
        "        Lưu Last Model lên WandB sau mỗi N epochs.\n",
        "        \"\"\"\n",
        "        if int(self.state.epoch) % self.save_every_n_epochs == 0 and int(self.state.epoch) != self.last_saved_epoch:\n",
        "            print(f\"Saving Last Model at epoch {self.state.epoch} to WandB...\")\n",
        "            last_model_dir = f\"./tmp_last_model_epoch_{int(self.state.epoch)}\"\n",
        "            artifact_name = f\"last_model_epoch_{int(self.state.epoch)}\"\n",
        "            self.async_save_model(last_model_dir, artifact_name)\n",
        "\n",
        "            # Log thông tin Last Model lên WandB\n",
        "            wandb.log({\n",
        "                \"last_model_epoch\": self.state.epoch\n",
        "            })\n",
        "\n",
        "            # Cập nhật epoch cuối cùng đã lưu\n",
        "            self.last_saved_epoch = int(self.state.epoch)\n",
        "\n",
        "    def train(self, *args, **kwargs):\n",
        "        result = super().train(*args, **kwargs)\n",
        "\n",
        "        # Sau mỗi epoch, lưu Last Model lên WandB\n",
        "        self.save_last_model()\n",
        "        # Chờ tất cả các luồng lưu hoàn thành trước khi kết thúc\n",
        "        self.executor.shutdown(wait=True)\n",
        "\n",
        "        return result\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7qjF1K2EW0Ys"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- model.safetensors là tệp trọng số được lưu bằng thư viện safetensors, không phải định dạng PyTorch tiêu chuẩn (.bin hoặc .pt).\n",
        "- Để tải tệp này, bạn cần sử dụng thư viện safetensors thay vì torch.load().\n",
        "\n",
        "```\n",
        "model = model.to(device)\n",
        "model.load_state_dict(torch.load(weights_path, map_location=\"cpu\"))\n",
        "```\n",
        "\n",
        "```\n",
        "# Đường dẫn đến file trọng số\n",
        "weights_path = os.path.join(artifact_dir, \"model.safetensors\")\n",
        "\n",
        "from safetensors.torch import load_file\n",
        "\n",
        "weights = load_file(weights_path)  # Tải trọng số từ tệp .safetensors\n",
        "\n",
        "# Áp dụng trọng số vào mô hình\n",
        "model.load_state_dict(weights)\n",
        "model = model.to(device)\n",
        "```"
      ],
      "metadata": {
        "id": "u9J0ZcIX1AIt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install safetensors"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjKg_dBj1DtL",
        "outputId": "45616516-a3e9-4dae-afe9-0b5c2ba2da69"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (0.5.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoConfig,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        ")\n",
        "from safetensors.torch import load_file\n",
        "import wandb\n",
        "\n",
        "\n",
        "# 1. Initialize WandB and download artifact\n",
        "run = wandb.init(project=\"bert-intent-classification\", name=\"continue_training\")\n",
        "artifact = run.use_artifact('doanngoccuong_nh/bert-intent-classification/last_model_epoch_30:v3', type='model')\n",
        "artifact_dir = artifact.download()\n",
        "print(\"Files in artifact_dir:\", os.listdir(artifact_dir))\n",
        "\n",
        "# 2. Create config.json if not available\n",
        "config_path = os.path.join(artifact_dir, \"config.json\")\n",
        "if not os.path.exists(config_path):\n",
        "    print(\"Creating config.json...\")\n",
        "    config = AutoConfig.from_pretrained(\"bert-base-uncased\")\n",
        "    with open(config_path, \"w\") as f:\n",
        "        json.dump(config.to_dict(), f, indent=4)\n",
        "    print(f\"Config.json created at {config_path}\")\n",
        "else:\n",
        "    print(\"Config.json already exists.\")\n",
        "\n",
        "# 3. Load the model\n",
        "print(\"Loading model weights...\")\n",
        "weights_path = os.path.join(artifact_dir, \"model.safetensors\")\n",
        "model = BERTIntentClassification(model_name=\"bert-base-uncased\", num_classes=6)  # Ensure num_classes matches your dataset\n",
        "weights = load_file(weights_path)\n",
        "model.load_state_dict(weights)\n",
        "print(\"Model loaded successfully.\")\n",
        "\n",
        "# Move model to device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"Trainer is running on GPU: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
        "else:\n",
        "    print(\"Trainer is running on CPU.\")\n",
        "\n",
        "# 4. Load or create tokenizer\n",
        "print(\"Loading tokenizer...\")\n",
        "tokenizer_path = os.path.join(artifact_dir, \"tokenizer_config.json\")\n",
        "if os.path.exists(tokenizer_path):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(artifact_dir)\n",
        "else:\n",
        "    print(\"Tokenizer not found. Loading from bert-base-uncased...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "    tokenizer.save_pretrained(artifact_dir)\n",
        "    print(f\"Tokenizer saved to {artifact_dir}.\")\n",
        "\n",
        "# 5. Load training arguments and trainer state\n",
        "trainer_state_path = os.path.join(artifact_dir, \"trainer_state.json\")\n",
        "if os.path.exists(trainer_state_path):\n",
        "    print(f\"Loading trainer state from {trainer_state_path}...\")\n",
        "else:\n",
        "    print(\"Trainer state not found. Training will start fresh.\")\n",
        "\n",
        "# 6. Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./result__s\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    learning_rate=2e-4,\n",
        "    per_device_train_batch_size=128,\n",
        "    per_device_eval_batch_size=128,\n",
        "    num_train_epochs=30,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=3,\n",
        "    label_names=[\"labels\"],\n",
        "    report_to=\"wandb\",\n",
        "    run_name=\"bert_continue_training\"\n",
        ")\n",
        "\n",
        "\n",
        "# 8. Initialize Trainer\n",
        "print(\"Initializing Trainer...\")\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    data_collator=collate_fn,\n",
        "    # save_every_n_epochs=10  # Lưu Best Model và Last Model mỗi 10 epochs\n",
        ")\n",
        "\n",
        "# 9. Continue Training\n",
        "print(\"Starting training...\")\n",
        "trainer.train()\n",
        "\n",
        "# 10. Finish WandB run\n",
        "wandb.finish()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 584
        },
        "id": "S9OlOzQjvNSm",
        "outputId": "bc283964-fa8e-4070-9cdc-acc775b84003"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact last_model_epoch_30:v3, 419.96MB. 3 files... \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n",
            "Done. 0:0:1.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files in artifact_dir: ['models--bert-base-uncased', 'model.safetensors', 'trainer_state.json', 'vocab.txt', 'tokenizer_config.json', 'special_tokens_map.json', 'training_args.bin', 'tokenizer.json', '.locks', 'config.json']\n",
            "Config.json already exists.\n",
            "Loading model weights...\n",
            "Model loaded successfully.\n",
            "Trainer is running on GPU: Tesla T4\n",
            "Loading tokenizer...\n",
            "Loading trainer state from /content/artifacts/last_model_epoch_30:v3/trainer_state.json...\n",
            "Initializing Trainer...\n",
            "Starting training...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "grad can be implicitly created only for scalar outputs",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-00a68cf96225>\u001b[0m in \u001b[0;36m<cell line: 98>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;31m# 9. Continue Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting training...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;31m# 10. Finish WandB run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2162\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2163\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2164\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2165\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2166\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2522\u001b[0m                     )\n\u001b[1;32m   2523\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2524\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2526\u001b[0m                     if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3685\u001b[0m                 \u001b[0mscaled_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3686\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3687\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3688\u001b[0m             \u001b[0;31m# Finally we need to normalize the loss for reporting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3689\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2246\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlomo_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2247\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2248\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2250\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_trigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0mgrad_tensors_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tensor_or_tensors_to_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m     \u001b[0mgrad_tensors_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_grads_batched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mretain_graph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m    196\u001b[0m                     \u001b[0mout_numel_is_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mout_numel_is_1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m                     raise RuntimeError(\n\u001b[0m\u001b[1;32m    199\u001b[0m                         \u001b[0;34m\"grad can be implicitly created only for scalar outputs\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m                     )\n",
            "\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Đúng vậy, trong đoạn mã bạn cung cấp, `trainer.evaluate()` được thực hiện một cách tự động bởi lớp `Trainer` trong thư viện `transformers`. Cụ thể:\n",
        "\n",
        "### Trong TrainingArguments:\n",
        "```python\n",
        "training_args = TrainingArguments(\n",
        "    ...\n",
        "    eval_strategy=\"epoch\",  # Đánh giá sau mỗi epoch\n",
        "    ...\n",
        ")\n",
        "```\n",
        "**`eval_strategy=\"epoch\"`** có nghĩa là quá trình đánh giá (evaluation) sẽ tự động được thực hiện sau mỗi epoch, sử dụng `eval_dataset` mà bạn đã cung cấp trong `TrainerCustom`.\n",
        "\n",
        "### Trong `TrainerCustom`:\n",
        "Trong lớp `TrainerCustom`, phương thức `evaluate()` đã được override. Bên trong, nó:\n",
        "1. Gọi phương thức `super().evaluate()` từ lớp cha `Trainer`, thực hiện việc tính toán loss và các metric.\n",
        "2. Lưu thông tin về Best Model nếu phát hiện `eval_loss` giảm so với trước đó.\n",
        "3. Ghi log kết quả lên WandB.\n",
        "\n",
        "Vì vậy, trong khi huấn luyện (`trainer.train()`), `trainer.evaluate()` được gọi tự động sau mỗi epoch để thực hiện đánh giá và lưu Best Model.\n",
        "\n",
        "---\n",
        "\n",
        "### Kết luận:\n",
        "Bạn không cần gọi riêng `trainer.evaluate()` trong lúc training nếu đã cấu hình `eval_strategy=\"epoch\"`. Tuy nhiên, nếu bạn muốn đánh giá mô hình ở một thời điểm cụ thể ngoài quá trình training (ví dụ, sau khi huấn luyện xong), bạn vẫn có thể gọi `trainer.evaluate()` thủ công."
      ],
      "metadata": {
        "id": "AzcBU9s0jYby"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Bước 9: Đánh giá trên tập kiểm tra\n",
        "# trainer.evaluate()"
      ],
      "metadata": {
        "id": "rC3AmIe0T9co"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiq5bRFTmzv5"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dưới đây là bảng chi tiết hơn so sánh giữa `safetensors` và `pytorch_model.bin` dựa trên các tiêu chí quan trọng:\n",
        "\n",
        "| **Tiêu chí**                        | **Safetensors**                                                                                       | **Pytorch_model.bin**                                                                                     |\n",
        "|-------------------------------------|-------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------|\n",
        "| **Định dạng**                       | Lưu trữ tensor (trọng số) với định dạng nhị phân an toàn, không lưu metadata.                         | Lưu cả tensor và metadata, hỗ trợ trạng thái optimizer, scheduler, và nhiều thông tin khác.              |\n",
        "| **Kích thước tệp**                  | Nhẹ hơn, tối ưu hóa kích thước bằng cách loại bỏ metadata không cần thiết.                             | Lớn hơn, do lưu trữ đầy đủ thông tin của mô hình, bao gồm metadata.                                       |\n",
        "| **Tốc độ tải (Load speed)**         | **Nhanh hơn** nhờ tải tensor trực tiếp từ ổ đĩa.                                                      | Chậm hơn, cần tải toàn bộ tệp vào RAM trước khi sử dụng.                                                  |\n",
        "| **Hiệu quả bộ nhớ (Memory efficiency)** | Hỗ trợ **lazy loading**, chỉ tải các tensor cần thiết, tiết kiệm RAM.                                   | Không hỗ trợ lazy loading, cần bộ nhớ lớn để tải toàn bộ mô hình.                                         |\n",
        "| **Bảo mật (Security)**              | **Rất an toàn**, không hỗ trợ thực thi mã nhị phân, giảm rủi ro bảo mật.                               | **Không an toàn** nếu tệp bị chỉnh sửa ác ý, có thể thực thi mã nhị phân qua pickle.                      |\n",
        "| **Phổ biến (Popularity)**           | Ít phổ biến hơn, cần thư viện `safetensors` hoặc phiên bản Hugging Face >= 4.25.                       | Rất phổ biến, tiêu chuẩn trong cộng đồng PyTorch và Hugging Face.                                         |\n",
        "| **Khả năng tương thích (Compatibility)** | Hỗ trợ các framework hiện đại (PyTorch, TensorFlow, JAX).                                              | Tương thích mạnh mẽ với PyTorch, nhưng hạn chế khi chuyển đổi giữa các framework khác.                    |\n",
        "| **Triển khai inference (Deployment)**| Phù hợp với môi trường hiện đại, cần cài thư viện bổ sung (`safetensors`).                             | Tương thích rộng rãi trên mọi thiết bị và môi trường, không cần cài thêm thư viện.                        |\n",
        "| **Hỗ trợ thiết bị cũ**              | Yêu cầu môi trường hiện đại, không tương thích tốt với các thiết bị hoặc framework cũ.                 | Hỗ trợ tốt trên cả các thiết bị hoặc thư viện cũ.                                                         |\n",
        "| **Lưu trạng thái mô hình (Model state)** | Chỉ lưu tensor (trọng số mô hình), không lưu trạng thái optimizer hoặc scheduler.                     | Lưu đầy đủ trạng thái, phù hợp cho việc fine-tuning hoặc khôi phục huấn luyện.                            |\n",
        "| **Tốc độ triển khai inference**     | Nhanh hơn, đặc biệt khi tải mô hình lớn trên CPU.                                                     | Chậm hơn một chút, nhưng không đáng kể trên GPU hiện đại.                                                 |\n",
        "| **Hỗ trợ khi huấn luyện (Training)**| Cần cấu hình thêm (`save_safetensors=True`) trong `TrainingArguments`.                                | Dễ dàng sử dụng mặc định, không cần cấu hình thêm.                                                        |\n",
        "| **Hỗ trợ tối ưu hóa (Optimizer support)** | Không hỗ trợ lưu optimizer, cần xử lý riêng nếu tiếp tục huấn luyện.                                   | Hỗ trợ lưu optimizer, thuận tiện cho việc khôi phục và tiếp tục huấn luyện.                               |\n",
        "| **Hỗ trợ trên WandB (Weights & Biases)** | Tích hợp tốt, giảm kích thước tệp khi đồng bộ mô hình.                                                 | Hỗ trợ đầy đủ, nhưng kích thước lớn hơn có thể ảnh hưởng đến tốc độ đồng bộ.                              |\n",
        "| **Tính năng đặc biệt**              | - Hỗ trợ chia nhỏ mô hình lớn khi tải.<br>- Tăng tốc inference trên môi trường sản xuất (production).  | - Tích hợp sâu với các công cụ debug và khôi phục huấn luyện.                                             |\n",
        "\n",
        "---\n",
        "\n",
        "### **Gợi ý sử dụng**\n",
        "#### **Khi nào chọn `safetensors`:**\n",
        "- Khi bạn ưu tiên tốc độ và bảo mật.\n",
        "- Khi làm việc trên các môi trường hiện đại, yêu cầu hiệu suất cao.\n",
        "- Khi kích thước tệp nhỏ gọn là ưu tiên (ví dụ: triển khai trên thiết bị biên).\n",
        "\n",
        "#### **Khi nào chọn `pytorch_model.bin`:**\n",
        "- Khi bạn cần tương thích rộng rãi với các thiết bị hoặc thư viện.\n",
        "- Khi bạn cần lưu cả trạng thái optimizer và scheduler để tiếp tục huấn luyện.\n",
        "- Khi triển khai trên các hệ thống cũ hoặc không hỗ trợ `safetensors`.\n",
        "\n",
        "---\n",
        "\n",
        "Nếu bạn cần thêm thông tin cụ thể hoặc muốn thử nghiệm một trường hợp thực tế, hãy cho mình biết nhé! 🚀"
      ],
      "metadata": {
        "id": "hw3L65VRn1e9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "ValueError: Unrecognized model in /content/artifacts/last_model_epoch_30:v0. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, align, altclip, audio\n",
        "```\n",
        "Lỗi trên xảy ra do thư mục chứa mô hình tải về từ WandB (artifact_dir) không có tệp config.json hoặc tokenizer_config.json, là các tệp bắt buộc để AutoTokenizer.from_pretrained() xác định loại mô hình và tokenizer."
      ],
      "metadata": {
        "id": "oJ6krKPHn93-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cần cả **`config.json`** và **`tokenizer_config.json`** nếu bạn sử dụng `AutoTokenizer.from_pretrained()` để tải tokenizer, vì mỗi tệp đóng vai trò riêng biệt trong việc định cấu hình mô hình và tokenizer.\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Vai trò của các tệp**\n",
        "\n",
        "#### **`config.json`**\n",
        "- **Mô tả mô hình (model configuration):**\n",
        "  - Cấu trúc và các siêu tham số của mô hình, như:\n",
        "    - `model_type` (ví dụ: `bert`, `gpt2`).\n",
        "    - `hidden_size`, `num_attention_heads`, `num_hidden_layers`.\n",
        "  - Dùng để khởi tạo mô hình bằng `AutoModel.from_pretrained()`.\n",
        "\n",
        "#### **`tokenizer_config.json`**\n",
        "- **Cấu hình tokenizer:**\n",
        "  - Quy định các tham số liên quan đến tokenizer, như:\n",
        "    - `do_lower_case`: Có chuyển đổi chữ hoa thành chữ thường không.\n",
        "    - `model_type`: Loại mô hình liên kết với tokenizer (ví dụ: `bert`).\n",
        "    - Các thông tin bổ sung như `max_length`, `padding_side`, `special_tokens_map`.\n",
        "\n",
        "- **Cần thiết cho `AutoTokenizer.from_pretrained()`**, để tải chính xác tokenizer và cấu hình các bước xử lý đầu vào.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Khi nào cần cả hai tệp?**\n",
        "\n",
        "- **Cần cả hai tệp nếu:**\n",
        "  - Bạn sử dụng `AutoTokenizer.from_pretrained()` để tải tokenizer và muốn đảm bảo cấu hình đầy đủ.\n",
        "  - Mô hình cần xử lý đầu vào đặc biệt (ví dụ: với `do_lower_case=True` hoặc sử dụng các token đặc biệt).\n",
        "  - Bạn muốn đồng bộ hóa cấu hình giữa mô hình (`config.json`) và tokenizer (`tokenizer_config.json`).\n",
        "\n",
        "- **Chỉ cần `config.json` nếu:**\n",
        "  - Bạn chỉ tải mô hình bằng `AutoModel.from_pretrained()` mà không sử dụng tokenizer.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Hậu quả nếu thiếu `tokenizer_config.json`**\n",
        "\n",
        "Nếu thiếu **`tokenizer_config.json`**:\n",
        "- `AutoTokenizer.from_pretrained()` sẽ không thể tự động thiết lập các tham số như `do_lower_case` hoặc các token đặc biệt (`[CLS]`, `[SEP]`).\n",
        "- Bạn sẽ cần phải chỉ định thủ công các tham số khi khởi tạo tokenizer, ví dụ:\n",
        "\n",
        "```python\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    artifact_dir,\n",
        "    do_lower_case=True,  # Thiết lập thủ công\n",
        "    max_length=512\n",
        ")\n",
        "```"
      ],
      "metadata": {
        "id": "vJtJO9k9okko"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "artifact_dir/\n",
        "│\n",
        "├── config.json\n",
        "├── tokenizer_config.json\n",
        "├── vocab.txt  # hoặc tokenizer.json\n",
        "├── model.safetensors\n",
        "├── training_args.bin\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "B9Bt4_V3psN2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import wandb\n",
        "\n",
        "# 1. Tải mô hình từ artifact trên WandB\n",
        "run = wandb.init(project=\"bert-intent-classification\")  # Tên dự án trong WandB\n",
        "artifact = run.use_artifact('doanngoccuong_nh/bert-intent-classification/last_model_epoch_30:v0', type='model')\n",
        "artifact_dir = artifact.download()\n",
        "print(os.listdir(artifact_dir))"
      ],
      "metadata": {
        "id": "Azar1JnApvdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# import json\n",
        "# import torch\n",
        "# from transformers import AutoTokenizer, AutoModel\n",
        "# import wandb\n",
        "\n",
        "# # 1. Tải mô hình từ artifact trên WandB\n",
        "# run = wandb.init(project=\"bert-intent-classification\")  # Tên dự án trong WandB\n",
        "# artifact = run.use_artifact('doanngoccuong_nh/bert-intent-classification/last_model_epoch_30:v0', type='model')\n",
        "# artifact_dir = artifact.download()\n",
        "# print(\"Files in artifact_dir:\", os.listdir(artifact_dir))\n",
        "\n",
        "# # Đường dẫn tệp cấu hình\n",
        "# config_path = os.path.join(artifact_dir, \"config.json\")\n",
        "# tokenizer_config_path = os.path.join(artifact_dir, \"tokenizer_config.json\")\n",
        "\n",
        "# # 2. Tạo file config.json\n",
        "# config = {\n",
        "#     \"model_type\": \"bert\",\n",
        "#     \"hidden_size\": 768,\n",
        "#     \"num_attention_heads\": 12,\n",
        "#     \"num_hidden_layers\": 12,\n",
        "#     \"vocab_size\": 30522\n",
        "# }\n",
        "\n",
        "# with open(config_path, \"w\") as f:\n",
        "#     json.dump(config, f, indent=4)\n",
        "\n",
        "# print(f\"Config.json created at {config_path}\")\n",
        "\n",
        "# # 3. Tạo file tokenizer_config.json\n",
        "# tokenizer_config = {\n",
        "#     \"model_type\": \"bert\",\n",
        "#     \"do_lower_case\": True,\n",
        "#     \"max_length\": 512,\n",
        "#     \"padding_side\": \"right\",\n",
        "#     \"special_tokens_map\": {\n",
        "#         \"[CLS]\": \"[CLS]\",\n",
        "#         \"[SEP]\": \"[SEP]\",\n",
        "#         \"[PAD]\": \"[PAD]\"\n",
        "#     }\n",
        "# }\n",
        "\n",
        "# with open(tokenizer_config_path, \"w\") as f:\n",
        "#     json.dump(tokenizer_config, f, indent=4)\n",
        "\n",
        "# print(f\"Tokenizer_config.json created at {tokenizer_config_path}\")\n",
        "\n",
        "# # 4. Tải mô hình đã lưu và tokenizer\n",
        "# model_path = artifact_dir  # Đường dẫn đến mô hình đã tải\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "# model = AutoModel.from_pretrained(model_path)\n",
        "\n",
        "# # Chuyển mô hình sang chế độ đánh giá\n",
        "# model.eval()\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# model = model.to(device)\n",
        "\n",
        "# print(f\"Model loaded and running on device: {device}\")\n",
        "\n",
        "# # 5. Xử lý đầu vào\n",
        "# sentence = \"What is the weather like today?\"\n",
        "# inputs = tokenizer(\n",
        "#     sentence,\n",
        "#     return_tensors=\"pt\",\n",
        "#     truncation=True,\n",
        "#     padding=True,\n",
        "#     max_length=512\n",
        "# )\n",
        "\n",
        "# # Chuyển đầu vào sang thiết bị phù hợp\n",
        "# inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "# # 6. Thực hiện dự đoán\n",
        "# with torch.no_grad():\n",
        "#     outputs = model(**inputs)  # Truyền đầu vào qua mô hình\n",
        "#     logits = outputs[0]  # Lấy logits từ đầu ra của mô hình\n",
        "#     predicted_class = torch.argmax(logits, dim=1).item()  # Lấy nhãn dự đoán\n",
        "\n",
        "# # 7. Mapping nhãn dự đoán sang tên nhãn\n",
        "# label_mapping = {0: \"intent_positive\", 1: \"intent_negative\", 2: \"intent_neutral\", 3: \"intent_fallback\", 4: \"silence\"}\n",
        "# predicted_label = label_mapping.get(predicted_class, \"Unknown\")\n",
        "\n",
        "# # 8. In kết quả dự đoán\n",
        "# print(f\"Input sentence: {sentence}\")\n",
        "# print(f\"Predicted class ID: {predicted_class}\")\n",
        "# print(f\"Predicted label: {predicted_label}\")\n",
        "\n",
        "# # Kết thúc phiên WandB\n",
        "# wandb.finish()\n"
      ],
      "metadata": {
        "id": "IA94LfitpIqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tải và lưu tokenizer từ model gốc"
      ],
      "metadata": {
        "id": "qQChSMCxqLOf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import wandb\n",
        "\n",
        "# 1. Tải mô hình từ artifact trên WandB\n",
        "run = wandb.init(project=\"bert-intent-classification\")  # Tên dự án trong WandB\n",
        "artifact = run.use_artifact('doanngoccuong_nh/bert-intent-classification/last_model_epoch_30:v0', type='model')\n",
        "artifact_dir = artifact.download()\n",
        "print(\"Files in artifact_dir:\", os.listdir(artifact_dir))\n",
        "\n",
        "# Tải tokenizer từ mô hình gốc\n",
        "original_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Lưu các tệp cần thiết vào artifact_dir\n",
        "original_tokenizer.save_pretrained(artifact_dir)\n",
        "\n",
        "print(f\"Tokenizer files saved to {artifact_dir}\")\n",
        "\n",
        "# 4. Tải mô hình đã lưu và tokenizer\n",
        "model_path = artifact_dir  # Đường dẫn đến mô hình đã tải\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "\n",
        "# Chuyển mô hình sang chế độ đánh giá\n",
        "model.eval()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "print(f\"Model loaded and running on device: {device}\")\n",
        "\n",
        "# 5. Xử lý đầu vào\n",
        "question = \"What is the weather like today?\"\n",
        "answer = \"\"\n",
        "inputs = tokenizer(\n",
        "    sentence,\n",
        "    return_tensors=\"pt\",\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "    max_length=512\n",
        ")\n",
        "\n",
        "# Chuyển đầu vào sang thiết bị phù hợp\n",
        "inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "# 6. Thực hiện dự đoán\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)  # Truyền đầu vào qua mô hình\n",
        "    logits = outputs.logits  # Lấy logits từ đầu ra của mô hình\n",
        "    predicted_class = torch.argmax(logits, dim=1).item()  # Lấy nhãn dự đoán\n",
        "\n",
        "# 7. Mapping nhãn dự đoán sang tên nhãn\n",
        "label_mapping = {0: \"intent_positive\", 1: \"intent_negative\", 2: \"intent_neutral\", 3: \"intent_fallback\", 4: \"silence\"}\n",
        "predicted_label = label_mapping.get(predicted_class, \"Unknown\")\n",
        "\n",
        "# 8. In kết quả dự đoán\n",
        "print(f\"Input sentence: {sentence}\")\n",
        "print(f\"Predicted class ID: {predicted_class}\")\n",
        "print(f\"Predicted label: {predicted_label}\")\n",
        "\n",
        "# Kết thúc phiên WandB\n",
        "wandb.finish()\n"
      ],
      "metadata": {
        "id": "SSvcjYuTk2T7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3c50229b7c484c738b70960585d9a192": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_040fd1fe0cc2463f951c1f69640b3bc7",
              "IPY_MODEL_45c366bdf5c14a238d20ca955f213ab7",
              "IPY_MODEL_dd8529db4f1d4712a6e09cdff2aa9a04"
            ],
            "layout": "IPY_MODEL_23ea11595dc146279338e8aaa2c67c9b"
          }
        },
        "040fd1fe0cc2463f951c1f69640b3bc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3081da01e0f54970aa6a3c4e54b8cb06",
            "placeholder": "​",
            "style": "IPY_MODEL_40ccfe508ef444d59da1d85da527c681",
            "value": "Map: 100%"
          }
        },
        "45c366bdf5c14a238d20ca955f213ab7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e0e6fa43efeb4980a685e398965fddd1",
            "max": 120,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b1ef581b00aa4983a7d96e3932e173b9",
            "value": 120
          }
        },
        "dd8529db4f1d4712a6e09cdff2aa9a04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8dde65697b5e4f07b22780df3b2ed994",
            "placeholder": "​",
            "style": "IPY_MODEL_cb1314888ee5473799a49e1784559726",
            "value": " 120/120 [00:00&lt;00:00, 1822.49 examples/s]"
          }
        },
        "23ea11595dc146279338e8aaa2c67c9b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3081da01e0f54970aa6a3c4e54b8cb06": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40ccfe508ef444d59da1d85da527c681": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e0e6fa43efeb4980a685e398965fddd1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1ef581b00aa4983a7d96e3932e173b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8dde65697b5e4f07b22780df3b2ed994": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb1314888ee5473799a49e1784559726": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}