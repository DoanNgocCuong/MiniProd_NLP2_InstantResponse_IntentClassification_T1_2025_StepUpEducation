{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "- 3500 Test (Ko Shuffle): 75.95%\n",
    "\n",
    "2. ƒê√°nh gi√° tr√™n b·ªô 600 c√¢u ƒë∆∞·ª£c t·∫°o b·ªüi 50 c·ª•m m·ªõi ho√†n to√†n: \n",
    "- V·ªõi BERT c≈©: ƒê·ªÉ b√¨nh th∆∞·ªùng v√† CLS, SEP b·ªã double => Accuracy: 68.16% (10 epochs). \n",
    "H√¥m qua (best epoch 90/100 epochs) ƒë√°nh gi√° tr√™n data m·ªõi toanh n√†y ch·ªâ ƒë·∫°t 46.67%\n",
    "- Khi Chuy·ªÉn sang: lowercase v√† question: {question}. answer: {answer} ƒë·ªô ch√≠nh x√°c l√™n : 69.65% (10 epochs). \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D∆∞·ªõi ƒë√¢y l√† b·∫£ng so s√°nh chi ti·∫øt gi·ªØa **mBERT**, **XLM-RoBERTa**, v√† **ProphetNet** khi √°p d·ª•ng v√†o b√†i to√°n ph√¢n lo·∫°i ng·ªØ nghƒ©a (classification) nh∆∞ b·∫°n m√¥ t·∫£:\n",
    "\n",
    "---\n",
    "\n",
    "| **Ti√™u ch√≠**                  | **mBERT**                                  | **XLM-RoBERTa**                              | **ProphetNet**                              |\n",
    "|--------------------------------|--------------------------------------------|---------------------------------------------|---------------------------------------------|\n",
    "| **Ki·∫øn tr√∫c**                 | Transformer                               | Transformer                                 | Transformer + Future N-Gram Prediction     |\n",
    "| **D·ªØ li·ªáu hu·∫•n luy·ªán**        | Wikipedia (104 ng√¥n ng·ªØ)                  | CommonCrawl (100 ng√¥n ng·ªØ, d·ªØ li·ªáu l·ªõn h∆°n) | Custom Dataset cho Text Generation         |\n",
    "| **T·ªëi ∆∞u h√≥a task n√†o?**      | Ph√¢n lo·∫°i (Classification)                | Ph√¢n lo·∫°i (Classification)                 | Sinh ng·ªØ c·∫£nh t∆∞∆°ng lai (Text Generation)  |\n",
    "| **Hi·ªáu su·∫•t Classification** | T·ªët, ph√π h·ª£p cho ƒëa ng√¥n ng·ªØ c∆° b·∫£n.       | T·ªët h∆°n mBERT nh·ªù d·ªØ li·ªáu phong ph√∫ h∆°n.   | Kh√¥ng t·ªëi ∆∞u cho classification tasks.     |\n",
    "| **Ng√¥n ng·ªØ h·ªó tr·ª£**           | 104                                       | 100                                         | T·∫≠p trung v√†o ti·∫øng Anh.                   |\n",
    "| **K√≠ch th∆∞·ªõc m√¥ h√¨nh**        | ~450MB                                    | ~550MB                                      | ~570MB                                      |\n",
    "| **Kh·∫£ nƒÉng x·ª≠ l√Ω song ng·ªØ (EN-VI)** | ·ªîn ƒë·ªãnh nh∆∞ng kh√¥ng t·ªët b·∫±ng XLM-RoBERTa. | T·ªët h∆°n nh·ªù d·ªØ li·ªáu ƒëa d·∫°ng.               | Kh√¥ng ph√π h·ª£p cho ƒëa ng√¥n ng·ªØ.             |\n",
    "| **∆Øu ƒëi·ªÉm**                   | Nh·∫π, ph·ªï bi·∫øn, ti·∫øt ki·ªám t√†i nguy√™n.      | Hi·ªáu su·∫•t cao, t·ªïng qu√°t t·ªët tr√™n d·ªØ li·ªáu ph·ª©c t·∫°p. | Sinh ng·ªØ c·∫£nh ch√≠nh x√°c, t·ªët cho QA v√† summarization. |\n",
    "| **Nh∆∞·ª£c ƒëi·ªÉm**                | T·ªïng qu√°t k√©m tr√™n d·ªØ li·ªáu l·ªõn.           | Y√™u c·∫ßu t√†i nguy√™n t√≠nh to√°n cao h∆°n mBERT. | Kh√¥ng ph√π h·ª£p cho classification.          |\n",
    "| **Th·ªùi gian inference**       | Nhanh h∆°n XLM-RoBERTa, ProphetNet.        | Ch·∫≠m h∆°n mBERT, nh∆∞ng nhanh h∆°n ProphetNet. | Ch·∫≠m nh·∫•t do c·∫•u tr√∫c d·ª± ƒëo√°n t∆∞∆°ng lai.   |\n",
    "| **ƒê·ªô ch√≠nh x√°c (EN-VI Classification)** | Kh√° t·ªët (nh∆∞ng d·ªÖ b·ªã overfitting).         | T·ªët nh·∫•t nh·ªù t√≠nh ƒëa d·∫°ng d·ªØ li·ªáu.         | Kh√¥ng ph√π h·ª£p do kh√¥ng t·ªëi ∆∞u classification.|\n",
    "\n",
    "---\n",
    "\n",
    "### ƒê√°nh gi√° chi ti·∫øt:\n",
    "\n",
    "#### **1. mBERT**:\n",
    "- **∆Øu ƒëi·ªÉm**: Nh·∫π, ph·ªï bi·∫øn, d·ªÖ tri·ªÉn khai. Ti·∫øt ki·ªám t√†i nguy√™n.\n",
    "- **Nh∆∞·ª£c ƒëi·ªÉm**: Kh·∫£ nƒÉng t·ªïng qu√°t v√† ƒë·ªô ch√≠nh x√°c k√©m h∆°n XLM-RoBERTa, ƒë·∫∑c bi·ªát tr√™n t·∫≠p d·ªØ li·ªáu ph·ª©c t·∫°p ho·∫∑c ƒëa ng√¥n ng·ªØ (nh∆∞ ti·∫øng Anh-Vi·ªát).\n",
    "- **Khi n√†o n√™n d√πng**: N·∫øu t√†i nguy√™n t√≠nh to√°n b·ªã gi·ªõi h·∫°n v√† task kh√¥ng qu√° ph·ª©c t·∫°p.\n",
    "\n",
    "#### **2. XLM-RoBERTa**:\n",
    "- **∆Øu ƒëi·ªÉm**: Hi·ªáu su·∫•t cao h∆°n mBERT trong h·∫ßu h·∫øt c√°c t√°c v·ª• ƒëa ng√¥n ng·ªØ. ƒê·∫∑c bi·ªát t·ªët cho ti·∫øng Anh v√† ti·∫øng Vi·ªát.\n",
    "- **Nh∆∞·ª£c ƒëi·ªÉm**: Y√™u c·∫ßu t√†i nguy√™n t√≠nh to√°n l·ªõn h∆°n, t·ªëc ƒë·ªô inference ch·∫≠m h∆°n m·ªôt ch√∫t.\n",
    "- **Khi n√†o n√™n d√πng**: N·∫øu ∆∞u ti√™n ƒë·ªô ch√≠nh x√°c cao h∆°n v√† t√†i nguy√™n kh√¥ng ph·∫£i v·∫•n ƒë·ªÅ l·ªõn.\n",
    "\n",
    "#### **3. ProphetNet**:\n",
    "- **∆Øu ƒëi·ªÉm**: T·ªëi ∆∞u cho b√†i to√°n sinh ng·ªØ c·∫£nh t∆∞∆°ng lai, nh∆∞ t√≥m t·∫Øt vƒÉn b·∫£n, tr·∫£ l·ªùi c√¢u h·ªèi.\n",
    "- **Nh∆∞·ª£c ƒëi·ªÉm**: Kh√¥ng ph√π h·ª£p v·ªõi c√°c t√°c v·ª• ph√¢n lo·∫°i. Kh√¥ng h·ªó tr·ª£ t·ªët ƒëa ng√¥n ng·ªØ.\n",
    "- **Khi n√†o n√™n d√πng**: N·∫øu b·∫°n l√†m b√†i to√°n text generation ho·∫∑c summarization, kh√¥ng ph·∫£i classification.\n",
    "\n",
    "---\n",
    "\n",
    "### **G·ª£i √Ω ch·ªçn m√¥ h√¨nh cho task c·ªßa b·∫°n**:\n",
    "1. **∆Øu ti√™n ch√≠nh x√°c**: D√πng **XLM-RoBERTa** (ho·∫∑c DistilXLM-RoBERTa n·∫øu c·∫ßn gi·∫£m k√≠ch th∆∞·ªõc).\n",
    "2. **∆Øu ti√™n t·ªëc ƒë·ªô v√† t√†i nguy√™n**: D√πng **mBERT**.\n",
    "3. **Kh√¥ng d√πng ProphetNet**: V√¨ kh√¥ng t·ªëi ∆∞u cho ph√¢n lo·∫°i.\n",
    "\n",
    "C·∫ßn th√™m h·ªó tr·ª£ g√¨, b·∫°n c√≥ th·ªÉ n√≥i r√µ h∆°n nh√©! üòä"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# 1. Chu·∫©n b·ªã d·ªØ li·ªáu\n",
    "def prepare_dataset(file_path, text_columns, label_column, tokenizer, max_seq_length):\n",
    "    \"\"\"\n",
    "    Chu·∫©n b·ªã d·ªØ li·ªáu t·ª´ file Excel.\n",
    "    - Gh√©p c√¢u `robot` v√† `user_answer` th√†nh input\n",
    "    - Tokenize d·ªØ li·ªáu.\n",
    "    \"\"\"\n",
    "    # ƒê·ªçc d·ªØ li·ªáu\n",
    "    df = pd.read_excel(file_path)\n",
    "\n",
    "    # Gh√©p vƒÉn b·∫£n\n",
    "    def combine_text(row):\n",
    "        question = row[text_columns[0]].strip().lower()\n",
    "        answer = row[text_columns[1]].strip().lower() if pd.notna(row[text_columns[1]]) else \"\"\n",
    "        return f\"question: {question}. answer: {answer}\"\n",
    "\n",
    "    df[\"input_text\"] = df.apply(combine_text, axis=1)\n",
    "\n",
    "    # Chuy·ªÉn ƒë·ªïi nh√£n th√†nh s·ªë\n",
    "    unique_labels = sorted(df[label_column].unique())\n",
    "    label2id = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "    df[\"label\"] = df[label_column].map(label2id)\n",
    "\n",
    "    # Tokenize\n",
    "    tokenized_data = tokenizer(\n",
    "        list(df[\"input_text\"]),\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=max_seq_length\n",
    "    )\n",
    "    tokenized_data[\"labels\"] = list(df[\"label\"])\n",
    "\n",
    "    # Chuy·ªÉn th√†nh HuggingFace Dataset\n",
    "    dataset = Dataset.from_dict(tokenized_data)\n",
    "    return dataset, label2id\n",
    "\n",
    "# C·∫•u h√¨nh\n",
    "file_path = \"/content/processed_data_example_v4_15000Data.xlsx\"  # ƒê∆∞·ªùng d·∫´n file d·ªØ li·ªáu\n",
    "text_columns = [\"robot\", \"user_answer\"]\n",
    "label_column = \"user_intent\"\n",
    "max_seq_length = 128\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "\n",
    "# Chu·∫©n b·ªã dataset\n",
    "train_dataset, label2id = prepare_dataset(file_path, text_columns, label_column, tokenizer, max_seq_length)\n",
    "print(f\"Label mapping: {label2id}\")\n",
    "\n",
    "# 2. Hu·∫•n luy·ªán m√¥ h√¨nh\n",
    "num_labels = len(label2id)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"xlm-roberta-base\", num_labels=num_labels)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"H√†m t√≠nh to√°n c√°c metric.\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    preds = predictions.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    return {\"accuracy\": acc, \"f1\": f1}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=train_dataset,  # Thay b·∫±ng valid_dataset n·∫øu c√≥\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# 3. ƒê√°nh gi√° tr√™n t·∫≠p test\n",
    "def evaluate_model(test_file_path):\n",
    "    \"\"\"ƒê√°nh gi√° m√¥ h√¨nh tr√™n t·∫≠p test.\"\"\"\n",
    "    test_dataset, _ = prepare_dataset(test_file_path, text_columns, label_column, tokenizer, max_seq_length)\n",
    "    results = trainer.predict(test_dataset)\n",
    "\n",
    "    print(\"Metrics:\", results.metrics)\n",
    "\n",
    "    # X·ª≠ l√Ω nh√£n d·ª± ƒëo√°n\n",
    "    predictions = results.predictions.argmax(axis=1)\n",
    "    test_dataset = test_dataset.to_pandas()\n",
    "    test_dataset[\"predicted_label\"] = predictions\n",
    "    test_dataset[\"predicted_label_name\"] = test_dataset[\"predicted_label\"].map({v: k for k, v in label2id.items()})\n",
    "\n",
    "    # L∆∞u k·∫øt qu·∫£ ra file\n",
    "    test_dataset.to_excel(\"test_results.xlsx\", index=False)\n",
    "    print(\"Test results saved to test_results.xlsx\")\n",
    "\n",
    "# ƒê∆∞·ªùng d·∫´n t·∫≠p test\n",
    "test_file_path = \"/content/test1_processed_TEST_500to1000Phrase.xlsx\"\n",
    "evaluate_model(test_file_path)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output: - google colab - t·∫ßm 5 min\n",
    "```\n",
    "Label mapping: {'intent_fallback': 0, 'intent_learn_more': 1, 'intent_negative': 2, 'intent_neutral': 3, 'intent_positive': 4, 'silence': 5}\n",
    "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
    "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
    "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
    "  warnings.warn(\n",
    "<ipython-input-77-0cf010a58281>:82: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
    "  trainer = Trainer(\n",
    " [246/246 07:22, Epoch 1/1]\n",
    "Epoch\tTraining Loss\tValidation Loss\tAccuracy\tF1\n",
    "1\t0.372600\t0.351577\t0.878297\t0.878652\n",
    "Metrics: {'test_loss': 0.6311067342758179, 'test_accuracy': 0.7761194029850746, 'test_f1': 0.781688838698698, 'test_runtime': 1.7385, 'test_samples_per_second': 346.844, 'test_steps_per_second': 5.752}\n",
    "Test results saved to test_results.xlsx\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
