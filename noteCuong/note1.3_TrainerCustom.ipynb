{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. thế def compute này thừa à -)) ??? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "class TrainerCustom(Trainer):\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        \"\"\"\n",
    "        How the loss is computed by Trainer. By default, all models return the loss in the first element.\n",
    "\n",
    "        Subclass and override for custom behavior.\n",
    "        \"\"\"\n",
    "        if \"labels\" in inputs:\n",
    "            labels = inputs.pop(\"labels\")\n",
    "        else:\n",
    "            labels = None\n",
    "\n",
    "        # Sử dụng nn.CrossEntropyLoss() thay vì nn.CrossEntropy\n",
    "        cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Chạy mô hình và nhận đầu ra (logits)\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        # Đảm bảo lấy logits từ outputs (mô hình trả về tuple, lấy phần tử đầu tiên là logits)\n",
    "        logits = outputs\n",
    "\n",
    "        if labels is None:\n",
    "            print(\"Labels are None during compute_loss.\")\n",
    "        if logits is None:\n",
    "            print(\"Logits are None during compute_loss.\")\n",
    "\n",
    "        # Tính toán loss\n",
    "        loss = cross_entropy_loss(logits, labels)\n",
    "\n",
    "        # Trả về loss và outputs nếu cần\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "```\n",
    "\n",
    "thế def compute này thừa à -)) ??? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hàm `compute_loss` trong lớp `TrainerCustom` không thực sự \"thừa\", mà nó có ý nghĩa nếu bạn cần một cách tính toán `loss` tùy chỉnh. Dưới đây là phân tích:\n",
    "\n",
    "---\n",
    "\n",
    "### **Mặc định của `Trainer`**\n",
    "- Mặc định, `Trainer` lấy giá trị `loss` được trả về từ mô hình (thường ở vị trí đầu tiên trong output).\n",
    "- Nếu mô hình của bạn trả về `loss` như một phần của output tuple, bạn không cần ghi đè `compute_loss`.\n",
    "\n",
    "---\n",
    "\n",
    "### **Hàm `compute_loss` của bạn**\n",
    "Hàm này ghi đè hành vi mặc định:\n",
    "1. **Tính toán `loss` tùy chỉnh**:\n",
    "   - Bạn sử dụng `nn.CrossEntropyLoss()` để tính toán `loss` thủ công từ `logits` và `labels`. Điều này hữu ích nếu:\n",
    "     - Bạn cần kiểm soát chi tiết cách tính toán `loss`.\n",
    "     - Bạn muốn thay đổi hàm mất mát (vd: `Focal Loss`, `Label Smoothing`,...).\n",
    "     - Mô hình của bạn không trả về `loss` trong output.\n",
    "\n",
    "2. **Trường hợp sử dụng cần thiết**:\n",
    "   - Mô hình của bạn **không trả về loss mặc định** hoặc trả về output không tương thích với cách xử lý mặc định của `Trainer`.\n",
    "   - Bạn cần tích hợp thêm logic, như in thông tin chẩn đoán (`Labels are None during compute_loss`).\n",
    "\n",
    "3. **Nếu mô hình đã trả về `loss` sẵn**:\n",
    "   - Nếu mô hình trả về `loss` trong output (như các model từ Hugging Face thường làm), bạn không cần ghi đè `compute_loss` trừ khi muốn thay đổi hàm mất mát.\n",
    "\n",
    "---\n",
    "\n",
    "### **Hàm này có thực sự cần thiết?**\n",
    "#### **Khi không cần:**\n",
    "- Mô hình của bạn trả về `loss` sẵn trong output (vd: `AutoModelForSequenceClassification` của Hugging Face).\n",
    "- Bạn không cần thay đổi cách tính `loss`.\n",
    "\n",
    "#### **Khi cần thiết:**\n",
    "- Bạn sử dụng một mô hình tùy chỉnh (`BERTIntentClassification` trong mã của bạn) mà **không trả về `loss`**.\n",
    "- Bạn cần logic tính `loss` đặc biệt (vd: `Focal Loss`, `Dice Loss`, hay các điều chỉnh khác).\n",
    "\n",
    "---\n",
    "\n",
    "### **Tóm lại**\n",
    "- **Nếu mô hình của bạn (vd: `BERTIntentClassification`) không tự trả về `loss`**, thì hàm này **không thừa**.\n",
    "- **Nếu mô hình của bạn đã trả về `loss` theo chuẩn Hugging Face**, thì hàm này có thể bỏ qua và dựa vào cách tính mặc định của `Trainer`.\n",
    "\n",
    "Nếu bạn không cần `compute_loss`, hãy thử chạy `Trainer` mà không ghi đè hàm này để xem mọi thứ hoạt động đúng không. Nếu không có lỗi, thì hàm này có thể bỏ qua."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "class BERTIntentClassification(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self, model_name=\"bert-base-uncased\", num_classes=10, dropout_rate=0.1, cache_dir = \"huggingface\"):\n",
    "        super(BERTIntentClassification, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name, cache_dir = cache_dir)\n",
    "        # Get BERT hidden size\n",
    "        hidden_size = self.bert.config.hidden_size\n",
    "        self.ffnn = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "\n",
    "\n",
    "    def freeze_bert(self):\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "\n",
    "    def get_pooling(self, hidden_state, attention_mask):\n",
    "        \"\"\"\n",
    "        Get mean pooled representation from BERT hidden states\n",
    "        Args:\n",
    "            hidden_state: BERT output containing hidden states\n",
    "        Returns:\n",
    "            pooled_output: Mean pooled representation of the sequence\n",
    "        \"\"\"\n",
    "        # Get last hidden state\n",
    "        last_hidden_state = hidden_state.last_hidden_state  # Shape: [batch_size, seq_len, hidden_size]\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            # Expand attention mask to match hidden state dimensions\n",
    "            attention_mask = attention_mask.unsqueeze(-1)  # [batch_size, seq_len, 1]\n",
    "\n",
    "            # Mask out padding tokens\n",
    "            masked_hidden = last_hidden_state * attention_mask\n",
    "\n",
    "            # Calculate mean (sum / number of actual tokens)\n",
    "            sum_hidden = torch.sum(masked_hidden, dim=1)  # [batch_size, hidden_size]\n",
    "            count_tokens = torch.sum(attention_mask, dim=1)  # [batch_size, 1]\n",
    "            pooled_output = sum_hidden / count_tokens\n",
    "        else:\n",
    "            # If no attention mask, simply take mean of all tokens\n",
    "            pooled_output = torch.mean(last_hidden_state, dim=1)\n",
    "\n",
    "        return pooled_output\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, **kwargs):\n",
    "        \"\"\"\n",
    "        Forward pass of the model\n",
    "        Args:\n",
    "            input_ids: Input token IDs\n",
    "            attention_mask: Attention mask for padding\n",
    "        Returns:\n",
    "            logits: Raw logits for each class\n",
    "        \"\"\"\n",
    "        # Get BERT hidden states\n",
    "        hidden_state = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "\n",
    "        # Get pooled representation\n",
    "        hidden_state_pooling = self.get_pooling(hidden_state=hidden_state, attention_mask=attention_mask)\n",
    "\n",
    "        # Pass through FFNN classifier\n",
    "        logits = self.ffnn(hidden_state_pooling)\n",
    "\n",
    "        return logits\n",
    "```\n",
    "\n",
    "Đây "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Với lớp `BERTIntentClassification` này, hàm `forward` trả về **logits**, chứ không trả về giá trị `loss`. Vì vậy, việc ghi đè hàm `compute_loss` trong `TrainerCustom` là **cần thiết** nếu bạn muốn huấn luyện mô hình này.\n",
    "\n",
    "---\n",
    "\n",
    "### **Lý do cần `compute_loss`**\n",
    "1. **Hành vi mặc định của `Trainer`:**\n",
    "   - `Trainer` mặc định sử dụng giá trị `loss` do mô hình trả về (nếu có). Tuy nhiên, mô hình của bạn (`BERTIntentClassification`) chỉ trả về `logits`, không có `loss`.\n",
    "   - Do đó, `Trainer` sẽ không thể tự tính `loss` nếu không có sự can thiệp của bạn.\n",
    "\n",
    "2. **Hàm `compute_loss`:**\n",
    "   - Hàm này cho phép bạn tính toán `loss` thủ công từ `logits` và `labels`, giúp `Trainer` hoạt động đúng.\n",
    "\n",
    "---\n",
    "\n",
    "### **Cách hoạt động hiện tại**\n",
    "Hàm `forward` trả về:\n",
    "```python\n",
    "logits = self.ffnn(hidden_state_pooling)\n",
    "```\n",
    "\n",
    "`logits` là đầu ra chưa được chuẩn hóa (raw logits), thường được sử dụng để tính toán `loss` bằng các hàm như `nn.CrossEntropyLoss`.\n",
    "\n",
    "Hàm `compute_loss` trong `TrainerCustom`:\n",
    "```python\n",
    "cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "logits = outputs\n",
    "loss = cross_entropy_loss(logits, labels)\n",
    "```\n",
    "- Sử dụng `nn.CrossEntropyLoss` để tính toán `loss` từ `logits` và `labels`.\n",
    "\n",
    "---\n",
    "\n",
    "### **Cách cải thiện (Tùy chọn)**\n",
    "Nếu bạn muốn đơn giản hóa việc tích hợp với `Trainer`, bạn có thể chỉnh sửa lớp `BERTIntentClassification` để trả về cả `loss` và `logits` trong hàm `forward`:\n",
    "\n",
    "#### **Cập nhật `forward`**\n",
    "```python\n",
    "def forward(self, input_ids, attention_mask, labels=None, **kwargs):\n",
    "    # Get BERT hidden states\n",
    "    hidden_state = self.bert(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "    )\n",
    "\n",
    "    # Get pooled representation\n",
    "    hidden_state_pooling = self.get_pooling(hidden_state=hidden_state, attention_mask=attention_mask)\n",
    "\n",
    "    # Pass through FFNN classifier\n",
    "    logits = self.ffnn(hidden_state_pooling)\n",
    "\n",
    "    # Tính loss nếu có nhãn (labels)\n",
    "    loss = None\n",
    "    if labels is not None:\n",
    "        cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "        loss = cross_entropy_loss(logits, labels)\n",
    "\n",
    "    return (loss, logits) if loss is not None else logits\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Kết hợp với `Trainer`**\n",
    "Sau khi cập nhật, bạn không cần ghi đè `compute_loss`. `Trainer` sẽ tự động sử dụng `loss` từ mô hình nếu có.\n",
    "\n",
    "#### **Cách sử dụng**\n",
    "```python\n",
    "trainer = Trainer(\n",
    "    model=model,  # Mô hình đã được cập nhật\n",
    "    args=training_args,\n",
    "    train_dataset=sample_train_dataset,\n",
    "    eval_dataset=sample_test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collate_fn,\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Tóm lại**\n",
    "1. **Với mã hiện tại:**\n",
    "   - Hàm `compute_loss` trong `TrainerCustom` **không thừa** vì mô hình không trả về `loss`.\n",
    "\n",
    "2. **Nếu cập nhật mô hình để trả về `loss`:**\n",
    "   - Bạn có thể bỏ qua `compute_loss` và sử dụng `Trainer` mặc định.\n",
    "\n",
    "Cách làm nào phù hợp hơn sẽ phụ thuộc vào việc bạn muốn giữ mô hình linh hoạt (không phụ thuộc vào cách tính `loss`) hay muốn tối ưu hóa cho `Trainer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
