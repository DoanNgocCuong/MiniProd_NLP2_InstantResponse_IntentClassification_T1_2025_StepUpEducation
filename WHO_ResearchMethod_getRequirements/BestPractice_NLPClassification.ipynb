{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. https://github.com/Krish95-bg/DistilBERT-for-Intent-Detection-Conversation-Message-Classification-with-LLM/blob/main/Intent_Detection_In_Conversation_Messages_Using_Distill_Bert_For_Multi_Class_Classification.ipynb\n",
    "- DistilBERT for Intent Detection\n",
    "Dự án này sử dụng mô hình DistilBERT đã được huấn luyện trước để phân loại ý định của người dùng trong ngôn ngữ tự nhiên, phù hợp cho các ứng dụng như chatbot và trợ lý ảo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đây là **hướng dẫn chi tiết cách fine-tune mô hình DistilBERT** trên một tập dữ liệu tuỳ chỉnh cho bài toán **phân loại ý định (Intent Classification)**. Đây là một bài toán phổ biến trong xử lý ngôn ngữ tự nhiên (NLP), đặc biệt trong các ứng dụng như chatbot hoặc hệ thống hỗ trợ ảo.\n",
    "\n",
    "---\n",
    "\n",
    "### **Phân tích từng phần:**\n",
    "\n",
    "1. **Cài đặt thư viện:**\n",
    "   - Sử dụng các thư viện như:\n",
    "     - **`transformers`**: Để xử lý tokenization và mô hình BERT (hoặc DistilBERT).\n",
    "     - **`datasets`**: Để quản lý và tiền xử lý dữ liệu.\n",
    "     - **`sklearn`**: Để đánh giá mô hình (classification report, confusion matrix).\n",
    "\n",
    "2. **Tải dữ liệu:**\n",
    "   - Dữ liệu là một tập hợp các câu (`text`) và nhãn ý định (`intent`) với nhiều lớp như: \n",
    "     - `playmusic`, `getweather`, `addtoplaylist`,...\n",
    "   - Dữ liệu được chia thành `text` và `intent`, với `intent` là nhãn đầu ra cho bài toán phân loại.\n",
    "\n",
    "3. **Tiền xử lý dữ liệu:**\n",
    "   - Loại bỏ stopwords (từ không mang ý nghĩa nhiều) bằng thư viện NLTK.\n",
    "   - Chuyển nhãn từ dạng văn bản (`intent`) sang dạng số (`Label`) để dễ huấn luyện.\n",
    "\n",
    "4. **Tokenization:**\n",
    "   - Sử dụng **DistilBERT Tokenizer** để chuyển đổi các câu thành chuỗi token và ánh xạ token sang ID.\n",
    "   - Thêm padding, truncation để đảm bảo các câu có độ dài nhất quán.\n",
    "\n",
    "5. **Huấn luyện mô hình:**\n",
    "   - Sử dụng mô hình **DistilBERTForSequenceClassification** từ thư viện HuggingFace.\n",
    "   - Tùy chỉnh số lượng nhãn (7 nhãn cho 7 lớp ý định).\n",
    "   - Sử dụng **Trainer API** để huấn luyện mô hình với các tham số:\n",
    "     - Batch size: 16 (huấn luyện), 64 (đánh giá).\n",
    "     - Epochs: 3.\n",
    "     - Learning rate: 2e-5.\n",
    "\n",
    "6. **Đánh giá mô hình:**\n",
    "   - Sử dụng **classification report** và **confusion matrix** để đánh giá hiệu quả dự đoán trên dữ liệu kiểm tra.\n",
    "   - Kết quả cho thấy độ chính xác rất cao (~99%), chứng tỏ mô hình phù hợp với bài toán.\n",
    "\n",
    "7. **Lưu mô hình:**\n",
    "   - Lưu mô hình và tokenizer để sử dụng lại sau này.\n",
    "   - Đóng gói và tải mô hình về máy tính.\n",
    "\n",
    "8. **Dự đoán ý định:**\n",
    "   - Xây dựng hệ thống dự đoán:\n",
    "     - Sử dụng mô hình đã huấn luyện để dự đoán ý định từ câu nhập vào.\n",
    "     - Hiển thị nhãn ý định tương ứng với câu nhập.\n",
    "\n",
    "---\n",
    "\n",
    "### **Ý nghĩa của dự án:**\n",
    "Dự án này minh họa cách:\n",
    "- **Fine-tune mô hình pre-trained (DistilBERT)** cho bài toán NLP tùy chỉnh.\n",
    "- Xây dựng quy trình đầy đủ từ tải dữ liệu, tiền xử lý, huấn luyện, đánh giá, lưu mô hình, và triển khai dự đoán.\n",
    "- Ứng dụng các kỹ thuật hiện đại trong xử lý ngôn ngữ tự nhiên.\n",
    "\n",
    "---\n",
    "\n",
    "Nếu bạn cần giải thích thêm hoặc hướng dẫn cụ thể hơn về một phần nào trong quy trình này, hãy cho mình biết nhé! 😊"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. https://github.com/HelpRam/Fine-Tune-Pretrained-model-DistilBert-/blob/main/BERT_FineTuning_Student.ipynb\n",
    "- Fine-Tune Pretrained DistilBERT Model\n",
    "Dự án này chứa mã nguồn để fine-tune mô hình DistilBERT đã được huấn luyện trước cho việc phát hiện ý định trong các tin nhắn hội thoại, bao gồm các bước từ chuẩn bị dữ liệu đến triển khai mô hình"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Đây là **hướng dẫn chi tiết và ví dụ** về cách fine-tune mô hình BERT (Bidirectional Encoder Representations from Transformers) cho bài toán **phân loại văn bản**, cụ thể là trên bộ dữ liệu **CoLA (Corpus of Linguistic Acceptability)** từ bộ GLUE Benchmark.\n",
    "\n",
    "### Ý nghĩa từng phần:\n",
    "1. **Cài đặt môi trường và thư viện**:\n",
    "   - Cài các thư viện như `torch`, `transformers`, `datasets`, là những thư viện cần thiết để làm việc với mô hình BERT và xử lý dữ liệu.\n",
    "\n",
    "2. **Chọn thiết bị (Device Selection)**:\n",
    "   - Kiểm tra xem có GPU không và chọn thiết bị GPU (nếu có) để tăng tốc độ huấn luyện. Nếu không, sẽ sử dụng CPU.\n",
    "\n",
    "3. **Tải và xử lý dữ liệu**:\n",
    "   - Bộ dữ liệu CoLA là một tập hợp các câu được gán nhãn là đúng ngữ pháp (label = 1) hoặc sai ngữ pháp (label = 0).\n",
    "   - Dữ liệu được tải bằng thư viện `datasets` của HuggingFace, và sau đó chuyển thành DataFrame để dễ thao tác.\n",
    "\n",
    "4. **Tokenization**:\n",
    "   - Chuyển đổi các câu thành các **token** (đơn vị nhỏ nhất mà mô hình BERT có thể hiểu), sau đó ánh xạ chúng sang các **ID số**.\n",
    "   - Sử dụng `BertTokenizer` của mô hình BERT để thêm các token đặc biệt như `[CLS]`, `[SEP]`, và xử lý padding/truncating để đưa các câu về cùng độ dài.\n",
    "\n",
    "5. **Tạo DataLoader**:\n",
    "   - Dữ liệu được chia thành các batch để huấn luyện và kiểm tra. Batch size được chọn dựa trên bài báo gốc của BERT.\n",
    "\n",
    "6. **Fine-tuning mô hình BERT**:\n",
    "   - Sử dụng lớp `BertForSequenceClassification` để tùy chỉnh BERT cho bài toán phân loại.\n",
    "   - Thêm một lớp phân loại đơn giản (fully connected layer) lên trên cùng của mô hình BERT.\n",
    "   - Sử dụng các hàm loss và optimizer phù hợp như `AdamW`.\n",
    "\n",
    "7. **Huấn luyện và đánh giá**:\n",
    "   - Chia dữ liệu thành 90% để huấn luyện và 10% để kiểm tra.\n",
    "   - Huấn luyện qua nhiều epoch, tính loss trung bình và độ chính xác sau mỗi epoch.\n",
    "   - Sử dụng Learning Rate Scheduler để điều chỉnh learning rate trong quá trình huấn luyện.\n",
    "\n",
    "8. **Kết quả**:\n",
    "   - Sau khi huấn luyện, hiển thị các thông số như loss, accuracy, và thời gian cho từng epoch.\n",
    "   - Các kết quả được trình bày dưới dạng bảng để dễ phân tích.\n",
    "\n",
    "### Ý nghĩa chính:\n",
    "- Hướng dẫn này minh họa cách tùy chỉnh và fine-tune mô hình BERT cho một bài toán cụ thể (phân loại ngữ pháp câu).\n",
    "- Nó cũng cung cấp cách sử dụng thư viện HuggingFace và PyTorch để thực hiện toàn bộ pipeline: từ tải dữ liệu, xử lý, đến huấn luyện mô hình.\n",
    "\n",
    "Nếu bạn đang học về Machine Learning hoặc NLP, đây là một bài tập rất tốt để hiểu cách ứng dụng các mô hình ngôn ngữ tiên tiến như BERT. Bạn cần thêm hỗ trợ ở phần nào cụ thể không? 😊"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. https://github.com/Beckendrof/intent-classification/\n",
    "- Intent Classification with DistilBERT\n",
    "Dự án này fine-tune mô hình DistilBERT để thực hiện phân loại ý định đa lớp, với tập dữ liệu gồm 84 lớp của các phản hồi hội thoại chung. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Code a Hùng \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "```python\n",
    "from transformers import AutoTokenizer, TrainingArguments, Trainer, AutoModel\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\" ## Setup CUDA GPU 1\n",
    "\n",
    "\n",
    "class BERTIntentClassification(nn.Module):\n",
    "    \n",
    "    \n",
    "    def __init__(self, model_name=\"bert-base-uncased\", num_classes=10, dropout_rate=0.1, cache_dir = \"huggingface\"):\n",
    "        super(BERTIntentClassification, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name, cache_dir = cache_dir)\n",
    "        # Get BERT hidden size\n",
    "        hidden_size = self.bert.config.hidden_size\n",
    "        self.ffnn = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "\n",
    "    \n",
    "    def freeze_bert(self):\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "\n",
    "    def get_pooling(self, hidden_state, attention_mask):\n",
    "        \"\"\"\n",
    "        Get mean pooled representation from BERT hidden states\n",
    "        Args:\n",
    "            hidden_state: BERT output containing hidden states\n",
    "        Returns:\n",
    "            pooled_output: Mean pooled representation of the sequence\n",
    "        \"\"\"\n",
    "        # Get last hidden state\n",
    "        last_hidden_state = hidden_state.last_hidden_state  # Shape: [batch_size, seq_len, hidden_size]\n",
    "        \n",
    "        if attention_mask is not None:\n",
    "            # Expand attention mask to match hidden state dimensions\n",
    "            attention_mask = attention_mask.unsqueeze(-1)  # [batch_size, seq_len, 1]\n",
    "            \n",
    "            # Mask out padding tokens\n",
    "            masked_hidden = last_hidden_state * attention_mask\n",
    "            \n",
    "            # Calculate mean (sum / number of actual tokens)\n",
    "            sum_hidden = torch.sum(masked_hidden, dim=1)  # [batch_size, hidden_size]\n",
    "            count_tokens = torch.sum(attention_mask, dim=1)  # [batch_size, 1]\n",
    "            pooled_output = sum_hidden / count_tokens\n",
    "        else:\n",
    "            # If no attention mask, simply take mean of all tokens\n",
    "            pooled_output = torch.mean(last_hidden_state, dim=1)\n",
    "        \n",
    "        return pooled_output\n",
    "        \n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, **kwargs):\n",
    "        \"\"\"\n",
    "        Forward pass of the model\n",
    "        Args:\n",
    "            input_ids: Input token IDs\n",
    "            attention_mask: Attention mask for padding\n",
    "        Returns:\n",
    "            logits: Raw logits for each class\n",
    "        \"\"\"\n",
    "        # Get BERT hidden states\n",
    "        hidden_state = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "        \n",
    "        # Get pooled representation\n",
    "        hidden_state_pooling = self.get_pooling(hidden_state=hidden_state, attention_mask=attention_mask)\n",
    "        \n",
    "        # Pass through FFNN classifier\n",
    "        logits = self.ffnn(hidden_state_pooling)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "\n",
    "\n",
    "class TrainerCustom(Trainer):\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        \"\"\"\n",
    "        How the loss is computed by Trainer. By default, all models return the loss in the first element.\n",
    "\n",
    "        Subclass and override for custom behavior.\n",
    "        \"\"\"\n",
    "        if \"labels\" in inputs:\n",
    "            labels = inputs.pop(\"labels\")\n",
    "        else:\n",
    "            labels = None\n",
    "        \n",
    "        # Sử dụng nn.CrossEntropyLoss() thay vì nn.CrossEntropy\n",
    "        cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Chạy mô hình và nhận đầu ra (logits)\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        # Đảm bảo lấy logits từ outputs (mô hình trả về tuple, lấy phần tử đầu tiên là logits)\n",
    "        logits = outputs\n",
    "        \n",
    "        # Tính toán loss\n",
    "        loss = cross_entropy_loss(logits, labels)\n",
    "        \n",
    "        # Trả về loss và outputs nếu cần\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "# Bước 1: Tải dữ liệu\n",
    "# Sử dụng dataset sẵn có từ Hugging Face hoặc tải từ file cục bộ\n",
    "dataset = load_dataset(\"imdb\", cache_dir = \"huggingface\")  # Ví dụ: Dữ liệu IMDB để phân loại sentiment\n",
    "# Thay thế trường 'text' thành 'input_ids' trong train_dataset và test_dataset\n",
    "def preprocess_dataset(dataset):\n",
    "    return dataset.map(lambda example: {\n",
    "            \"input_ids\": example['text'],\n",
    "            \"label\": example['label']\n",
    "        }, \n",
    "        remove_columns=[\"text\"],\n",
    "        num_proc=4  # Sử dụng 4 tiến trình song song để xử lý nhanh hơn\n",
    "    )\n",
    "\n",
    "train_dataset = preprocess_dataset(dataset[\"train\"])\n",
    "test_dataset = preprocess_dataset(dataset[\"test\"])\n",
    "\n",
    "\n",
    "# Bước 2: Chuẩn bị tokenizer và token hóa dữ liệu\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir = \"huggingface\")\n",
    "model = BERTIntentClassification(\n",
    "    model_name=model_name,\n",
    "    num_classes=2\n",
    ")\n",
    "model.freeze_bert() # Froze Layer BERT\n",
    "max_seq_length = 512\n",
    "\n",
    "\n",
    "def collate_fn(features):\n",
    "    inputs = []\n",
    "    labels = []\n",
    "    for element in features:\n",
    "        inputs.append(element.get(\"input_ids\"))\n",
    "        labels.append(element.get(\"label\"))\n",
    "    \n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    \n",
    "    token_inputs = tokenizer(\n",
    "        inputs,\n",
    "        add_special_tokens=True,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=max_seq_length,\n",
    "        return_overflowing_tokens=False,\n",
    "        return_length=False,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    token_inputs.update({\n",
    "        \"labels\": labels,\n",
    "    })\n",
    "    return token_inputs\n",
    "\n",
    "# Bước 6: Cài đặt tham số huấn luyện\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",          # Thư mục lưu kết quả\n",
    "    eval_strategy=\"epoch\",    # Đánh giá sau mỗi epoch\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=128,\n",
    "    per_device_eval_batch_size=128,\n",
    "    num_train_epochs=50,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=None,\n",
    "    logging_strategy = \"epoch\",\n",
    "    save_strategy=\"epoch\",          # Lưu trọng số sau mỗi epoch\n",
    "    save_total_limit=3,\n",
    ")\n",
    "\n",
    "# Bước 7: Tạo Trainer\n",
    "trainer = TrainerCustom(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator = collate_fn,\n",
    ")\n",
    "\n",
    "# Bước 8: Huấn luyện\n",
    "trainer.train()\n",
    "\n",
    "# Bước 9: Đánh giá trên tập kiểm tra\n",
    "trainer.evaluate()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So sánh : So sánh 2 code này ??? response dạng bảng. Cái nào: BEST PRACTICES HƠN >??\n",
    "- Code a Hùng BERT với code: Fine_Tunning_DistilBert https://github.com/HelpRam/Fine-Tune-Pretrained-model-DistilBert-/blob/main/Fine_Tunning_DistilBert_for_intention_Detection_.ipynb   và https://github.com/Krish95-bg/DistilBERT-for-Intent-Detection-Conversation-Message-Classification-with-LLM/blob/main/Intent_Detection_In_Conversation_Messages_Using_Distill_Bert_For_Multi_Class_Classification.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So sánh hai đoạn mã trên\n",
    "| **Tiêu chí**                          | **Đoạn mã 1 (DistilBERT Trainer)**                                                                                                                                                                                | **Đoạn mã 2 (BERT Custom Trainer)**                                                                                                                                                                                                                                                                                                             | **BEST PRACTICES**                                |\n",
    "|---------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------|\n",
    "| **Mô hình sử dụng**                   | Sử dụng **DistilBERTForSequenceClassification** từ thư viện HuggingFace.                                                                                                   | Tự xây dựng mô hình **BERTIntentClassification**, tùy chỉnh lớp FFNN, pooling, và dropout để linh hoạt hơn.                                                                                                                                                                                                                                     | **Đoạn mã 2**: Linh hoạt và tùy chỉnh tốt hơn.   |\n",
    "| **Tích hợp HuggingFace Trainer API**  | Sử dụng `Trainer` API có sẵn, đơn giản hóa huấn luyện và đánh giá.                                                                                                         | Tùy chỉnh `TrainerCustom` với hàm `compute_loss` riêng, phù hợp cho các bài toán phức tạp hơn.                                                                                                                                                                                                                                                   | **Đoạn mã 2**: Tùy chỉnh linh hoạt hơn.          |\n",
    "| **Tokenization và xử lý dữ liệu**     | Sử dụng `Dataset` từ HuggingFace và áp dụng tokenization trực tiếp.                                                                                                       | Tokenization với hàm `collate_fn`, đảm bảo cấu trúc phù hợp cho các tác vụ tùy chỉnh.                                                                                                                                                                                                                                                             | **Đoạn mã 2**: Tối ưu cho dữ liệu lớn, cấu trúc tốt. |\n",
    "| **Sử dụng GPU (CUDA)**                | Không chỉ định cụ thể thiết bị.                                                                                                                                            | Chỉ định sử dụng GPU cụ thể (`os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"`) cho bài toán lớn.                                                                                                                                                                                                                                                        | **Đoạn mã 2**: Quản lý tài nguyên GPU tốt hơn.   |\n",
    "| **Freeze Layer BERT**                 | Không áp dụng freezing cho các lớp BERT, tất cả các tham số đều được huấn luyện lại.                                                                                       | Có thể đóng băng các lớp BERT (`freeze_bert`) để tối ưu thời gian khi dữ liệu nhỏ hoặc không cần fine-tune toàn bộ.                                                                                                                                                                                                                               | **Đoạn mã 2**: Tùy chỉnh phù hợp hơn.            |\n",
    "| **Cài đặt tham số huấn luyện**        | Sử dụng tham số chuẩn với **Trainer** (batch size, learning rate, epochs).                                                                                                 | Sử dụng tham số tùy chỉnh cao hơn (batch size lớn, epochs dài, logging chi tiết).                                                                                                                                                                                                                                                                 | **Đoạn mã 2**: Kiểm soát tốt hơn quy trình huấn luyện. |\n",
    "| **Xử lý dữ liệu song song**           | Không có xử lý song song.                                                                                                                                                  | Sử dụng `num_proc=4` để xử lý dữ liệu song song, tăng tốc tokenization và tiền xử lý dữ liệu.                                                                                                                                                                                                                                                      | **Đoạn mã 2**: Tăng tốc xử lý dữ liệu.            |\n",
    "| **Đánh giá mô hình**                  | Sử dụng `Trainer` để thực hiện đánh giá tự động, bao gồm classification report và confusion matrix.                                                                        | Tự thiết kế pipeline đánh giá, linh hoạt hơn cho bài toán tùy chỉnh.                                                                                                                                                                                                                                                                              | **Đoạn mã 2**: Tùy chỉnh tốt hơn.                |\n",
    "| **Tốc độ huấn luyện**                 | Nhanh hơn do mô hình DistilBERT nhẹ hơn.                                                                                                                                   | Chậm hơn vì mô hình BERT lớn hơn và tùy chỉnh chi tiết hơn.                                                                                                                                                                                                                                                                                       | **Đoạn mã 1**: Nhanh hơn cho bài toán đơn giản.   |\n",
    "| **Mức độ linh hoạt**                  | Hạn chế, chỉ phù hợp với bài toán đơn giản do phụ thuộc nhiều vào API của HuggingFace.                                                                                     | Cao hơn, có thể tùy chỉnh cho các bài toán phức tạp hoặc mô hình riêng biệt.                                                                                                                                                                                                                                                                       | **Đoạn mã 2**: Linh hoạt hơn.                    |\n",
    "| **Khả năng mở rộng**                  | Phụ thuộc vào cấu trúc sẵn có của HuggingFace `Trainer`.                                                                                                                   | Linh hoạt, có thể mở rộng sang các bài toán khác như multi-label classification, regression, hoặc custom pooling.                                                                                                                                                                                                                                 | **Đoạn mã 2**: Mở rộng tốt hơn.                  |\n",
    "\n",
    "---\n",
    "\n",
    "### **Kết luận**\n",
    "- **Đoạn mã 1 (DistilBERT Trainer)**: \n",
    "  - Phù hợp cho các bài toán đơn giản, nơi yêu cầu triển khai nhanh chóng và không cần nhiều tùy chỉnh.\n",
    "  - Tối ưu hóa tốt hơn về tốc độ nhờ sử dụng mô hình DistilBERT (nhẹ hơn).\n",
    "- **Đoạn mã 2 (BERT Custom Trainer)**:\n",
    "  - Tốt hơn cho bài toán phức tạp, yêu cầu linh hoạt và khả năng tùy chỉnh cao.\n",
    "  - Hỗ trợ các mô hình lớn hơn và tối ưu tốt hơn cho quy trình huấn luyện, đánh giá.\n",
    "\n",
    "**BEST PRACTICES**: **Đoạn mã 2** vì khả năng tùy chỉnh, mở rộng, và kiểm soát chi tiết tốt hơn."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
